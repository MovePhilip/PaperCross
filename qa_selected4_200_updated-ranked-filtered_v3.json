{
 "./longdocdata/docs/2.json": {
  "question": "List the performance scores of various methods on the Moving MNIST (Moving MNIST) dataset on the Video Prediction task using metric MSE.",
  "answer": "| Method | MSE |\n| --- | --- |\n| PhyDNet | 24.4 |\n| MSPred | 34.44 |\n| LMC | 41.5 |\n| MIM* | 44.2 |\n| Causal LSTM | 46.5 |\n| PredRNN-V2 | 48.4 |\n| MIM | 52.0 |\n| ConvLSTM | 103.3 |",
  "src_docs": [
   "2104.00924",
   "1811.07490",
   "1506.04214",
   "1804.06300",
   "2003.01460",
   "2203.09303",
   "2103.09504"
  ],
  "updated_answer": [
   [
    "2003.01460",
    "PhyDNet",
    "24.4",
    24.4
   ],
   [
    "2203.09303",
    "MSPred",
    "34.44",
    34.44
   ],
   [
    "2104.00924",
    "LMC",
    "41.5",
    41.5
   ],
   [
    "1811.07490",
    "MIM*",
    "44.2",
    44.2
   ],
   [
    "1804.06300",
    "Causal LSTM",
    "46.5",
    46.5
   ],
   [
    "2103.09504",
    "PredRNN-V2",
    "48.4",
    48.4
   ],
   [
    "1811.07490",
    "MIM",
    "52.0",
    52.0
   ],
   [
    "1506.04214",
    "ConvLSTM",
    "103.3",
    103.3
   ]
  ],
  "meta_info": {
   "datasets": "Moving MNIST",
   "datasets_short": "Moving MNIST",
   "task": "Video Prediction",
   "metric": "MSE"
  },
  "updated_answer2": [
   [
    "2003.01460",
    "PhyDNet",
    "24.4",
    24.4
   ],
   [
    "2203.09303",
    "MSPred",
    "34.44",
    34.44
   ],
   [
    "2104.00924",
    "LMC",
    "41.5",
    41.5
   ],
   [
    "1811.07490",
    "MIM*",
    "44.2",
    44.2
   ],
   [
    "1804.06300",
    "Causal LSTM",
    "46.5",
    46.5
   ],
   [
    "2103.09504",
    "PredRNN-V2",
    "48.4",
    48.4
   ],
   [
    "1506.04214",
    "ConvLSTM",
    "103.3",
    103.3
   ]
  ]
 },
 "./longdocdata/docs/41.json": {
  "question": "List the performance scores of various methods on the MNIST (MNIST) dataset on the Image Generation task using metric FID.",
  "answer": "| Method | FID |\n| --- | --- |\n| Sliced Iterative Generator | 4.5 |\n| GLF+perceptual loss (ours) | 5.8 |\n| PeerGAN | 7.87 |\n| HypGAN | 12.884 |\n| Feature Alignment | 38.53 |\n| PresGAN | 42.019 |",
  "src_docs": [
   "1910.04302",
   "2007.00674",
   "2106.12562",
   "2101.07524",
   "2102.05567",
   "1905.10485"
  ],
  "updated_answer": [
   [
    "2007.00674",
    "Sliced Iterative Generator",
    "4.5",
    4.5
   ],
   [
    "1905.10485",
    "GLF+perceptual loss (ours)",
    "5.8",
    5.8
   ],
   [
    "2101.07524",
    "PeerGAN",
    "7.87",
    7.87
   ],
   [
    "2102.05567",
    "HypGAN",
    "12.884",
    12.884
   ],
   [
    "2106.12562",
    "Feature Alignment",
    "38.53",
    38.53
   ],
   [
    "1910.04302",
    "PresGAN",
    "42.019",
    42.019
   ]
  ],
  "meta_info": {
   "datasets": "MNIST",
   "datasets_short": "MNIST",
   "task": "Image Generation",
   "metric": "FID"
  },
  "updated_answer2": [
   [
    "2007.00674",
    "Sliced Iterative Generator",
    "4.5",
    4.5
   ],
   [
    "1905.10485",
    "GLF+perceptual loss (ours)",
    "5.8",
    5.8
   ],
   [
    "2101.07524",
    "PeerGAN",
    "7.87",
    7.87
   ],
   [
    "2102.05567",
    "HypGAN",
    "12.884",
    12.884
   ],
   [
    "2106.12562",
    "Feature Alignment",
    "38.53",
    38.53
   ],
   [
    "1910.04302",
    "PresGAN",
    "42.019",
    42.019
   ]
  ]
 },
 "./longdocdata/docs/42.json": {
  "question": "List the performance scores of various methods on the MNIST (MNIST) dataset on the Image Generation task using metric bits/dimension.",
  "answer": "| Method | bits/dimension |\n| --- | --- |\n| Locally Masked PixelCNN (8 orders) | 0.65 |\n| Residual Flow | 0.97 |\n| RNODE | 0.97 |\n| MintNet | 0.98 |\n| i-ResNet | 1.06 |\n| Sliced Iterative Generator | 1.34 |",
  "src_docs": [
   "2007.00674",
   "2002.02798",
   "1906.02735",
   "2006.12486",
   "1811.00995",
   "1907.07945"
  ],
  "updated_answer": [
   [
    "2006.12486",
    "Locally Masked PixelCNN (8 orders)",
    "0.65",
    0.65
   ],
   [
    "1906.02735",
    "Residual Flow",
    "0.97",
    0.97
   ],
   [
    "2002.02798",
    "RNODE",
    "0.97",
    0.97
   ],
   [
    "1907.07945",
    "MintNet",
    "0.98",
    0.98
   ],
   [
    "1811.00995",
    "i-ResNet",
    "1.06",
    1.06
   ],
   [
    "2007.00674",
    "Sliced Iterative Generator",
    "1.34",
    1.34
   ]
  ],
  "meta_info": {
   "datasets": "MNIST",
   "datasets_short": "MNIST",
   "task": "Image Generation",
   "metric": "bits/dimension"
  },
  "updated_answer2": [
   [
    "2006.12486",
    "Locally Masked PixelCNN (8 orders)",
    "0.65",
    0.65
   ],
   [
    "1906.02735",
    "Residual Flow",
    "0.97",
    0.97
   ],
   [
    "2002.02798",
    "RNODE",
    "0.97",
    0.97
   ],
   [
    "1907.07945",
    "MintNet",
    "0.98",
    0.98
   ],
   [
    "1811.00995",
    "i-ResNet",
    "1.06",
    1.06
   ],
   [
    "2007.00674",
    "Sliced Iterative Generator",
    "1.34",
    1.34
   ]
  ]
 },
 "./longdocdata/docs/52.json": {
  "question": "List the performance scores of various methods on the 75 Superpixel MNIST (MNIST) dataset on the Superpixel Image Classification task using metric Classification Error.",
  "answer": "| Method | Classification Error |\n| --- | --- |\n| Dynamic Reduction Network (256 HD) | 0.95 |\n| PNCNN | 1.24 |\n| GAT | 3.81 |\n| GCGP | 4.2 |\n| SplineCNN | 4.78 |\n| Monet | 8.89 |",
  "src_docs": [
   "2002.05544",
   "1611.08402",
   "2003.08013",
   "2010.10876",
   "1711.08920",
   "1905.05739"
  ],
  "updated_answer": [
   [
    "2003.08013",
    "Dynamic Reduction Network (256 HD)",
    "0.95",
    0.95
   ],
   [
    "2010.10876",
    "PNCNN",
    "1.24",
    1.24
   ],
   [
    "2002.05544",
    "GAT",
    "3.81",
    3.81
   ],
   [
    "1905.05739",
    "GCGP",
    "4.2",
    4.2
   ],
   [
    "1711.08920",
    "SplineCNN",
    "4.78",
    4.78
   ],
   [
    "1611.08402",
    "Monet",
    "8.89",
    8.89
   ]
  ],
  "meta_info": {
   "datasets": "75 Superpixel MNIST",
   "datasets_short": "MNIST",
   "task": "Superpixel Image Classification",
   "metric": "Classification Error"
  },
  "updated_answer2": [
   [
    "2003.08013",
    "Dynamic Reduction Network (256 HD)",
    "0.95",
    0.95
   ],
   [
    "2010.10876",
    "PNCNN",
    "1.24",
    1.24
   ],
   [
    "2002.05544",
    "GAT",
    "3.81",
    3.81
   ],
   [
    "1905.05739",
    "GCGP",
    "4.2",
    4.2
   ],
   [
    "1711.08920",
    "SplineCNN",
    "4.78",
    4.78
   ],
   [
    "1611.08402",
    "Monet",
    "8.89",
    8.89
   ]
  ]
 },
 "./longdocdata/docs/152.json": {
  "question": "List the performance scores of various methods on the ImageNet (ImageNet) dataset on the Classification with Binary Neural Network task using metric Top-1 Accuracy.",
  "answer": "| Method | Top-1 Accuracy |\n| --- | --- |\n| Ours++ | 71.2 |\n| BATS | 66.1 |\n| Real-to-Bin (ours)** | 65.4 |\n| ResNet-18 | 57.1 |\n| ResNet-18 | 55.6 |\n| Improved Binary Network (ResNet-18) | 53.7 |\n| XNOR-Network | 51.2 |\n| Improved Binary Network (AlexNet) | 48.6 |",
  "src_docs": [
   "1904.05868",
   "1603.05279",
   "2003.01711",
   "1904.07852",
   "2003.11535",
   "1909.13863",
   "2010.03558"
  ],
  "updated_answer": [
   [
    "2010.03558",
    "Ours++",
    "71.2",
    71.2
   ],
   [
    "2003.01711",
    "BATS",
    "66.1",
    66.1
   ],
   [
    "2003.11535",
    "Real-to-Bin (ours)**",
    "65.4",
    65.4
   ],
   [
    "1909.13863",
    "ResNet-18",
    "57.1",
    57.1
   ],
   [
    "1904.07852",
    "ResNet-18",
    "55.6",
    55.6
   ],
   [
    "1904.05868",
    "Improved Binary Network (ResNet-18)",
    "53.7",
    53.7
   ],
   [
    "1603.05279",
    "XNOR-Network",
    "51.2",
    51.2
   ],
   [
    "1904.05868",
    "Improved Binary Network (AlexNet)",
    "48.6",
    48.6
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet",
   "datasets_short": "ImageNet",
   "task": "Classification with Binary Neural Network",
   "metric": "Top-1 Accuracy"
  },
  "updated_answer2": [
   [
    "2010.03558",
    "Ours++",
    "71.2",
    71.2
   ],
   [
    "2003.01711",
    "BATS",
    "66.1",
    66.1
   ],
   [
    "2003.11535",
    "Real-to-Bin (ours)**",
    "65.4",
    65.4
   ],
   [
    "1909.13863",
    "ResNet-18",
    "57.1",
    57.1
   ],
   [
    "1904.07852",
    "ResNet-18",
    "55.6",
    55.6
   ],
   [
    "1904.05868",
    "Improved Binary Network (ResNet-18)",
    "53.7",
    53.7
   ],
   [
    "1603.05279",
    "XNOR-Network",
    "51.2",
    51.2
   ]
  ]
 },
 
 "./longdocdata/docs/172.json": {
  "question": "List the performance scores of various methods on the ImageNet (ImageNet) dataset on the Unsupervised Image Classification task using metric Accuracy (%).",
  "answer": "| Method | Accuracy (%) |\n| --- | --- |\n| SimCLRv2 ResNet-152 + SK (PCA+k-means, 1500 clusters) | 46.03 |\n| iBOT (ViT-S/16) | 43.4 |\n| Self-Classifier (ResNet-50) | 41.1 |\n| TWIST (ResNet-50) | 40.6 |\n| SCAN (ResNet-50) | 39.9 |\n| SimCLRv2 ResNet-152 + SK (PCA+k-means) | 39.07 |\n| DeepDPM (nonparametric) | 25.0 |",
  "src_docs": [
   "2203.14309",
   "2111.07832",
   "2110.07402",
   "2103.10994",
   "2008.10312",
   "2005.12320"
  ],
  "updated_answer": [
    [
        "2008.10312",
        "SimCLRv2 ResNet-152 + SK",
        "46.03",
        46.03
    ],
   [
    "2111.07832",
    "iBOT",
    "43.4",
    43.4
   ],
   [
    "2103.10994",
    "Self-Classifier",
    "41.1",
    41.1
   ],
   [
    "2110.07402",
    "TWIST",
    "40.6",
    40.6
   ],

   [
    "2005.12320",
    "SCAN",
    "39.9",
    39.9
   ],

   [
    "2203.14309",
    "DeepDPM (nonparametric)",
    "25",
    25.0
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet",
   "datasets_short": "ImageNet",
   "task": "Unsupervised Image Classification",
   "metric": "Accuracy (%)"
  },
  "updated_answer2": [
   [
    "2008.10312",
    "SimCLRv2 ResNet-152 + SK (PCA+k-means, 1500 clusters)",
    "46.03Â±0.21",
    46.03
   ],
   [
    "2111.07832",
    "iBOT (ViT-S/16)",
    "43.4",
    43.4
   ],
   [
    "2103.10994",
    "Self-Classifier (ResNet-50)",
    "41.1",
    41.1
   ],
   [
    "2110.07402",
    "TWIST (ResNet-50)",
    "40.6",
    40.6
   ],
   [
    "2005.12320",
    "SCAN (ResNet-50)",
    "39.9",
    39.9
   ],
   [
    "2203.14309",
    "DeepDPM (nonparametric)",
    "25",
    25.0
   ]
  ]
 },
 "./longdocdata/docs/173.json": {
  "question": "List the performance scores of various methods on the ImageNet (ImageNet) dataset on the Unsupervised Image Classification task using metric ARI.",
  "answer": "| Method | ARI |\n| --- | --- |\n| iBOT (ViT-S/16) | 32.8 |\n| TWIST (ResNet-50) | 30.0 |\n| Self-Classifier (ResNet-50) | 29.5 |\n| SCAN (ResNet-50) | 27.5 |\n| SimCLRv2 ResNet-152 + SK (PCA+k-means, 1500 clusters) | 23.94 |\n| SimCLRv2 ResNet-152 + SK (PCA+k-means) | 22.8 |\n| DeepDPM (nonparametric) | 14.0 |",
  "src_docs": [
   "2203.14309",
   "2111.07832",
   "2110.07402",
   "2103.10994",
   "2008.10312",
   "2005.12320"
  ],
  "updated_answer": [
   [
    "2111.07832",
    "iBOT",
    "32.8",
    32.8
   ],
   [
    "2110.07402",
    "TWIST",
    "30.0",
    30.0
   ],
   [
    "2103.10994",
    "Self-Classifier",
    "29.5",
    29.5
   ],
   [
    "2005.12320",
    "SCAN",
    "27.5",
    27.5
   ],
   [
    "2008.10312",
    "SimCLRv2 ResNet-152 + SK",
    "23.94Â±0.16",
    23.94
   ],
   [
    "2203.14309",
    "DeepDPM (nonparametric)",
    "14",
    14.0
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet",
   "datasets_short": "ImageNet",
   "task": "Unsupervised Image Classification",
   "metric": "ARI"
  },
  "updated_answer2": [
   [
    "2111.07832",
    "iBOT (ViT-S/16)",
    "32.8",
    32.8
   ],
   [
    "2110.07402",
    "TWIST (ResNet-50)",
    "30.0",
    30.0
   ],
   [
    "2103.10994",
    "Self-Classifier (ResNet-50)",
    "29.5",
    29.5
   ],
   [
    "2005.12320",
    "SCAN (ResNet-50)",
    "27.5",
    27.5
   ],
   [
    "2008.10312",
    "SimCLRv2 ResNet-152 + SK (PCA+k-means, 1500 clusters)",
    "23.94Â±0.16",
    23.94
   ],
   [
    "2203.14309",
    "DeepDPM (nonparametric)",
    "14",
    14.0
   ]
  ]
 },
 
 
 
 "./longdocdata/docs/214.json": {
  "question": "List the performance scores of various methods on the ImageNet (ImageNet) dataset on the Image Classification task using metric Operations per network pass.",
  "answer": "| Method | Operations per network pass |\n| --- | --- |\n| PNASNet-5 | 2.5 |\n| NASNET-A(6) | 2.38 |\n| Oct-ResNet-152 (SE) | 2.22 |\n| Xception | 0.838 |\n| SCARLET-A4 | 0.42 |\n| MnasNet-A3 | 0.0403 |\n| MoGA-A | 0.0304 |",
  "src_docs": [
   "1707.07012",
   "1807.11626",
   "1610.02357",
   "1712.00559",
   "1908.01314",
   "1904.05049",
   "1908.06022"
  ],
  "updated_answer": [
   [
    "1904.05049",
    "Oct-ResNet-152 (SE)",
    "2.22G",
    2.22
   ],
   [
    "1712.00559",
    "PNASNet-5",
    "2.5G",
    2.5
   ],
   [
    "1707.07012",
    "NASNET-A(6)",
    "2.38G",
    2.38
   ],
   [
    "1908.06022",
    "SCARLET-A4",
    "0.42G",
    0.42
   ],
   [
    "1610.02357",
    "Xception",
    "0.838G",
    0.838
   ],
   [
    "1807.11626",
    "MnasNet-A3",
    "0.0403G",
    0.0403
   ],
   [
    "1908.01314",
    "MoGA-A",
    "0.0304G",
    0.0304
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet",
   "datasets_short": "ImageNet",
   "task": "Image Classification",
   "metric": "Operations per network pass (GFLOPs)"
  },
  "updated_answer2": [
   [
    "1712.00559",
    "PNASNet-5",
    "2.5G",
    2.5
   ],
   [
    "1707.07012",
    "NASNET-A(6)",
    "2.38G",
    2.38
   ],
   [
    "1904.05049",
    "Oct-ResNet-152 (SE)",
    "2.22G",
    2.22
   ],
   [
    "1610.02357",
    "Xception",
    "0.838G",
    0.838
   ],
   [
    "1908.06022",
    "SCARLET-A4",
    "0.42G",
    0.42
   ],
   [
    "1807.11626",
    "MnasNet-A3",
    "0.0403G",
    0.0403
   ],
   [
    "1908.01314",
    "MoGA-A",
    "0.0304G",
    0.0304
   ]
  ]
 },
 
 "./longdocdata/docs/256.json": {
  "question": "List the performance scores of various methods on the ImageNet 64x64 (ImageNet) dataset on the Image Generation task using metric FID.",
  "answer": "| Method | FID |\n| --- | --- |\n| CDM | 1.48 |\n| StyleGAN-XL | 1.51 |\n| ADM (dropout) | 2.07 |\n| Improved DDPM | 2.92 |\n| PGMGAN | 21.73 |",
  "src_docs": [
   "2202.00273",
   "2102.09672",
   "2106.15282",
   "2105.05233",
   "2104.00816"
  ],
  "updated_answer": [
   [
    "2106.15282",
    "CDM",
    "1.48",
    1.48
   ],
   [
    "2202.00273",
    "StyleGAN-XL",
    "1.51",
    1.51
   ],
   [
    "2105.05233",
    "ADM (dropout)",
    "2.07",
    2.07
   ],
   [
    "2102.09672",
    "Improved DDPM",
    "2.92",
    2.92
   ],
   [
    "2104.00816",
    "PGMGAN",
    "21.73",
    21.73
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet 64x64 (train set)",
   "datasets_short": "ImageNet",
   "task": "Image Generation",
   "metric": "FID"
  },
  "updated_answer2": [
   [
    "2106.15282",
    "CDM",
    "1.48",
    1.48
   ],
   [
    "2202.00273",
    "StyleGAN-XL",
    "1.51",
    1.51
   ],
   [
    "2105.05233",
    "ADM (dropout)",
    "2.07",
    2.07
   ],
   [
    "2102.09672",
    "Improved DDPM",
    "2.92",
    2.92
   ],
   [
    "2104.00816",
    "PGMGAN",
    "21.73",
    21.73
   ]
  ]
 },
 "./longdocdata/docs/262.json": {
  "question": "List the performance scores of various methods on the ImageNet 128x128 (ImageNet) dataset on the Image Generation task using metric IS.",
  "answer": "| Method | IS |\n| --- | --- |\n| BIGRoC-gt (Guided-Diffusion) | 169.73 |\n| BIGRoC-pl (Guided-Diffusion) | 150.43 |\n| BigGAN-deep | 124.5 |\n| LeCAM + DA | 108.0 |\n| DiffAugment-BigGAN | 100.8 |\n| BigGAN | 98.8 |\n| PGMGAN | 23.31 |",
  "src_docs": [
   "2006.10738",
   "1809.11096",
   "2104.03310",
   "2108.03702",
   "2104.00816"
  ],
  "updated_answer": [
   [
    "2108.03702",
    "BIGRoC-gt (BigGAN-deep)",
    "226.17",
    226.17
   ],
   [
    "1809.11096",
    "BigGAN-deep",
    "124.5",
    124.5
   ],
   [
    "2104.03310",
    "LeCAM + DA",
    "108",
    108.0
   ],
   [
    "2006.10738",
    "DiffAugment-BigGAN",
    "100.8",
    100.8
   ],
   [
    "1809.11096",
    "BigGAN",
    "98.8",
    98.8
   ],
   [
    "2104.00816",
    "PGMGAN",
    "23.31",
    23.31
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet 128x128",
   "datasets_short": "ImageNet",
   "task": "Image Generation",
   "metric": "IS"
  },
  "updated_answer2": [
    [
        "2108.03702",
        "BIGRoC-gt (BigGAN-deep)",
        "226.17",
        226.17
    ],
   [
    "1809.11096",
    "BigGAN-deep",
    "124.5",
    124.5
   ],
   [
    "2104.03310",
    "LeCAM + DA",
    "108",
    108.0
   ],
   [
    "2006.10738",
    "DiffAugment-BigGAN",
    "100.8",
    100.8
   ],
   [
    "2104.00816",
    "PGMGAN",
    "23.31",
    23.31
   ]
  ]
 },
 "./longdocdata/docs/300.json": {
  "question": "List the performance scores of various methods on the Imagenet-dog-15 (ImageNet) dataset on the Image Clustering task using metric ARI.",
  "answer": "| Method | ARI |\n| --- | --- |\n| ProPos* | 0.675 |\n| ProPos | 0.627 |\n| ConCURL | 0.531 |\n| SPICE | 0.526 |\n| IDFD | 0.413 |\n| MiCE | 0.286 |\n| CC | 0.274 |",
  "src_docs": [
   "2105.01289",
   "2111.11821",
   "2105.01899",
   "2009.09687",
   "2103.09382",
   "2106.00131"
  ],
  "updated_answer": [
   [
    "2111.11821",
    "ProPos",
    "0.675",
    0.675
   ],
   [
    "2105.01289",
    "ConCURL",
    "0.531",
    0.531
   ],
   [
    "2103.09382",
    "SPICE",
    "0.526",
    0.526
   ],
   [
    "2106.00131",
    "IDFD",
    "0.413",
    0.413
   ],
   [
    "2105.01899",
    "MiCE",
    "0.286",
    0.286
   ],
   [
    "2009.09687",
    "CC",
    "0.274",
    0.274
   ]
  ],
  "meta_info": {
   "datasets": "Imagenet-dog-15",
   "datasets_short": "ImageNet",
   "task": "Image Clustering",
   "metric": "ARI"
  },
  "updated_answer2": [
   [
    "2111.11821",
    "ProPos",
    "0.675",
    0.675
   ],
   [
    "2105.01289",
    "ConCURL",
    "0.531",
    0.531
   ],
   [
    "2103.09382",
    "SPICE",
    "0.526",
    0.526
   ],
   [
    "2106.00131",
    "IDFD",
    "0.413",
    0.413
   ],
   [
    "2105.01899",
    "MiCE",
    "0.286",
    0.286
   ],
   [
    "2009.09687",
    "CC",
    "0.274",
    0.274
   ]
  ]
 },
 "./longdocdata/docs/339.json": {
  "question": "List the performance scores of various methods on the WikiSQL (WikiSQL) dataset on the Code Generation task using metric Execution Accuracy.",
  "answer": "| Method | Execution Accuracy |\n| --- | --- |\n| NL2SQL-RULE | 89.2 |\n| TypeSQL+TC (Yu et al., 2018)+ | 82.6 |\n| Tranx | 78.6 |\n| STAMP+RL (Sun et al., 2018)+ | 74.6 |\n| STAMP (Sun et al., 2018)+ | 74.4 |\n| TypeSQL (Yu et al., 2018) | 73.5 |\n| PT-MAML (Huang et al., 2018) | 68.0 |\n| Seq2SQL (Zhong et al., 2017) | 59.4 |\n| Seq2Seq (Zhong et al., 2017) | 35.9 |",
  "src_docs": [
   "1910.07179",
   "1810.02720",
   "1804.09769",
   "1803.02400",
   "1804.08338",
   "1709.00103"
  ],
  "updated_answer": [
   [
    "1910.07179",
    "NL2SQL-RULE+EG",
    "90.1",
    90.1
   ],
   [
    "1804.09769",
    "TypeSQL+TC",
    "82.6",
    82.6
   ],
   [
    "1810.02720",
    "Tranx",
    "78.6",
    78.6
   ],
   [
    "1804.08338",
    "STAMP+RL",
    "74.6",
    74.6
   ],
   [
    "1804.08338",
    "STAMP+",
    "74.4",
    74.4
   ],
   [
    "1804.09769",
    "TypeSQL",
    "73.5",
    73.5
   ],
   [
    "1803.02400",
    "PT-MAML",
    "68.0",
    68.0
   ],
   [
    "1709.00103",
    "Seq2SQL",
    "59.4",
    59.4
   ],
   [
    "1709.00103",
    "Seq2Seq",
    "35.9",
    35.9
   ]
  ],
  "meta_info": {
   "datasets": "WikiSQL test set",
   "datasets_short": "WikiSQL",
   "task": "Code Generation",
   "metric": "Execution Accuracy (%)"
  },
  "updated_answer2": [
   [
    "1910.07179",
    "NL2SQL-RULE",
    "90.1",
    90.1
   ],
   [
    "1804.09769",
    "TypeSQL+TC",
    "82.6",
    82.6
   ],
   [
    "1810.02720",
    "Tranx",
    "78.6",
    78.6
   ],
   [
    "1804.08338",
    "STAMP+RL",
    "74.6",
    74.6
   ],
   [
    "1803.02400",
    "PT-MAML",
    "68.0",
    68.0
   ],
   [
    "1709.00103",
    "Seq2SQL",
    "59.4",
    59.4
   ]
  ]
 },
 "./longdocdata/docs/351.json": {
  "question": "List the performance scores of various methods on the Food-101 (Food-101) dataset on the Fine-Grained Image Classification task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| EffNet-L2 (SAM) | 96.18 |\n| ALIGN | 95.88 |\n| Grafit (RegNet-8GF) | 93.7 |\n| EfficientNet-B7 | 93.0 |\n| Assemble-ResNet-FGVC-50 | 92.5 |\n| NAT-M4 | 89.4 |\n| NAT-M3 | 89.0 |\n| NAT-M2 | 88.5 |\n| NAT-M1 | 87.4 |",
  "src_docs": [
   "2011.12982",
   "2010.01412",
   "2001.06268",
   "2005.05859",
   "2102.05918",
   "1905.11946"
  ],
  "updated_answer": [
   [
    "2010.01412",
    "EffNet-L2 (SAM)",
    "96.18",
    96.18
   ],
   [
    "2102.05918",
    "ALIGN",
    "95.88",
    95.88
   ],
   [
    "2011.12982",
    "Grafit (RegNet-8GF)",
    "93.7",
    93.7
   ],
   [
    "1905.11946",
    "EfficientNet-B7",
    "93.0",
    93.0
   ],
   [
    "2001.06268",
    "Assemble-ResNet-FGVC-50",
    "92.5",
    92.5
   ],
   [
    "2005.05859",
    "NAT-M4",
    "89.4",
    89.4
   ],
   [
    "2005.05859",
    "NAT-M3",
    "89.0",
    89.0
   ],
   [
    "2005.05859",
    "NAT-M2",
    "88.5",
    88.5
   ],
   [
    "2005.05859",
    "NAT-M1",
    "87.4",
    87.4
   ]
  ],
  "meta_info": {
   "datasets": "Food-101",
   "datasets_short": "Food-101",
   "task": "Fine-Grained Image Classification",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2010.01412",
    "EffNet-L2 (SAM)",
    "96.18",
    96.18
   ],
   [
    "2102.05918",
    "ALIGN",
    "95.88",
    95.88
   ],
   [
    "2011.12982",
    "Grafit (RegNet-8GF)",
    "93.7",
    93.7
   ],
   [
    "1905.11946",
    "EfficientNet-B7",
    "93.0",
    93.0
   ],
   [
    "2001.06268",
    "Assemble-ResNet-FGVC-50",
    "92.5",
    92.5
   ],
   [
    "2005.05859",
    "NAT-M4",
    "89.4",
    89.4
   ]
  ]
 },

 
 "./longdocdata/docs/429.json": {
  "question": "List the performance scores of various methods on the Flickr (Flickr30k) dataset on the Node Classification task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| GCN+GAugM (Zhao et al., 2021) | 0.682 |\n| DEMO-Net(weight) | 0.656 |\n| GraphSAGE (Hamilton et al., [2017a]) | 0.641 |\n| Intersection (Li et al., 2018) | 0.557 |\n| GCN (Kipf and Welling, 2017) | 0.546 |\n| GCN_cheby (Kipf and Welling, 2017) | 0.479 |\n| GAT (Velickovic et al., 2018) | 0.359 |",
  "src_docs": [
   "1801.07606",
   "1906.02319",
   "1706.02216",
   "1609.02907",
   "1710.10903",
   "2006.06830"
  ],
  "updated_answer": [
   [
    "2006.06830",
    "GCN+GAugM",
    "0.682",
    0.682
   ],
   [
    "1906.02319",
    "DEMO-Net(hash)",
    "0.678",
    0.678
   ],
   [
    "1706.02216",
    "GraphSAGE",
    "0.641",
    0.641
   ],
   [
    "1801.07606",
    "Intersection",
    "0.557",
    0.557
   ],
   [
    "1609.02907",
    "GCN",
    "0.546",
    0.546
   ],
   [
    "1609.02907",
    "GCN_cheby",
    "0.479",
    0.479
   ],
   [
    "1710.10903",
    "GAT",
    "0.359",
    0.359
   ]
  ],
  "meta_info": {
   "datasets": "Flickr",
   "datasets_short": "Flickr30k",
   "task": "Node Classification",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2006.06830",
    "GCN+GAug-M",
    "0.682",
    0.682
   ],
   [
    "1906.02319",
    "DEMO-Net(hash)",
    "0.678",
    0.678
   ],
   [
    "1706.02216",
    "GraphSAGE",
    "0.641",
    0.641
   ],
   [
    "1801.07606",
    "Intersection",
    "0.557",
    0.557
   ],
   [
    "1609.02907",
    "GCN",
    "0.546",
    0.546
   ],
   [
    "1710.10903",
    "GAT",
    "0.359",
    0.359
   ]
  ]
 },
 "./longdocdata/docs/446.json": {
  "question": "List the performance scores of various methods on the COCO minival (COCO) dataset on the Instance Segmentation task using metric APM.",
  "answer": "| Method | APM |\n| --- | --- |\n| QueryInst (single scale) | 52.6 |\n| R3-CNN (ResNet-50-FPN, DCN) | 43.6 |\n| R3-CNN (ResNet-50-FPN, GC-Net) | 42.8 |\n| R3-CNN (ResNet-50-FPN, GRoIE) | 42.1 |\n| Mask R-CNN-FPN (ResNeXt-101, GN+WS) | 41.73 |\n| R3-CNN (ResNet-50-FPN) | 41.0 |\n| GCnet (ResNet-50-FPN, GRoIE) | 41.0 |\n| Mask R-CNN (FPN, X-volution, SA) | 40.0 |\n| Mask R-CNN (ResNet-50-FPN, GRoIE) | 39.0 |\n| Faster R-CNN (Res2Net-50) | 37.9 |",
  "src_docs": [
   "2104.01329",
   "2004.13665",
   "1904.01169",
   "1903.10520",
   "2106.02253",
   "2105.01928"
  ],
  "updated_answer": [
   [
    "2105.01928",
    "QueryInst (single scale)",
    "52.6",
    52.6
   ],
   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN, DCN)",
    "43.6",
    43.6
   ],
   [
    "1903.10520",
    "Mask R-CNN-FPN (ResNeXt-101, WS+BCN)",
    "42.1",
    42.1
   ],
   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN)",
    "41.0",
    41.0
   ],
   [
    "2004.13665",
    "GCnet (ResNet-50-FPN, GRoIE)",
    "41.0",
    41.0
   ],
   [
    "2106.02253",
    "Mask R-CNN (FPN, X-volution, SA)",
    "40",
    40.0
   ],
   [
    "2004.13665",
    "Mask R-CNN (ResNet-50-FPN, GRoIE)",
    "39",
    39.0
   ],
   [
    "1904.01169",
    "Faster R-CNN (Res2Net-50)",
    "37.9",
    37.9
   ]
  ],
  "meta_info": {
   "datasets": "COCO minival",
   "datasets_short": "COCO",
   "task": "Instance Segmentation",
   "metric": "APM"
  },
  "updated_answer2": [
   [
    "2105.01928",
    "QueryInst (single scale)",
    "52.6",
    52.6
   ],
   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN, DCN)",
    "43.6",
    43.6
   ],
   [
    "1903.10520",
    "Mask R-CNN-FPN (ResNeXt-101, WS+BCN)",
    "42.1",
    42.1
   ],
   [
    "2004.13665",
    "GCnet (ResNet-50-FPN, GRoIE)",
    "41.0",
    41.0
   ],
   [
    "2106.02253",
    "Mask R-CNN (FPN, X-volution, SA)",
    "40",
    40.0
   ],
   [
    "1904.01169",
    "Faster R-CNN (Res2Net-50)",
    "37.9",
    37.9
   ]
  ]
 },
 "./longdocdata/docs/448.json": {
  "question": "List the performance scores of various methods on the COCO minival (COCO) dataset on the Instance Segmentation task using metric APS.",
  "answer": "| Method | APS |\n| --- | --- |\n| QueryInst (single scale) | 30.8 |\n| R3-CNN (ResNet-50-FPN, GC-Net) | 22.6 |\n| R3-CNN (ResNet-50-FPN, DCN) | 22.3 |\n| R3-CNN (ResNet-50-FPN, GRoIE) | 20.7 |\n| R3-CNN (ResNet-50-FPN) | 20.4 |\n| GCnet (ResNet-50-FPN, GRoIE) | 20.2 |\n| Mask R-CNN (FPN, X-volution, SA) | 19.2 |\n| Mask R-CNN (ResNet-50-FPN, GRoIE) | 19.1 |\n| Mask R-CNN-FPN (ResNeXt-101, GN+WS) | 18.32 |\n| Faster R-CNN (Res2Net-50) | 15.7 |",
  "src_docs": [
   "2104.01329",
   "2004.13665",
   "1904.01169",
   "1903.10520",
   "2106.02253",
   "2105.01928"
  ],
  "updated_answer": [
   [
    "2105.01928",
    "QueryInst",
    "31.5",
    31.5
   ],

   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN, GC-Net)",
    "22.6",
    22.6
   ],
   [
    "2004.13665",
    "GCnet (ResNet-50-FPN, GRoIE)",
    "20.4",
    20.4
   ],
   [
    "1903.10520",
    "Mask R-CNN-FPN (ResNeXt-101, GN+WS)",
    "19.1",
    19.1
   ],

   [
    "2106.02253",
    "Mask R-CNN (FPN, X-volution, SA)",
    "19.2",
    19.2
   ],
   [
    "2004.13665",
    "Mask R-CNN (ResNet-50-FPN, GRoIE)",
    "19.1",
    19.1
   ],
   [
    "1904.01169",
    "Faster R-CNN (Res2Net-50)",
    "15.7",
    15.7
   ]
  ],
  "meta_info": {
   "datasets": "COCO minival",
   "datasets_short": "COCO",
   "task": "Instance Segmentation",
   "metric": "APS"
  },
  "updated_answer2": [
   [
    "2105.01928",
    "QueryInst",
    "31.5",
    31.5
   ],
   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN, GC-Net)",
    "22.6",
    22.6
   ],
   [
    "2004.13665",
    "GCnet (ResNet-50-FPN, GRoIE)",
    "20.4",
    20.4
   ],
   [
    "2106.02253",
    "Mask R-CNN (FPN, X-volution, SA)",
    "19.2",
    19.2
   ],
   [
    "1903.10520",
    "Mask R-CNN-FPN (ResNeXt-101, GN+WS)",
    "18.32",
    18.32
   ],
   [
    "1904.01169",
    "Faster R-CNN (Res2Net-50)",
    "15.7",
    15.7
   ]
  ]
 },
 "./longdocdata/docs/450.json": {
  "question": "List the performance scores of various methods on the COCO minival (COCO) dataset on the Instance Segmentation task using metric APL.",
  "answer": "| Method | APL |\n| --- | --- |\n| QueryInst (single scale) | 68.3 |\n| R3-CNN (ResNet-50-FPN, DCN) | 56.1 |\n| Mask R-CNN-FPN (ResNeXt-101, GN+WS) | 56.08 |\n| R3-CNN (ResNet-50-FPN, GRoIE) | 54.3 |\n| Faster R-CNN (Res2Net-50) | 53.7 |\n| Mask R-CNN (FPN, X-volution, SA) | 53.1 |\n| R3-CNN (ResNet-50-FPN) | 52.8 |\n| GCnet (ResNet-50-FPN, GRoIE) | 51.2 |\n| Mask R-CNN (ResNet-50-FPN, GRoIE) | 48.7 |",
  "src_docs": [
   "2104.01329",
   "2004.13665",
   "1903.10520",
   "2106.02253",
   "1904.01169",
   "2105.01928"
  ],
  "updated_answer": [
   [
    "2105.01928",
    "QueryInst",
    "68.3",
    68.3
   ], 
     [
    "1903.10520",
    "Mask R-CNN-FPN (ResNeXt-101, WS+BCN)",
    "57.3",
    57.3
   ],
   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN, DCN)",
    "56.1",
    56.1
   ],   

   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN, GRoIE)",
    "54.3",
    54.3
   ],

   [
    "2104.01329",
    "R3-CNN (ResNet-50-FPN)",
    "52.8",
    52.8
   ],
   [
    "2004.13665",
    "GCnet (ResNet-50-FPN, GRoIE)",
    "51.2",
    51.2
   ],
   [
    "2106.02253",
    "Mask R-CNN (FPN, X-volution, SA)",
    "53.1",
    53.1
   ],
   [
    "2004.13665",
    "Mask R-CNN (ResNet-50-FPN, GRoIE)",
    "48.7",
    48.7
   ],
   [
    "1904.01169",
    "Faster R-CNN (Res2Net-50)",
    "53.7",
    53.7
   ]
  ],
  "meta_info": {
   "datasets": "COCO minival",
   "datasets_short": "COCO",
   "task": "Instance Segmentation",
   "metric": "APL"
  },
  "updated_answer2": [
    [
        "2105.01928",
        "QueryInst",
        "68.3",
        68.3
       ], 
         [
        "1903.10520",
        "Mask R-CNN-FPN (ResNeXt-101, GN+WS)",
        "57.3",
        57.3
       ],
       [
        "2104.01329",
        "R3-CNN (ResNet-50-FPN, DCN)",
        "56.1",
        56.1
       ],   
   [
    "1904.01169",
    "Faster R-CNN (Res2Net-50)",
    "53.7",
    53.7
   ],
   [
    "2106.02253",
    "Mask R-CNN (FPN, X-volution, SA)",
    "53.1",
    53.1
   ],
   [
    "2004.13665",
    "GCnet (ResNet-50-FPN, GRoIE)",
    "51.2",
    51.2
   ]
  ]
 },
 
 "./longdocdata/docs/495.json": {
  "question": "List the performance scores of various methods on the COCO test-dev (COCO) dataset on the Keypoint Detection task using metric AR50.",
  "answer": "| Method | AR50 |\n| --- | --- |\n| MSPN | 96.3 |\n| Simple Base+* | 95.8 |\n| CPN+ | 95.1 |\n| CPN | 95.1 |\n| AE | 89.5 |\n| G-RMI | 88.7 |\n| Simple Pose | 88.2 |\n| CMU Pose | 87.2 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1911.10529",
   "1611.05424",
   "1804.06208",
   "1611.08050",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "1804.06208",
    "Simple Base+*",
    "95.8",
    95.8
   ],
   [
    "1901.00148",
    "MSPN",
    "96.3",
    96.3
   ],
   [
    "1711.07319",
    "CPN",
    "95.1",
    95.1
   ],
   [
    "1911.10529",
    "Simple Pose",
    "88.6",
    88.6
   ],
   [
    "1701.01779",
    "G-RMI",
    "88.7",
    88.7
   ],
   [
    "1611.05424",
    "AE",
    "89.5",
    89.5
   ],
   [
    "1611.08050",
    "CMU Pose",
    "87.2",
    87.2
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-dev",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "AR50"
  },
  "updated_answer2": [
   [
    "1901.00148",
    "MSPN",
    "96.3",
    96.3
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "95.8",
    95.8
   ],
   [
    "1711.07319",
    "CPN+",
    "95.1",
    95.1
   ],
   [
    "1711.07319",
    "CPN",
    "95.1",
    95.1
   ],
   [
    "1701.01779",
    "G-RMI",
    "90.1",
    90.1
   ],
   [
    "1611.05424",
    "AE",
    "89.5",
    89.5
   ],
   [
    "1911.10529",
    "Simple Pose",
    "88.2",
    88.2
   ],
   [
    "1611.08050",
    "CMU Pose",
    "87.2",
    87.2
   ]
  ]
 },
 "./longdocdata/docs/496.json": {
  "question": "List the performance scores of various methods on the COCO test-dev (COCO) dataset on the Keypoint Detection task using metric ARM.",
  "answer": "| Method | ARM |\n| --- | --- |\n| MSPN | 77.5 |\n| Simple Base+* | 77.4 |\n| CPN+ | 74.8 |\n| CPN | 74.2 |\n| AE | 64.6 |\n| G-RMI | 64.4 |\n| CMU Pose | 60.6 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1611.05424",
   "1804.06208",
   "1611.08050",
   "1701.01779"
  ],
  "updated_answer": [
    [
        "1901.00148",
        "MSPN",
        "77.5",
        77.5
       ],
   [
    "1804.06208",
    "Simple Base+*",
    "77.4",
    77.4
   ],

   [
    "1711.07319",
    "CPN+",
    "74.8",
    74.8
   ],
   [
    "1711.07319",
    "CPN",
    "74.2",
    74.2
   ],
   [
    "1701.01779",
    "G-RMI",
    "64.4",
    64.4
   ],
   [
    "1611.05424",
    "AE",
    "64.6",
    64.6
   ],
   [
    "1611.08050",
    "CMU Pose",
    "60.6",
    60.6
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-dev",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "ARM"
  },
  "updated_answer2": [
   [
    "1901.00148",
    "MSPN",
    "77.5",
    77.5
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "77.4",
    77.4
   ],
   [
    "1711.07319",
    "CPN+",
    "74.8",
    74.8
   ],
   [
    "1611.05424",
    "AE",
    "64.6",
    64.6
   ],
   [
    "1701.01779",
    "G-RMI",
    "64.4",
    64.4
   ],
   [
    "1611.08050",
    "CMU Pose",
    "60.6",
    60.6
   ]
  ]
 },
 "./longdocdata/docs/499.json": {
  "question": "List the performance scores of various methods on the COCO test-dev (COCO) dataset on the Keypoint Detection task using metric AR75.",
  "answer": "| Method | AR75 |\n| --- | --- |\n| Simple Base+* | 88.2 |\n| MSPN | 88.1 |\n| CPN+ | 85.9 |\n| CPN | 85.3 |\n| AE | 76.0 |\n| G-RMI | 75.5 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1611.05424",
   "1804.06208",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "1804.06208",
    "Simple Base+*",
    "88.2",
    88.2
   ],
   [
    "1901.00148",
    "MSPN",
    "88.1",
    88.1
   ],
   [
    "1711.07319",
    "CPN+",
    "85.9",
    85.9
   ],
   [
    "1711.07319",
    "CPN",
    "85.3",
    85.3
   ],
   [
    "1701.01779",
    "G-RMI",
    "75.5",
    75.5
   ],
   [
    "1611.05424",
    "AE",
    "76.0",
    76.0
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-dev",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "AR75"
  },
  "updated_answer2": [
   [
    "1804.06208",
    "Simple Base+*",
    "88.2",
    88.2
   ],
   [
    "1901.00148",
    "MSPN",
    "88.1",
    88.1
   ],
   [
    "1711.07319",
    "CPN+",
    "85.9",
    85.9
   ],
   [
    "1611.05424",
    "AE",
    "76.0",
    76.0
   ],
   [
    "1701.01779",
    "G-RMI",
    "75.5",
    75.5
   ]
  ]
 },
 "./longdocdata/docs/502.json": {
  "question": "List the performance scores of various methods on the COCO test-dev (COCO) dataset on the Keypoint Detection task using metric ARL.",
  "answer": "| Method | ARL |\n| --- | --- |\n| Simple Base+* | 87.2 |\n| MSPN | 87.1 |\n| CPN+ | 84.6 |\n| CPN | 84.3 |\n| AE | 78.1 |\n| G-RMI | 77.1 |\n| CMU Pose | 74.6 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1611.05424",
   "1804.06208",
   "1611.08050",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "1804.06208",
    "Simple Base+*",
    "87.2",
    87.2
   ],
   [
    "1901.00148",
    "MSPN",
    "87.1",
    87.1
   ],
   [
    "1711.07319",
    "CPN+",
    "84.6",
    84.6
   ],
   [
    "1711.07319",
    "CPN",
    "84.3",
    84.3
   ],
   [
    "1701.01779",
    "G-RMI",
    "77.1",
    77.1
   ],
   [
    "1611.05424",
    "AE",
    "78.1",
    78.1
   ],
   [
    "1611.08050",
    "CMU Pose",
    "74.6",
    74.6
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-dev",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "ARL"
  },
  "updated_answer2": [
   [
    "1804.06208",
    "Simple Base+*",
    "87.2",
    87.2
   ],
   [
    "1901.00148",
    "MSPN",
    "87.1",
    87.1
   ],
   [
    "1711.07319",
    "CPN+",
    "84.6",
    84.6
   ],
   [
    "1611.05424",
    "AE",
    "78.1",
    78.1
   ],
   [
    "1701.01779",
    "G-RMI",
    "77.1",
    77.1
   ],
   [
    "1611.08050",
    "CMU Pose",
    "74.6",
    74.6
   ]
  ]
 },
 "./longdocdata/docs/527.json": {
  "question": "List the performance scores of various methods on the MS-COCO (COCO) dataset on the Zero-Shot Object Detection task using metric Recall.",
  "answer": "| Method | Recall |\n| --- | --- |\n| ZSD-RRFS | 62.3 |\n| SUZOD | 61.4 |\n| ContrastZSD | 59.5 |\n| BLC | 54.68 |\n| ZSD-Polarity Loss | 43.56 |",
  "src_docs": [
   "2010.09425",
   "1811.08982",
   "2109.06062",
   "2010.04502",
   "2201.00103"
  ],
  "updated_answer": [
   [
    "2201.00103",
    "ZSD-RRFS",
    "62.3",
    62.3
   ],
   [
    "2109.06062",
    "ContrastZSD",
    "59.50",
    59.5
   ],
   [
    "2010.09425",
    "SUZOD",
    "61.40",
    61.4
   ],
   [
    "2010.04502",
    "BLC",
    "54.68",
    54.68
   ],
   [
    "1811.08982",
    "ZSD-Polarity Loss",
    "43.56",
    43.56
   ]
  ],
  "meta_info": {
   "datasets": "MS-COCO",
   "datasets_short": "COCO",
   "task": "Zero-Shot Object Detection",
   "metric": "Recall"
  },
  "updated_answer2": [
   [
    "2201.00103",
    "ZSD-RRFS",
    "62.3",
    62.3
   ],
   [
    "2010.09425",
    "SUZOD",
    "61.40",
    61.4
   ],
   [
    "2109.06062",
    "ContrastZSD",
    "59.50",
    59.5
   ],
   [
    "2010.04502",
    "BLC",
    "54.68",
    54.68
   ],
   [
    "1811.08982",
    "ZSD-Polarity Loss",
    "43.56",
    43.56
   ]
  ]
 },
 "./longdocdata/docs/584.json": {
  "question": "List the performance scores of various methods on the COCO (COCO) dataset on the Multi-Person Pose Estimation task using metric Validation AP.",
  "answer": "| Method | Validation AP |\n| --- | --- |\n| OmniPose (WASPv2) | 79.5 |\n| EvoPose2D-L | 77.5 |\n| PoseFix | 77.3 |\n| BAPose | 72.7 |\n| OpenPifPaf | 71.0 |\n| LitePose-S (Ours) | 56.8 |",
  "src_docs": [
   "2103.10180",
   "2011.08446",
   "2103.02440",
   "2112.10716",
   "1812.03595",
   "2205.01271"
  ],
  "updated_answer": [
   [
    "2103.10180",
    "OmniPose (WASPv2)",
    "79.5",
    79.5
   ],
   [
    "2011.08446",
    "EvoPose2D-L",
    "77.5",
    77.5
   ],
   [
    "2112.10716",
    "BAPose",
    "72.7",
    72.7
   ],
   [
    "2103.02440",
    "OpenPifPaf",
    "71.0",
    71.0
   ],

   [
    "1812.03595",
    "PoseFix",
    "77.3",
    77.3
   ],
   [
    "2205.01271",
    "LitePose-S (Ours)",
    "56.8",
    56.8
   ]
  ],
  "meta_info": {
   "datasets": "COCO",
   "datasets_short": "COCO",
   "task": "Multi-Person Pose Estimation",
   "metric": "Validation AP"
  },
  "updated_answer2": [
   [
    "2103.10180",
    "OmniPose (WASPv2)",
    "79.5",
    79.5
   ],
   [
    "2011.08446",
    "EvoPose2D-L",
    "77.5",
    77.5
   ],
   [
    "2112.10716",
    "BAPose",
    "72.7",
    72.7
   ],
   [
    "1812.03595",
    "PoseFix",
    "77.3",
    77.3
   ],

   [
    "2103.02440",
    "OpenPifPaf",
    "71.0",
    71.0
   ],
   [
    "2205.01271",
    "LitePose-S (Ours)",
    "56.8",
    56.8
   ]
  ]
 },
 "./longdocdata/docs/585.json": {
  "question": "List the performance scores of various methods on the COCO (COCO) dataset on the Multi-Person Pose Estimation task using metric Test AP.",
  "answer": "| Method | Test AP |\n| --- | --- |\n| EvoPose2D-L | 76.8 |\n| PoseFix | 76.7 |\n| CenterGroup | 71.4 |\n| BAPose | 71.2 |\n| OpenPifPaf | 70.9 |\n| LitePose-S (Ours) | 56.7 |",
  "src_docs": [
   "2011.08446",
   "2103.02440",
   "2110.05132",
   "2112.10716",
   "1812.03595",
   "2205.01271"
  ],
  "updated_answer": [
    [
        "2011.08446",
        "EvoPose2D-L",
        "76.8",
        76.8
       ],
       [
        "1812.03595",
        "PoseFix",
        "74.9",
        74.9
       ],
   [
    "2112.10716",
    "BAPose",
    "71.2",
    71.2
   ],
   [
    "2110.05132",
    "CenterGroup",
    "71.4",
    71.4
   ],
   [
    "2103.02440",
    "OpenPifPaf",
    "70.9",
    70.9
   ],


   [
    "2205.01271",
    "LitePose-S (Ours)",
    "56.7",
    56.7
   ]
  ],
  "meta_info": {
   "datasets": "COCO",
   "datasets_short": "COCO",
   "task": "Multi-Person Pose Estimation",
   "metric": "Test AP"
  },
  "updated_answer2": [
    [
        "2011.08446",
        "EvoPose2D-L",
        "76.8",
        76.8
       ],
       [
        "1812.03595",
        "PoseFix",
        "74.9",
        74.9
       ],
   [
    "2112.10716",
    "BAPose",
    "71.2",
    71.2
   ],
   [
    "2103.02440",
    "OpenPifPaf",
    "70.9",
    70.9
   ],
   [
    "2205.01271",
    "LitePose-S (Ours)",
    "56.7",
    56.7
   ]
  ]
 },
 "./longdocdata/docs/608.json": {
  "question": "List the performance scores of various methods on the MSCOCO (COCO) dataset on the Real-time Instance Segmentation task using metric APM.",
  "answer": "| Method | APM |\n| --- | --- |\n| maYOLACT-700 (ResNet-50) | 40.8 |\n| SparseInst-608 (ResNet-50-vd) | 39.4 |\n| SipMask++ (ResNet-101, single-scale test) | 38.3 |\n| maYOLACT-550 (ResNet-50) | 38.0 |\n| SparseInst-448 (ResNet-50-vd) | 37.1 |\n| YOLACT-550++ (ResNet-101-FPN) | 36.8 |\n| SipMask (ResNet-101, single-scale test) | 35.6 |\n| CenterMask-Lite (ResNet-50-FPN) | 34.7 |\n| SipMask (ResNet-50, single-scale test) | 33.6 |\n| YOLACT-550 (ResNet-101-FPN) | 29.3 |\n| YOLACT | 25.3 |",
  "src_docs": [
   "2110.09734",
   "1904.02689",
   "1912.06218",
   "1911.06667",
   "2007.14772",
   "2203.12827"
  ],
  "updated_answer": [
    [
        "2110.09734",
        "maYOLACT-700 (ResNet-50)",
        "40.8",
        40.8
       ],
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "39.4",
    39.4
   ],

   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "38.3",
    38.3
   ],
   [
    "2110.09734",
    "maYOLACT-550 (ResNet-50)",
    "38.0",
    38.0
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "36.8",
    36.8
   ],
   [
    "1911.06667",
    "CenterMask-Lite (ResNet-50-FPN)",
    "34.7",
    34.7
   ],
   [
    "2007.14772",
    "SipMask (ResNet-101, single-scale test)",
    "35.6",
    35.6
   ],
   [
    "2007.14772",
    "SipMask (ResNet-50, single-scale test)",
    "33.6",
    33.6
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "29.3",
    29.3
   ],
   [
    "1904.02689",
    "YOLACT",
    "25.3",
    25.3
   ]
  ],
  "meta_info": {
   "datasets": "MSCOCO",
   "datasets_short": "COCO",
   "task": "Real-time Instance Segmentation",
   "metric": "APM"
  },
  "updated_answer2": [
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "40.8",
    40.8
   ],
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "39.4",
    39.4
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "38.3",
    38.3
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "36.8",
    36.8
   ],
   [
    "1911.06667",
    "CenterMask-Lite (ResNet-50-FPN)",
    "34.7",
    34.7
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "29.3",
    29.3
   ]
  ]
 },
 "./longdocdata/docs/610.json": {
  "question": "List the performance scores of various methods on the MSCOCO (COCO) dataset on the Real-time Instance Segmentation task using metric Frame (fps).",
  "answer": "| Method | Frame (fps) |\n| --- | --- |\n| SparseInst-448 (ResNet-50-vd) | 58.5 |\n| YOLACT | 45.3 |\n| SipMask (ResNet-50, single-scale test) | 41.7 |\n| SparseInst-608 (ResNet-50-vd) | 40.0 |\n| BlendMask-512 (DLA_34) | 33.3 |\n| YOLACT-550 (ResNet-101-FPN) | 33.3 |\n| SipMask (ResNet-101, single-scale test) | 31.3 |\n| SOLO-512 | 31.3 |\n| maYOLACT-550 (ResNet-50) | 30.0 |\n| YOLACT-550++ (ResNet-101-FPN) | 27.3 |\n| SipMask++ (ResNet-101, single-scale test) | 27.0 |\n| maYOLACT-700 (ResNet-50) | 25.0 |",
  "src_docs": [
   "1904.02689",
   "2110.09734",
   "1912.06218",
   "2003.10152",
   "2007.14772",
   "2001.00309",
   "2203.12827"
  ],
  "updated_answer": [
    [
        "2203.12827",
        "SparseInst-448 (ResNet-50-vd)",
        "58.5 (2080 Ti)",
        58.5
       ],
       [
        "1904.02689",
        "YOLACT",
        "45.3 (Titan Xp)",
        45.3
       ],
       [
        "2007.14772",
        "SipMask (ResNet-50, single-scale test)",
        "41.7 (Titan Xp)",
        41.7
       ],
   [
    "2203.12827",
    "SparseInst-448 (ResNet-50-vd)",
    "58.5 (2080 Ti)",
    58.5
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "27.0 (Titan Xp)",
    27.0
   ],
   [
    "2110.09734",
    "maYOLACT-550 (ResNet-50)",
    "30 (Tesla V100)",
    30.0
   ],
   [
    "2001.00309",
    "BlendMask-512 (DLA_34)",
    "33.3",
    33.3
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "27.3 (Titan Xp)",
    27.3
   ],
   [
    "2007.14772",
    "SipMask (ResNet-101, single-scale test)",
    "31.3 (Titan Xp)",
    31.3
   ],
   [
    "2007.14772",
    "SipMask (ResNet-50, single-scale test)",
    "41.7 (Titan Xp)",
    41.7
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "33.3 (Titan Xp)",
    33.3
   ],
   [
    "1904.02689",
    "YOLACT",
    "45.3 (Titan Xp)",
    45.3
   ]
  ],
  "meta_info": {
   "datasets": "MSCOCO",
   "datasets_short": "COCO",
   "task": "Real-time Instance Segmentation",
   "metric": "Frame (fps)"
  },
  "updated_answer2": [
   [
    "2203.12827",
    "SparseInst-448 (ResNet-50-vd)",
    "58.5 (2080 Ti)",
    58.5
   ],
   [
    "1904.02689",
    "YOLACT",
    "45.3 (Titan Xp)",
    45.3
   ],
   [
    "2007.14772",
    "SipMask (ResNet-50, single-scale test)",
    "41.7 (Titan Xp)",
    41.7
   ],
   [
    "2001.00309",
    "BlendMask-512 (DLA_34)",
    "33.3",
    33.3
   ],
   [
    "2003.10152",
    "SOLO-512",
    "31.3",
    31.3
   ],
   [
    "2110.09734",
    "maYOLACT-550 (ResNet-50)",
    "30 (Tesla V100)",
    30.0
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "27.3 (Titan Xp)",
    27.3
   ]
  ]
 },
 "./longdocdata/docs/611.json": {
  "question": "List the performance scores of various methods on the MSCOCO (COCO) dataset on the Real-time Instance Segmentation task using metric APS.",
  "answer": "| Method | APS |\n| --- | --- |\n| maYOLACT-700 (ResNet-50) | 18.1 |\n| SparseInst-608 (ResNet-50-vd) | 15.7 |\n| maYOLACT-550 (ResNet-50) | 14.7 |\n| CenterMask-Lite (ResNet-50-FPN) | 12.9 |\n| SparseInst-448 (ResNet-50-vd) | 12.3 |\n| YOLACT-550++ (ResNet-101-FPN) | 11.9 |\n| SipMask++ (ResNet-101, single-scale test) | 11.2 |\n| SipMask (ResNet-101, single-scale test) | 9.3 |\n| SipMask (ResNet-50, single-scale test) | 9.2 |\n| YOLACT-550 (ResNet-101-FPN) | 9.2 |\n| YOLACT | 5.0 |",
  "src_docs": [
   "2110.09734",
   "1904.02689",
   "1912.06218",
   "1911.06667",
   "2007.14772",
   "2203.12827"
  ],
  "updated_answer": [
    [
        "2110.09734",
        "maYOLACT-700 (ResNet-50)",
        "18.1",
        18.1
       ],
       [
        "2203.12827",
        "SparseInst-608 (ResNet-50-vd)",
        "15.7",
        15.7
       ],
       [
        "1911.06667",
        "CenterMask-Lite (ResNet-50-FPN)",
        "12.9",
        12.9
       ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "11.2",
    11.2
   ],
   [
    "2110.09734",
    "maYOLACT-550 (ResNet-50)",
    "14.7",
    14.7
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "11.9",
    11.9
   ],
   [
    "1911.06667",
    "CenterMask-Lite (ResNet-50-FPN)",
    "12.9",
    12.9
   ],
   [
    "2007.14772",
    "SipMask (ResNet-101, single-scale test)",
    "9.3",
    9.3
   ],
   [
    "2007.14772",
    "SipMask (ResNet-50, single-scale test)",
    "9.2",
    9.2
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "9.2",
    9.2
   ],
   [
    "1904.02689",
    "YOLACT",
    "5.0",
    5.0
   ]
  ],
  "meta_info": {
   "datasets": "MSCOCO",
   "datasets_short": "COCO",
   "task": "Real-time Instance Segmentation",
   "metric": "APS"
  },
  "updated_answer2": [
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "18.1",
    18.1
   ],
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "15.7",
    15.7
   ],
   [
    "1911.06667",
    "CenterMask-Lite (ResNet-50-FPN)",
    "12.9",
    12.9
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "11.9",
    11.9
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "11.2",
    11.2
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "9.2",
    9.2
   ]
  ]
 },
 "./longdocdata/docs/612.json": {
  "question": "List the performance scores of various methods on the MSCOCO (COCO) dataset on the Real-time Instance Segmentation task using metric AP50.",
  "answer": "| Method | AP50 |\n| --- | --- |\n| maYOLACT-700 (ResNet-50) | 59.4 |\n| SparseInst-608 (ResNet-50-vd) | 59.2 |\n| SOLO-512 | 57.7 |\n| SparseInst-448 (ResNet-50-vd) | 56.5 |\n| maYOLACT-550 (ResNet-50) | 56.2 |\n| SipMask++ (ResNet-101, single-scale test) | 55.6 |\n| YOLACT-550++ (ResNet-101-FPN) | 53.8 |\n| SipMask (ResNet-101, single-scale test) | 53.4 |\n| SipMask (ResNet-50, single-scale test) | 51.9 |\n| YOLACT-550 (ResNet-101-FPN) | 46.6 |\n| YOLACT | 42.0 |",
  "src_docs": [
   "2110.09734",
   "1904.02689",
   "1912.06218",
   "2003.10152",
   "2007.14772",
   "2203.12827"
  ],
  "updated_answer": [
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "59.2",
    59.2
   ],
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "59.4",
    59.4
   ],
   [
    "2003.10152",
    "SOLO-512",
    "57.7",
    57.7
   ],
   [
    "2203.12827",
    "SparseInst-448 (ResNet-50-vd)",
    "56.5",
    56.5
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "55.6",
    55.6
   ],
   [
    "2110.09734",
    "maYOLACT-550 (ResNet-50)",
    "56.2",
    56.2
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "53.8",
    53.8
   ],
   [
    "2007.14772",
    "SipMask (ResNet-101, single-scale test)",
    "53.4",
    53.4
   ],
   [
    "2007.14772",
    "SipMask (ResNet-50, single-scale test)",
    "51.9",
    51.9
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "46.6",
    46.6
   ],
   [
    "1904.02689",
    "YOLACT",
    "42.0",
    42.0
   ]
  ],
  "meta_info": {
   "datasets": "MSCOCO",
   "datasets_short": "COCO",
   "task": "Real-time Instance Segmentation",
   "metric": "AP50"
  },
  "updated_answer2": [
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "59.4",
    59.4
   ],
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "59.2",
    59.2
   ],
   [
    "2003.10152",
    "SOLO-512",
    "57.7",
    57.7
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "55.6",
    55.6
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "53.8",
    53.8
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "46.6",
    46.6
   ]
  ]
 },
 "./longdocdata/docs/613.json": {
  "question": "List the performance scores of various methods on the MSCOCO (COCO) dataset on the Real-time Instance Segmentation task using metric APL.",
  "answer": "| Method | APL |\n| --- | --- |\n| SparseInst-448 (ResNet-50-vd) | 57.0 |\n| SparseInst-608 (ResNet-50-vd) | 56.9 |\n| SipMask++ (ResNet-101, single-scale test) | 56.8 |\n| YOLACT-550++ (ResNet-101-FPN) | 55.1 |\n| SipMask (ResNet-101, single-scale test) | 54.0 |\n| maYOLACT-700 (ResNet-50) | 52.5 |\n| maYOLACT-550 (ResNet-50) | 51.4 |\n| SipMask (ResNet-50, single-scale test) | 49.8 |\n| CenterMask-Lite (ResNet-50-FPN) | 48.7 |\n| YOLACT | 45.0 |\n| YOLACT-550 (ResNet-101-FPN) | 44.8 |",
  "src_docs": [
   "2110.09734",
   "1904.02689",
   "1912.06218",
   "1911.06667",
   "2007.14772",
   "2203.12827"
  ],
  "updated_answer": [
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "56.9",
    56.9
   ],
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "52.5",
    52.5
   ],
   [
    "2203.12827",
    "SparseInst-448 (ResNet-50-vd)",
    "57.0",
    57.0
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "56.8",
    56.8
   ],
   [
    "2110.09734",
    "maYOLACT-550 (ResNet-50)",
    "51.4",
    51.4
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "55.1",
    55.1
   ],
   [
    "1911.06667",
    "CenterMask-Lite (ResNet-50-FPN)",
    "48.7",
    48.7
   ],
   [
    "2007.14772",
    "SipMask (ResNet-101, single-scale test)",
    "54.0",
    54.0
   ],
   [
    "2007.14772",
    "SipMask (ResNet-50, single-scale test)",
    "49.8",
    49.8
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "44.8",
    44.8
   ],
   [
    "1904.02689",
    "YOLACT",
    "45.0",
    45.0
   ]
  ],
  "meta_info": {
   "datasets": "MSCOCO",
   "datasets_short": "COCO",
   "task": "Real-time Instance Segmentation",
   "metric": "APL"
  },
  "updated_answer2": [
   [
    "2203.12827",
    "SparseInst-448 (ResNet-50-vd)",
    "57.0",
    57.0
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "56.8",
    56.8
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "55.1",
    55.1
   ],
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "52.5",
    52.5
   ],
   [
    "1911.06667",
    "CenterMask-Lite (ResNet-50-FPN)",
    "48.7",
    48.7
   ],
   [
    "1904.02689",
    "YOLACT",
    "45.0",
    45.0
   ]
  ]
 },
 "./longdocdata/docs/614.json": {
  "question": "List the performance scores of various methods on the MSCOCO (COCO) dataset on the Real-time Instance Segmentation task using metric AP75.",
  "answer": "| Method | AP75 |\n| --- | --- |\n| SparseInst-608 (ResNet-50-vd) | 40.2 |\n| maYOLACT-700 (ResNet-50) | 39.9 |\n| SOLO-512 | 39.7 |\n| SparseInst-448 (ResNet-50-vd) | 37.7 |\n| SipMask++ (ResNet-101, single-scale test) | 37.6 |\n| maYOLACT-550 (ResNet-50) | 37.1 |\n| YOLACT-550++ (ResNet-101-FPN) | 36.9 |\n| SipMask (ResNet-101, single-scale test) | 34.3 |\n| SipMask (ResNet-50, single-scale test) | 32.3 |\n| YOLACT-550 (ResNet-101-FPN) | 29.2 |\n| YOLACT | 25.4 |",
  "src_docs": [
   "2110.09734",
   "1904.02689",
   "1912.06218",
   "2003.10152",
   "2007.14772",
   "2203.12827"
  ],
  "updated_answer": [
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "40.2",
    40.2
   ],
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "39.9",
    39.9
   ],
   [
    "2003.10152",
    "SOLO-512",
    "39.7",
    39.7
   ],
   [
    "2203.12827",
    "SparseInst-448 (ResNet-50-vd)",
    "37.7",
    37.7
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "37.6",
    37.6
   ],
   [
    "2110.09734",
    "maYOLACT-550 (ResNet-50)",
    "37.1",
    37.1
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "36.9",
    36.9
   ],
   [
    "2007.14772",
    "SipMask (ResNet-101, single-scale test)",
    "34.3",
    34.3
   ],
   [
    "2007.14772",
    "SipMask (ResNet-50, single-scale test)",
    "32.3",
    32.3
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "29.2",
    29.2
   ],
   [
    "1904.02689",
    "YOLACT",
    "25.4",
    25.4
   ]
  ],
  "meta_info": {
   "datasets": "MSCOCO",
   "datasets_short": "COCO",
   "task": "Real-time Instance Segmentation",
   "metric": "AP75"
  },
  "updated_answer2": [
   [
    "2203.12827",
    "SparseInst-608 (ResNet-50-vd)",
    "40.2",
    40.2
   ],
   [
    "2110.09734",
    "maYOLACT-700 (ResNet-50)",
    "39.9",
    39.9
   ],
   [
    "2003.10152",
    "SOLO-512",
    "39.7",
    39.7
   ],
   [
    "2007.14772",
    "SipMask++ (ResNet-101, single-scale test)",
    "37.6",
    37.6
   ],
   [
    "1912.06218",
    "YOLACT-550++ (ResNet-101-FPN)",
    "36.9",
    36.9
   ],
   [
    "1904.02689",
    "YOLACT-550 (ResNet-101-FPN)",
    "29.2",
    29.2
   ]
  ]
 },
 "./longdocdata/docs/629.json": {
  "question": "List the performance scores of various methods on the COCO test-challenge (COCO) dataset on the Keypoint Detection task using metric AR50.",
  "answer": "| Method | AR50 |\n| --- | --- |\n| 4ÃRSN-50 | 96.1 |\n| MSPN+* | 96.0 |\n| Simple Base+* | 95.1 |\n| CPN+ | 94.7 |\n| Mask R-CNN* | 93.2 |\n| G-RMI* | 90.7 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1703.06870",
   "1804.06208",
   "2003.04030",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "96.1",
    96.1
   ],
   [
    "1901.00148",
    "MSPN+*",
    "96",
    96.0
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "95.1",
    95.1
   ],
   [
    "1711.07319",
    "CPN+",
    "94.7",
    94.7
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "93.2",
    93.2
   ],
   [
    "1701.01779",
    "G-RMI*",
    "90.7",
    90.7
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-challenge",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "AR50"
  },
  "updated_answer2": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "96.1",
    96.1
   ],
   [
    "1901.00148",
    "MSPN+*",
    "96",
    96.0
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "95.1",
    95.1
   ],
   [
    "1711.07319",
    "CPN+",
    "94.7",
    94.7
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "93.2",
    93.2
   ],
   [
    "1701.01779",
    "G-RMI*",
    "90.7",
    90.7
   ]
  ]
 },
 "./longdocdata/docs/632.json": {
  "question": "List the performance scores of various methods on the COCO test-challenge (COCO) dataset on the Keypoint Detection task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| 4ÃRSN-50 | 77.1 |\n| MSPN+* | 76.4 |\n| Simple Base+* | 74.5 |\n| CPN+ | 72.1 |\n| G-RMI* | 69.1 |\n| Mask R-CNN* | 68.9 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1703.06870",
   "1804.06208",
   "2003.04030",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "77.1",
    77.1
   ],
   [
    "1901.00148",
    "MSPN+*",
    "76.4",
    76.4
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "74.5",
    74.5
   ],
   [
    "1711.07319",
    "CPN+",
    "72.1",
    72.1
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "68.9",
    68.9
   ],
   [
    "1701.01779",
    "G-RMI*",
    "69.1",
    69.1
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-challenge",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "77.1",
    77.1
   ],
   [
    "1901.00148",
    "MSPN+*",
    "76.4",
    76.4
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "74.5",
    74.5
   ],
   [
    "1711.07319",
    "CPN+",
    "72.1",
    72.1
   ],
   [
    "1701.01779",
    "G-RMI*",
    "69.1",
    69.1
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "68.9",
    68.9
   ]
  ]
 },
 "./longdocdata/docs/633.json": {
  "question": "List the performance scores of various methods on the COCO test-challenge (COCO) dataset on the Keypoint Detection task using metric AR75.",
  "answer": "| Method | AR75 |\n| --- | --- |\n| 4ÃRSN-50 | 88.2 |\n| MSPN+* | 87.7 |\n| Simple Base+* | 86.3 |\n| CPN+ | 84.8 |\n| Mask R-CNN* | 81.2 |\n| G-RMI* | 80.7 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1703.06870",
   "1804.06208",
   "2003.04030",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "88.2",
    88.2
   ],
   [
    "1901.00148",
    "MSPN+*",
    "87.7",
    87.7
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "86.3",
    86.3
   ],
   [
    "1711.07319",
    "CPN+",
    "84.8",
    84.8
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "81.2",
    81.2
   ],
   [
    "1701.01779",
    "G-RMI*",
    "80.7",
    80.7
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-challenge",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "AR75"
  },
  "updated_answer2": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "88.2",
    88.2
   ],
   [
    "1901.00148",
    "MSPN+*",
    "87.7",
    87.7
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "86.3",
    86.3
   ],
   [
    "1711.07319",
    "CPN+",
    "84.8",
    84.8
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "81.2",
    81.2
   ],
   [
    "1701.01779",
    "G-RMI*",
    "80.7",
    80.7
   ]
  ]
 },
 "./longdocdata/docs/634.json": {
  "question": "List the performance scores of various methods on the COCO test-challenge (COCO) dataset on the Keypoint Detection task using metric APL.",
  "answer": "| Method | APL |\n| --- | --- |\n| MSPN+* | 88.6 |\n| Simple Base+* | 87.5 |\n| CPN+ | 84.7 |\n| 4ÃRSN-50 | 82.6 |\n| Mask R-CNN* | 82.6 |\n| G-RMI* | 82.4 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1703.06870",
   "2003.04030",
   "1804.06208",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "82.6",
    82.6
   ],
   [
    "1901.00148",
    "MSPN+*",
    "88.6",
    88.6
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "87.5",
    87.5
   ],
   [
    "1711.07319",
    "CPN+",
    "84.7",
    84.7
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "82.6",
    82.6
   ],
   [
    "1701.01779",
    "G-RMI*",
    "82.4",
    82.4
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-challenge",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "APL"
  },
  "updated_answer2": [
   [
    "1901.00148",
    "MSPN+*",
    "88.6",
    88.6
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "87.5",
    87.5
   ],
   [
    "1711.07319",
    "CPN+",
    "84.7",
    84.7
   ],
   [
    "2003.04030",
    "4ÃRSN-50",
    "82.6",
    82.6
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "82.6",
    82.6
   ],
   [
    "1701.01779",
    "G-RMI*",
    "82.4",
    82.4
   ]
  ]
 },
 "./longdocdata/docs/635.json": {
  "question": "List the performance scores of various methods on the COCO test-challenge (COCO) dataset on the Keypoint Detection task using metric AP75.",
  "answer": "| Method | AP75 |\n| --- | --- |\n| 4ÃRSN-50 | 83.6 |\n| MSPN+* | 82.6 |\n| Simple Base+* | 80.8 |\n| CPN+ | 78.9 |\n| Mask R-CNN* | 75.2 |\n| G-RMI* | 75.2 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1703.06870",
   "1804.06208",
   "2003.04030",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "83.6",
    83.6
   ],
   [
    "1901.00148",
    "MSPN+*",
    "82.6",
    82.6
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "80.8",
    80.8
   ],
   [
    "1711.07319",
    "CPN+",
    "78.9",
    78.9
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "75.2",
    75.2
   ],
   [
    "1701.01779",
    "G-RMI*",
    "75.2",
    75.2
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-challenge",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "AP75"
  },
  "updated_answer2": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "83.6",
    83.6
   ],
   [
    "1901.00148",
    "MSPN+*",
    "82.6",
    82.6
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "80.8",
    80.8
   ],
   [
    "1711.07319",
    "CPN+",
    "78.9",
    78.9
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "75.2",
    75.2
   ],
   [
    "1701.01779",
    "G-RMI*",
    "75.2",
    75.2
   ]
  ]
 },
 "./longdocdata/docs/636.json": {
  "question": "List the performance scores of various methods on the COCO test-challenge (COCO) dataset on the Keypoint Detection task using metric ARL.",
  "answer": "| Method | ARL |\n| --- | --- |\n| 4ÃRSN-50 | 88.7 |\n| MSPN+* | 83.2 |\n| Simple Base+* | 82.9 |\n| CPN+ | 78.1 |\n| Mask R-CNN* | 76.8 |\n| G-RMI* | 74.5 |",
  "src_docs": [
   "1901.00148",
   "1711.07319",
   "1703.06870",
   "1804.06208",
   "2003.04030",
   "1701.01779"
  ],
  "updated_answer": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "88.7",
    88.7
   ],
   [
    "1901.00148",
    "MSPN+*",
    "83.2",
    83.2
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "82.9",
    82.9
   ],
   [
    "1711.07319",
    "CPN+",
    "78.1",
    78.1
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "76.8",
    76.8
   ],
   [
    "1701.01779",
    "G-RMI*",
    "74.5",
    74.5
   ]
  ],
  "meta_info": {
   "datasets": "COCO test-challenge",
   "datasets_short": "COCO",
   "task": "Keypoint Detection",
   "metric": "ARL"
  },
  "updated_answer2": [
   [
    "2003.04030",
    "4ÃRSN-50",
    "88.7",
    88.7
   ],
   [
    "1901.00148",
    "MSPN+*",
    "83.2",
    83.2
   ],
   [
    "1804.06208",
    "Simple Base+*",
    "82.9",
    82.9
   ],
   [
    "1711.07319",
    "CPN+",
    "78.1",
    78.1
   ],
   [
    "1703.06870",
    "Mask R-CNN*",
    "76.8",
    76.8
   ],
   [
    "1701.01779",
    "G-RMI*",
    "74.5",
    74.5
   ]
  ]
 },
 "./longdocdata/docs/696.json": {
  "question": "List the performance scores of various methods on the KITTI (KITTI) dataset on the 3D Multi-Object Tracking task using metric MOTP.",
  "answer": "| Method | MOTP |\n| --- | --- |\n| BeyondPixels | 85.73 |\n| 3D Kalman Filter + Birth and Death Memory | 85.23 |\n| DSM | 83.42 |\n| FANTrack | 82.32 |\n| EagerMOT | 80.0 |\n| Complexer-YOLO | 78.46 |",
  "src_docs": [
   "1905.02843",
   "1904.07537",
   "1907.03961",
   "1806.11534",
   "1802.09298",
   "2104.14682"
  ],
  "updated_answer": [
   [
    "2104.14682",
    "EagerMOT",
    "80%",
    80.0
   ],
   [
    "1802.09298",
    "BeyondPixels",
    "85.73%",
    85.73
   ],
   [
    "1907.03961",
    "3D Kalman Filter + Birth and Death Memory",
    "85.23%",
    85.23
   ],
   [
    "1905.02843",
    "FANTrack",
    "82.32%",
    82.32
   ],
   [
    "1806.11534",
    "DSM",
    "83.42%",
    83.42
   ],
   [
    "1904.07537",
    "Complexer-YOLO",
    "78.46%",
    78.46
   ]
  ],
  "meta_info": {
   "datasets": "KITTI",
   "datasets_short": "KITTI",
   "task": "3D Multi-Object Tracking",
   "metric": "MOTP"
  },
  "updated_answer2": [
   [
    "1802.09298",
    "BeyondPixels",
    "85.73%",
    85.73
   ],
   [
    "1907.03961",
    "3D Kalman Filter + Birth and Death Memory",
    "85.23%",
    85.23
   ],
   [
    "1806.11534",
    "DSM",
    "83.42%",
    83.42
   ],
   [
    "1905.02843",
    "FANTrack",
    "82.32%",
    82.32
   ],
   [
    "2104.14682",
    "EagerMOT",
    "80%",
    80.0
   ],
   [
    "1904.07537",
    "Complexer-YOLO",
    "78.46%",
    78.46
   ]
  ]
 },
 "./longdocdata/docs/698.json": {
  "question": "List the performance scores of various methods on the KITTI (KITTI) dataset on the 3D Multi-Object Tracking task using metric MOTA.",
  "answer": "| Method | MOTA |\n| --- | --- |\n| EagerMOT | 96.61 |\n| BeyondPixels | 84.24 |\n| 3D Kalman Filter + Birth and Death Memory | 83.34 |\n| FANTrack | 77.72 |\n| DSM | 76.15 |\n| Complexer-YOLO | 75.7 |",
  "src_docs": [
   "1905.02843",
   "1904.07537",
   "1907.03961",
   "1806.11534",
   "1802.09298",
   "2104.14682"
  ],
  "updated_answer": [
   [
    "2104.14682",
    "EagerMOT",
    "96.61%",
    96.61
   ],
   [
    "1802.09298",
    "BeyondPixels",
    "84.24%",
    84.24
   ],
   [
    "1907.03961",
    "3D Kalman Filter + Birth and Death Memory",
    "83.34%",
    83.34
   ],
   [
    "1905.02843",
    "FANTrack",
    "77.72%",
    77.72
   ],
   [
    "1806.11534",
    "DSM",
    "76.15%",
    76.15
   ],
   [
    "1904.07537",
    "Complexer-YOLO",
    "75.70%",
    75.7
   ]
  ],
  "meta_info": {
   "datasets": "KITTI",
   "datasets_short": "KITTI",
   "task": "3D Multi-Object Tracking",
   "metric": "MOTA"
  },
  "updated_answer2": [
   [
    "2104.14682",
    "EagerMOT",
    "96.61%",
    96.61
   ],
   [
    "1802.09298",
    "BeyondPixels",
    "84.24%",
    84.24
   ],
   [
    "1907.03961",
    "3D Kalman Filter + Birth and Death Memory",
    "83.34%",
    83.34
   ],
   [
    "1905.02843",
    "FANTrack",
    "77.72%",
    77.72
   ],
   [
    "1806.11534",
    "DSM",
    "76.15%",
    76.15
   ],
   [
    "1904.07537",
    "Complexer-YOLO",
    "75.70%",
    75.7
   ]
  ]
 },
 "./longdocdata/docs/732.json": {
  "question": "List the performance scores of various methods on the KITTI Pedestrians Moderate (KITTI) dataset on the 3D Object Detection From Stereo Images task using metric AP50.",
  "answer": "| Method | AP50 |\n| --- | --- |\n| DSGN++ | 32.74 |\n| LIGA-Stereo | 30.0 |\n| Disp R-CNN | 25.8 |\n| CG-Stereo | 24.31 |\n| YoLoStereo3D | 19.75 |\n| DSGN | 15.55 |",
  "src_docs": [
   "2204.03039",
   "2103.09422",
   "2003.05505",
   "2004.03572",
   "2108.08258",
   "2001.03398"
  ],
  "updated_answer": [
   [
    "2204.03039",
    "DSGN++",
    "32.74",
    32.74
   ],
   [
    "2108.08258",
    "LIGA-Stereo",
    "30.00",
    30.0
   ],
   [
    "2004.03572",
    "Disp R-CNN",
    "25.80",
    25.8
   ],
   [
    "2003.05505",
    "CG-Stereo",
    "24.31",
    24.31
   ],
   [
    "2103.09422",
    "YoLoStereo3D",
    "19.75",
    19.75
   ],
   [
    "2001.03398",
    "DSGN",
    "15.55",
    15.55
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Pedestrians Moderate",
   "datasets_short": "KITTI",
   "task": "3D Object Detection From Stereo Images",
   "metric": "AP50"
  },
  "updated_answer2": [
   [
    "2204.03039",
    "DSGN++",
    "32.74",
    32.74
   ],
   [
    "2108.08258",
    "LIGA-Stereo",
    "30.00",
    30.0
   ],
   [
    "2004.03572",
    "Disp R-CNN",
    "25.80",
    25.8
   ],
   [
    "2003.05505",
    "CG-Stereo",
    "24.31",
    24.31
   ],
   [
    "2103.09422",
    "YoLoStereo3D",
    "19.75",
    19.75
   ],
   [
    "2001.03398",
    "DSGN",
    "15.55",
    15.55
   ]
  ]
 },
 "./longdocdata/docs/753.json": {
  "question": "List the performance scores of various methods on the KITTI Cars Easy (KITTI) dataset on the Object Detection task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| Patches | 87.87 |\n| PointRCNN Shi et al. (2019) | 85.94 |\n| Roarnet | 83.71 |\n| Vote3Deep | 76.79 |\n| VeloFCN | 60.34 |",
  "src_docs": [
   "1910.04093",
   "1609.06666",
   "1812.04244",
   "1811.03818",
   "1608.07916"
  ],
  "updated_answer": [
   [
    "1910.04093",
    "Patches",
    "87.87",
    87.87
   ],
   [
    "1812.04244",
    "PointRCNN Shi et al. (2019)",
    "85.94",
    85.94
   ],
   [
    "1811.03818",
    "Roarnet",
    "83.71",
    83.71
   ],
   [
    "1609.06666",
    "Vote3Deep",
    "76.79",
    76.79
   ],
   [
    "1608.07916",
    "VeloFCN",
    "60.34",
    60.34
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Cars Easy",
   "datasets_short": "KITTI",
   "task": "Object Detection",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "1910.04093",
    "Patches",
    "87.87",
    87.87
   ],
   [
    "1812.04244",
    "PointRCNN Shi et al. (2019)",
    "85.94",
    85.94
   ],
   [
    "1811.03818",
    "Roarnet",
    "83.71",
    83.71
   ],
   [
    "1609.06666",
    "Vote3Deep",
    "76.79",
    76.79
   ],
   [
    "1608.07916",
    "VeloFCN",
    "60.34",
    60.34
   ]
  ]
 },
 "./longdocdata/docs/763.json": {
  "question": "List the performance scores of various methods on the KITTI Cars Hard (KITTI) dataset on the Object Detection task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| Patches | 68.91 |\n| PointRCNN Shi et al. (2019) | 68.32 |\n| Vote3Deep | 63.23 |\n| F-PointNet | 62.19 |\n| VeloFCN | 42.74 |",
  "src_docs": [
   "1910.04093",
   "1609.06666",
   "1812.04244",
   "1608.07916",
   "1711.08488"
  ],
  "updated_answer": [
   [
    "1910.04093",
    "Patches",
    "68.91",
    68.91
   ],
   [
    "1812.04244",
    "PointRCNN Shi et al. (2019)",
    "68.32",
    68.32
   ],
   [
    "1609.06666",
    "Vote3Deep",
    "63.23",
    63.23
   ],
   [
    "1711.08488",
    "F-PointNet",
    "62.19",
    62.19
   ],
   [
    "1608.07916",
    "VeloFCN",
    "42.74",
    42.74
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Cars Hard",
   "datasets_short": "KITTI",
   "task": "Object Detection",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "1910.04093",
    "Patches",
    "68.91",
    68.91
   ],
   [
    "1812.04244",
    "PointRCNN Shi et al. (2019)",
    "68.32",
    68.32
   ],
   [
    "1609.06666",
    "Vote3Deep",
    "63.23",
    63.23
   ],
   [
    "1711.08488",
    "F-PointNet",
    "62.19",
    62.19
   ],
   [
    "1608.07916",
    "VeloFCN",
    "42.74",
    42.74
   ]
  ]
 },
 "./longdocdata/docs/780.json": {
  "question": "List the performance scores of various methods on the KITTI Eigen split unsupervised (KITTI) dataset on the Monocular Depth Estimation task using metric RMSE log.",
  "answer": "| Method | RMSE log |\n| --- | --- |\n| DIFFNet (MS+1024x320) | 0.172 |\n| CADepth-Net (MS+1024x320) | 0.173 |\n| EPCDepth(S+1024x320) | 0.176 |\n| X-Distill (M+1024x320) | 0.18 |\n| GCNDepth | 0.181 |\n| EPCDepth(S+640x192) | 0.183 |\n| FeatDepth-MS | 0.184 |\n| SharinGAN | 0.19 |",
  "src_docs": [
   "2112.13047",
   "2109.12484",
   "2006.04026",
   "2007.10603",
   "2112.06782",
   "2110.12516",
   "2110.09482"
  ],
  "updated_answer": [
   [
    "2109.12484",
    "EPCDepth(S+1024x320)",
    "0.176",
    0.176
   ],
   [
    "2110.09482",
    "DIFFNet (MS+1024x320)",
    "0.172",
    0.172
   ],
   [
    "2112.13047",
    "CADepth-Net (MS+1024x320)",
    "0.173",
    0.173
   ],
   [
    "2007.10603",
    "FeatDepth-MS",
    "0.184",
    0.184
   ],
   [
    "2109.12484",
    "EPCDepth(S+640x192)",
    "0.183",
    0.183
   ],
   [
    "2110.12516",
    "X-Distill (M+1024x320)",
    "0.180",
    0.18
   ],
   [
    "2112.06782",
    "GCNDepth",
    "0.181",
    0.181
   ],
   [
    "2006.04026",
    "SharinGAN",
    "0.19",
    0.19
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Eigen split unsupervised",
   "datasets_short": "KITTI",
   "task": "Monocular Depth Estimation",
   "metric": "RMSE log"
  },
  "updated_answer2": [
   [
    "2110.09482",
    "DIFFNet (MS+1024x320)",
    "0.172",
    0.172
   ],
   [
    "2112.13047",
    "CADepth-Net (MS+1024x320)",
    "0.173",
    0.173
   ],
   [
    "2109.12484",
    "EPCDepth(S+1024x320)",
    "0.176",
    0.176
   ],
   [
    "2110.12516",
    "X-Distill (M+1024x320)",
    "0.180",
    0.18
   ],
   [
    "2112.06782",
    "GCNDepth",
    "0.181",
    0.181
   ],
   [
    "2007.10603",
    "FeatDepth-MS",
    "0.184",
    0.184
   ],
   [
    "2006.04026",
    "SharinGAN",
    "0.19",
    0.19
   ]
  ]
 },
 "./longdocdata/docs/784.json": {
  "question": "List the performance scores of various methods on the KITTI Semantic Segmentation (KITTI) dataset on the Semantic Segmentation task using metric Mean IoU (class).",
  "answer": "| Method | Mean IoU (class) |\n| --- | --- |\n| DeepLabV3Plus + SDCNetAug | 72.83 |\n| MapillaryAI | 69.56 |\n| SIW | 68.9 |\n| AHiSS | 61.24 |\n| SegStereo | 59.1 |\n| APMoE_seg | 47.96 |",
  "src_docs": [
   "1807.11699",
   "1712.02616",
   "2202.02002",
   "1805.01556",
   "1812.01593",
   "1803.05675"
  ],
  "updated_answer": [
   [
    "1812.01593",
    "DeepLabV3Plus + SDCNetAug",
    "72.83",
    72.83
   ],
   [
    "1712.02616",
    "MapillaryAI",
    "69.56",
    69.56
   ],
   [
    "2202.02002",
    "SIW",
    "68.9",
    68.9
   ],
   [
    "1803.05675",
    "AHiSS",
    "61.24",
    61.24
   ],
   [
    "1807.11699",
    "SegStereo",
    "59.10",
    59.1
   ],
   [
    "1805.01556",
    "APMoE_seg",
    "47.96",
    47.96
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Semantic Segmentation",
   "datasets_short": "KITTI",
   "task": "Semantic Segmentation",
   "metric": "Mean IoU (class)"
  },
  "updated_answer2": [
   [
    "1812.01593",
    "DeepLabV3Plus + SDCNetAug",
    "72.83",
    72.83
   ],
   [
    "1712.02616",
    "MapillaryAI",
    "69.56",
    69.56
   ],
   [
    "2202.02002",
    "SIW",
    "68.9",
    68.9
   ],
   [
    "1803.05675",
    "AHiSS",
    "61.24",
    61.24
   ],
   [
    "1807.11699",
    "SegStereo",
    "59.10",
    59.1
   ],
   [
    "1805.01556",
    "APMoE_seg",
    "47.96",
    47.96
   ]
  ]
 },
 "./longdocdata/docs/808.json": {
  "question": "List the performance scores of various methods on the KITTI Cyclists Moderate (KITTI) dataset on the 3D Object Detection From Stereo Images task using metric AP50.",
  "answer": "| Method | AP50 |\n| --- | --- |\n| DSGN++ | 43.9 |\n| LIGA-Stereo | 36.86 |\n| Disp R-CNN | 24.4 |\n| DSGN | 18.17 |\n| OC-Stereo | 16.63 |",
  "src_docs": [
   "2204.03039",
   "1909.07566",
   "2004.03572",
   "2108.08258",
   "2001.03398"
  ],
  "updated_answer": [
   [
    "2204.03039",
    "DSGN++",
    "43.90",
    43.9
   ],
   [
    "2108.08258",
    "LIGA-Stereo",
    "36.86",
    36.86
   ],
   [
    "2004.03572",
    "Disp R-CNN",
    "24.40",
    24.4
   ],
   [
    "2001.03398",
    "DSGN",
    "18.17",
    18.17
   ],
   [
    "1909.07566",
    "OC-Stereo",
    "16.63",
    16.63
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Cyclists Moderate",
   "datasets_short": "KITTI",
   "task": "3D Object Detection From Stereo Images",
   "metric": "AP50"
  },
  "updated_answer2": [
   [
    "2204.03039",
    "DSGN++",
    "43.90",
    43.90
   ],
   [
    "2108.08258",
    "LIGA-Stereo",
    "36.86",
    36.86
   ],
   [
    "2004.03572",
    "Disp R-CNN",
    "24.40",
    24.4
   ],
   [
    "2001.03398",
    "DSGN",
    "18.17",
    18.17
   ],
   [
    "1909.07566",
    "OC-Stereo",
    "16.63",
    16.63
   ]
  ]
 },
 "./longdocdata/docs/810.json": {
  "question": "List the performance scores of various methods on the KITTI Cyclists Moderate (KITTI) dataset on the Birds Eye View Object Detection task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| PV-RCNN | 68.89 |\n| STD | 65.32 |\n| PointPillars | 62.25 |\n| F-PointNet | 61.96 |\n| AVOD-FPN | 57.48 |\n| VoxelNet | 54.76 |",
  "src_docs": [
   "1812.05784",
   "1711.06396",
   "1907.10471",
   "1912.13192",
   "1712.02294",
   "1711.08488"
  ],
  "updated_answer": [
   [
    "1912.13192",
    "PV-RCNN",
    "68.89%",
    68.89
   ],
   [
    "1907.10471",
    "STD",
    "65.32%",
    65.32
   ],
   [
    "1812.05784",
    "PointPillars",
    "62.25%",
    62.25
   ],
   [
    "1711.08488",
    "F-PointNet",
    "61.96%",
    61.96
   ],
   [
    "1712.02294",
    "AVOD-FPN",
    "57.48%",
    57.48
   ],
   [
    "1711.06396",
    "VoxelNet",
    "54.76%",
    54.76
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Cyclists Moderate",
   "datasets_short": "KITTI",
   "task": "Birds Eye View Object Detection",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "1912.13192",
    "PV-RCNN",
    "68.89%",
    68.89
   ],
   [
    "1907.10471",
    "STD",
    "65.32%",
    65.32
   ],
   [
    "1812.05784",
    "PointPillars",
    "62.25%",
    62.25
   ],
   [
    "1711.08488",
    "F-PointNet",
    "61.96%",
    61.96
   ],
   [
    "1712.02294",
    "AVOD-FPN",
    "57.48%",
    57.48
   ],
   [
    "1711.06396",
    "VoxelNet",
    "54.76%",
    54.76
   ]
  ]
 },
 "./longdocdata/docs/840.json": {
  "question": "List the performance scores of various methods on the KITTI Depth Completion (KITTI) dataset on the Depth Completion task using metric iMAE.",
  "answer": "| Method | iMAE |\n| --- | --- |\n| NLSPN | 0.84 |\n| FusionNet (RGB_guide&certainty) | 0.93 |\n| PENet | 0.94 |\n| KBNet | 1.02 |\n| ScaffNet-FusionNet | 1.15 |\n| VOICED | 1.2 |",
  "src_docs": [
   "2007.10042",
   "2108.10531",
   "2106.02994",
   "1905.08616",
   "1902.05356",
   "2103.00783"
  ],
  "updated_answer": [
   [
    "2103.00783",
    "PENet",
    "0.94",
    0.94
   ],
   [
    "2007.10042",
    "NLSPN",
    "0.84",
    0.84
   ],
   [
    "1902.05356",
    "FusionNet (RGB_guide&certainty)",
    "0.93",
    0.93
   ],
   [
    "2108.10531",
    "KBNet",
    "1.02",
    1.02
   ],
   [
    "2106.02994",
    "ScaffNet-FusionNet",
    "1.15",
    1.15
   ],
   [
    "1905.08616",
    "VOICED",
    "1.20",
    1.2
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Depth Completion",
   "datasets_short": "KITTI",
   "task": "Depth Completion",
   "metric": "iMAE"
  },
  "updated_answer2": [
   [
    "2007.10042",
    "NLSPN",
    "0.84",
    0.84
   ],
   [
    "1902.05356",
    "FusionNet (RGB_guide&certainty)",
    "0.93",
    0.93
   ],
   [
    "2103.00783",
    "PENet",
    "0.94",
    0.94
   ],
   [
    "2108.10531",
    "KBNet",
    "1.02",
    1.02
   ],
   [
    "2106.02994",
    "ScaffNet-FusionNet",
    "1.15",
    1.15
   ],
   [
    "1905.08616",
    "VOICED",
    "1.20",
    1.2
   ]
  ]
 },

 "./longdocdata/docs/877.json": {
  "question": "List the performance scores of various methods on the UCF-101 16 frames, 64x64, Unconditional (UCF101) dataset on the Video Generation task using metric Inception Score.",
  "answer": "| Method | Inception Score |\n| --- | --- |\n| Video Diffusion Model | 57.0 |\n| TGAN-ODE | 15.2 |\n| TGAN-F | 13.62 |\n| MoCoGAN | 12.42 |\n| MoCoGAN-MDP | 11.86 |\n| TGAN-SVC | 11.85 |\n| VGAN | 8.18 |",
  "src_docs": [
   "2011.03864",
   "1707.04993",
   "2204.03458",
   "1609.02612",
   "1611.06624",
   "1909.12400",
   "1912.08860"
  ],
  "updated_answer": [
   [
    "2204.03458",
    "Video Diffusion Model",
    "57",
    57.0
   ],
   [
    "2011.03864",
    "TGAN-ODE",
    "15.20",
    15.2
   ],
   [
    "1912.08860",
    "TGAN-F",
    "13.62",
    13.62
   ],
   [
    "1707.04993",
    "MoCoGAN",
    "12.42",
    12.42
   ],
   [
    "1909.12400",
    "MoCoGAN-MDP",
    "11.86",
    11.86
   ],
   [
    "1611.06624",
    "TGAN-SVC",
    "11.85",
    11.85
   ],
   [
    "1609.02612",
    "VGAN",
    "8.18",
    8.18
   ]
  ],
  "meta_info": {
   "datasets": "UCF-101 16 frames, 64x64, Unconditional",
   "datasets_short": "UCF101",
   "task": "Video Generation",
   "metric": "Inception Score"
  },
  "updated_answer2": [
   [
    "2204.03458",
    "Video Diffusion Model",
    "57",
    57.0
   ],
   [
    "2011.03864",
    "TGAN-ODE",
    "15.20",
    15.2
   ],
   [
    "1912.08860",
    "TGAN-F",
    "13.62",
    13.62
   ],
   [
    "1707.04993",
    "MoCoGAN",
    "12.42",
    12.42
   ],
   [
    "1909.12400",
    "MoCoGAN-MDP",
    "11.86",
    11.86
   ],
   [
    "1611.06624",
    "TGAN-SVC",
    "11.85",
    11.85
   ],
   [
    "1609.02612",
    "VGAN",
    "8.18",
    8.18
   ]
  ]
 },
 "./longdocdata/docs/882.json": {
  "question": "List the performance scores of various methods on the UCF-101 16 frames, Unconditional, Single GPU (UCF101) dataset on the Video Generation task using metric Inception Score.",
  "answer": "| Method | Inception Score |\n| --- | --- |\n| TGAN-F | 22.91 |\n| TGANv2 | 21.45 |\n| TGANv2-ODE | 21.02 |\n| MoCoGAN | 12.42 |\n| MoCoGAN-MDP | 11.86 |\n| TGAN-SVC | 11.85 |\n| VGAN | 8.18 |",
  "src_docs": [
   "2011.03864",
   "1707.04993",
   "1811.09245",
   "1609.02612",
   "1611.06624",
   "1909.12400",
   "1912.08860"
  ],
  "updated_answer": [
   [
    "1912.08860",
    "TGAN-F",
    "22.91",
    22.91
   ],
   [
    "1811.09245",
    "TGANv2",
    "21.45",
    21.45
   ],
   [
    "2011.03864",
    "TGANv2-ODE",
    "21.02",
    21.02
   ],
   [
    "1707.04993",
    "MoCoGAN",
    "12.42",
    12.42
   ],
   [
    "1909.12400",
    "MoCoGAN-MDP",
    "11.86",
    11.86
   ],
   [
    "1611.06624",
    "TGAN-SVC",
    "11.85",
    11.85
   ],
   [
    "1609.02612",
    "VGAN",
    "8.18",
    8.18
   ]
  ],
  "meta_info": {
   "datasets": "UCF-101 16 frames, Unconditional, Single GPU",
   "datasets_short": "UCF101",
   "task": "Video Generation",
   "metric": "Inception Score"
  },
  "updated_answer2": [
   [
    "1912.08860",
    "TGAN-F",
    "22.91",
    22.91
   ],
   [
    "1811.09245",
    "TGANv2",
    "21.45",
    21.45
   ],
   [
    "2011.03864",
    "TGANv2-ODE",
    "21.02",
    21.02
   ],
   [
    "1707.04993",
    "MoCoGAN",
    "12.42",
    12.42
   ],
   [
    "1909.12400",
    "MoCoGAN-MDP",
    "11.86",
    11.86
   ],
   [
    "1611.06624",
    "TGAN-SVC",
    "11.85",
    11.85
   ],
   [
    "1609.02612",
    "VGAN",
    "8.18",
    8.18
   ]
  ]
 },
 "./longdocdata/docs/928.json": {
  "question": "List the performance scores of various methods on the MIMIC-III (MIMIC-III) dataset on the Medical Code Prediction task using metric Precision@8.",
  "answer": "| Method | Precision@8 |\n| --- | --- |\n| RAC | 75.4 |\n| MSMN | 75.2 |\n| LAAT | 73.8 |\n| JointLAAT | 73.5 |\n| MultiResCNN | 73.4 |\n| CAML | 70.9 |\n| DR-CAML | 69.0 |\n| HAN | 61.4 |\n| Bi-GRU | 58.5 |\n| CNN | 58.1 |\n| Logistic Regression | 54.2 |",
  "src_docs": [
   "1912.00862",
   "2010.15728",
   "2007.06351",
   "2203.01515",
   "1802.05695",
   "2107.10650"
  ],
  "updated_answer": [
   [
    "2107.10650",
    "RAC",
    "75.4",
    75.4
   ],
   [
    "2203.01515",
    "MSMN",
    "75.2",
    75.2
   ],
   [
    "2007.06351",
    "JointLAAT",
    "73.5",
    73.5
   ],
   [
    "2007.06351",
    "LAAT",
    "73.8",
    73.8
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "73.4",
    73.4
   ],
   [
    "1802.05695",
    "CAML",
    "70.9",
    70.9
   ],
   [
    "1802.05695",
    "DR-CAML",
    "69.0",
    69.0
   ],
   [
    "1802.05695",
    "CNN",
    "58.1",
    58.1
   ],
   [
    "1802.05695",
    "Bi-GRU",
    "58.5",
    58.5
   ],
   [
    "2010.15728",
    "HAN",
    "61.4",
    61.4
   ],
   [
    "1802.05695",
    "Logistic Regression",
    "54.2",
    54.2
   ]
  ],
  "meta_info": {
   "datasets": "MIMIC-III",
   "datasets_short": "MIMIC-III",
   "task": "Medical Code Prediction",
   "metric": "Precision@8"
  },
  "updated_answer2": [
   [
    "2107.10650",
    "RAC",
    "75.4",
    75.4
   ],
   [
    "2203.01515",
    "MSMN",
    "75.2",
    75.2
   ],
   [
    "2007.06351",
    "LAAT",
    "73.8",
    73.8
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "73.4",
    73.4
   ],
   [
    "1802.05695",
    "CAML",
    "70.9",
    70.9
   ],
   [
    "2010.15728",
    "HAN",
    "61.4",
    61.4
   ]
  ]
 },
 "./longdocdata/docs/929.json": {
  "question": "List the performance scores of various methods on the MIMIC-III (MIMIC-III) dataset on the Medical Code Prediction task using metric Macro-AUC.",
  "answer": "| Method | Macro-AUC |\n| --- | --- |\n| MSMN | 95.0 |\n| RAC | 94.8 |\n| JointLAAT | 92.1 |\n| LAAT | 91.9 |\n| MultiResCNN | 91.0 |\n| DR-CAML | 89.7 |\n| CAML | 89.5 |\n| HAN | 88.5 |\n| Bi-GRU | 82.2 |\n| CNN | 80.6 |\n| Logistic Regression | 56.1 |",
  "src_docs": [
   "1912.00862",
   "2010.15728",
   "2007.06351",
   "2203.01515",
   "1802.05695",
   "2107.10650"
  ],
  "updated_answer": [
   [
    "2107.10650",
    "RAC",
    "94.8",
    94.8
   ],
   [
    "2203.01515",
    "MSMN",
    "95.0",
    95.0
   ],
   [
    "2007.06351",
    "JointLAAT",
    "92.1",
    92.1
   ],
   [
    "2007.06351",
    "LAAT",
    "91.9",
    91.9
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "91.0",
    91.0
   ],
   [
    "1802.05695",
    "CAML",
    "89.5",
    89.5
   ],
   [
    "1802.05695",
    "DR-CAML",
    "89.7",
    89.7
   ],
   [
    "1802.05695",
    "CNN",
    "80.6",
    80.6
   ],
   [
    "1802.05695",
    "Bi-GRU",
    "82.2",
    82.2
   ],
   [
    "2010.15728",
    "HAN",
    "88.5",
    88.5
   ],
   [
    "1802.05695",
    "Logistic Regression",
    "56.1",
    56.1
   ]
  ],
  "meta_info": {
   "datasets": "MIMIC-III",
   "datasets_short": "MIMIC-III",
   "task": "Medical Code Prediction",
   "metric": "Macro-AUC"
  },
  "updated_answer2": [
   [
    "2203.01515",
    "MSMN",
    "95.0",
    95.0
   ],
   [
    "2107.10650",
    "RAC",
    "94.8",
    94.8
   ],
   [
    "2007.06351",
    "JointLAAT",
    "92.1",
    92.1
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "91.0",
    91.0
   ],
   [
    "1802.05695",
    "DR-CAML",
    "89.7",
    89.7
   ],
   [
    "2010.15728",
    "HAN",
    "88.5",
    88.5
   ]
  ]
 },
 "./longdocdata/docs/932.json": {
  "question": "List the performance scores of various methods on the MIMIC-III (MIMIC-III) dataset on the Medical Code Prediction task using metric Micro-AUC.",
  "answer": "| Method | Micro-AUC |\n| --- | --- |\n| RAC | 99.2 |\n| MSMN | 99.2 |\n| JointLAAT | 98.8 |\n| LAAT | 98.8 |\n| MultiResCNN | 98.6 |\n| CAML | 98.6 |\n| DR-CAML | 98.5 |\n| HAN | 98.1 |\n| Bi-GRU | 97.1 |\n| CNN | 96.9 |\n| Logistic Regression | 93.7 |",
  "src_docs": [
   "1912.00862",
   "2010.15728",
   "2007.06351",
   "2203.01515",
   "1802.05695",
   "2107.10650"
  ],
  "updated_answer": [
   [
    "2107.10650",
    "RAC",
    "99.2",
    99.2
   ],
   [
    "2203.01515",
    "MSMN",
    "99.2",
    99.2
   ],
   [
    "2007.06351",
    "JointLAAT",
    "98.8",
    98.8
   ],
   [
    "2007.06351",
    "LAAT",
    "98.8",
    98.8
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "98.6",
    98.6
   ],
   [
    "1802.05695",
    "CAML",
    "98.6",
    98.6
   ],
   [
    "1802.05695",
    "DR-CAML",
    "98.5",
    98.5
   ],
   [
    "1802.05695",
    "CNN",
    "96.9",
    96.9
   ],
   [
    "1802.05695",
    "Bi-GRU",
    "97.1",
    97.1
   ],
   [
    "2010.15728",
    "HAN",
    "98.1",
    98.1
   ],
   [
    "1802.05695",
    "Logistic Regression",
    "93.7",
    93.7
   ]
  ],
  "meta_info": {
   "datasets": "MIMIC-III test set",
   "datasets_short": "MIMIC-III",
   "task": "Medical Code Prediction",
   "metric": "Micro-AUC"
  },
  "updated_answer2": [
   [
    "2107.10650",
    "RAC",
    "99.2",
    99.2
   ],
   [
    "2203.01515",
    "MSMN",
    "99.2",
    99.2
   ],
   [
    "2007.06351",
    "JointLAAT",
    "98.8",
    98.8
   ],
   [
    "2007.06351",
    "LAAT",
    "98.8",
    98.8
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "98.6",
    98.6
   ],
   [
    "1802.05695",
    "CAML",
    "98.6",
    98.6
   ],
   [
    "2010.15728",
    "HAN",
    "98.1",
    98.1
   ]
  ]
 },
 "./longdocdata/docs/933.json": {
  "question": "List the performance scores of various methods on the MIMIC-III (MIMIC-III) dataset on the Medical Code Prediction task using metric Macro-F1.",
  "answer": "| Method | Macro-F1 |\n| --- | --- |\n| RAC | 12.7 |\n| JointLAAT | 10.7 |\n| MSMN | 10.3 |\n| LAAT | 9.9 |\n| CAML | 8.8 |\n| DR-CAML | 8.6 |\n| MultiResCNN | 8.5 |\n| CNN | 4.2 |\n| Bi-GRU | 3.8 |\n| HAN | 3.6 |\n| Logistic Regression | 1.1 |",
  "src_docs": [
   "1912.00862",
   "2010.15728",
   "2007.06351",
   "2203.01515",
   "1802.05695",
   "2107.10650"
  ],
  "updated_answer": [
   [
    "2107.10650",
    "RAC",
    "12.7",
    12.7
   ],
   [
    "2203.01515",
    "MSMN",
    "10.3",
    10.3
   ],
   [
    "2007.06351",
    "JointLAAT",
    "10.7",
    10.7
   ],
   [
    "2007.06351",
    "LAAT",
    "9.9",
    9.9
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "8.5",
    8.5
   ],
   [
    "1802.05695",
    "CAML",
    "8.8",
    8.8
   ],
   [
    "1802.05695",
    "DR-CAML",
    "8.6",
    8.6
   ],
   [
    "1802.05695",
    "CNN",
    "4.2",
    4.2
   ],
   [
    "1802.05695",
    "Bi-GRU",
    "3.8",
    3.8
   ],
   [
    "2010.15728",
    "HAN",
    "3.6",
    3.6
   ],
   [
    "1802.05695",
    "Logistic Regression",
    "1.1",
    1.1
   ]
  ],
  "meta_info": {
   "datasets": "MIMIC-III",
   "datasets_short": "MIMIC-III",
   "task": "Medical Code Prediction",
   "metric": "Macro-F1"
  },
  "updated_answer2": [
   [
    "2107.10650",
    "RAC",
    "12.7",
    12.7
   ],
   [
    "2007.06351",
    "JointLAAT",
    "10.7",
    10.7
   ],
   [
    "2203.01515",
    "MSMN",
    "10.3",
    10.3
   ],
   [
    "1802.05695",
    "CAML",
    "8.8",
    8.8
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "8.5",
    8.5
   ],
   [
    "2010.15728",
    "HAN",
    "3.6",
    3.6
   ]
  ]
 },
 "./longdocdata/docs/934.json": {
  "question": "List the performance scores of various methods on the MIMIC-III (MIMIC-III) dataset on the Medical Code Prediction task using metric Micro-F1.",
  "answer": "| Method | Micro-F1 |\n| --- | --- |\n| RAC | 58.6 |\n| MSMN | 58.4 |\n| JointLAAT | 57.5 |\n| LAAT | 57.5 |\n| MultiResCNN | 55.2 |\n| CAML | 53.9 |\n| DR-CAML | 52.9 |\n| SVM | 44.1 |\n| CNN | 41.9 |\n| Bi-GRU | 41.7 |\n| HAN | 40.7 |\n| Logistic Regression | 27.2 |",
  "src_docs": [
   "1912.00862",
   "2010.15728",
   "2007.06351",
   "2203.01515",
   "1802.05695",
   "2107.10650"
  ],
  "updated_answer": [
   [
    "2107.10650",
    "RAC",
    "58.6",
    58.6
   ],
   [
    "2203.01515",
    "MSMN",
    "58.4",
    58.4
   ],
   [
    "2007.06351",
    "JointLAAT",
    "57.5",
    57.5
   ],
   [
    "2007.06351",
    "LAAT",
    "57.5",
    57.5
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "55.2",
    55.2
   ],
   [
    "1802.05695",
    "CAML",
    "53.9",
    53.9
   ],
   [
    "1802.05695",
    "DR-CAML",
    "52.9",
    52.9
   ],
   [
    "1802.05695",
    "SVM",
    "44.1",
    44.1
   ],
   [
    "1802.05695",
    "CNN",
    "41.9",
    41.9
   ],
   [
    "1802.05695",
    "Bi-GRU",
    "41.7",
    41.7
   ],
   [
    "2010.15728",
    "HAN",
    "40.7",
    40.7
   ],
   [
    "1802.05695",
    "Logistic Regression",
    "27.2",
    27.2
   ]
  ],
  "meta_info": {
   "datasets": "MIMIC-III",
   "datasets_short": "MIMIC-III",
   "task": "Medical Code Prediction",
   "metric": "Micro-F1"
  },
  "updated_answer2": [
   [
    "2107.10650",
    "RAC",
    "58.6",
    58.6
   ],
   [
    "2203.01515",
    "MSMN",
    "58.4",
    58.4
   ],
   [
    "2007.06351",
    "JointLAAT",
    "57.5",
    57.5
   ],
   [
    "2007.06351",
    "LAAT",
    "57.5",
    57.5
   ],
   [
    "1912.00862",
    "MultiResCNN",
    "55.2",
    55.2
   ],
   [
    "1802.05695",
    "CAML",
    "53.9",
    53.9
   ],
   [
    "2010.15728",
    "HAN",
    "40.7",
    40.7
   ]
  ]
 },
 "./longdocdata/docs/940.json": {
  "question": "List the performance scores of various methods on the Visual Genome (Visual Genome) dataset on the Unbiased Scene Graph Generation task using metric mR@20.",
  "answer": "| Method | mR@20 |\n| --- | --- |\n| SHA-GCL (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode) | 35.6 |\n| IETrans (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode) | 28.9 |\n| DLFE (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode) | 22.1 |\n| CogTree (VCTree-ResNeXt-101-FPN backbone; PredCls mode) | 22.0 |\n| CogTree (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode) | 20.9 |\n| DLFE (VCTree-ResNeXt-101-FPN backbone; PredCls mode) | 20.8 |\n| PCPL (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode) | 19.3 |\n| TDE (VCTree-ResNeXt-101-FPN backbone; PredCls mode) | 19.2 |\n| PCPL (VCTree-ResNeXt-101-FPN backbone; PredCls mode) | 18.7 |\n| IETrans (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode) | 17.5 |\n| TDE (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode) | 17.4 |\n| DLFE (VCTree-ResNeXt-101-FPN backbone; SGCls mode) | 15.8 |\n| CogTree (VCTree-ResNeXt-101-FPN backbone; SGCls mode) | 15.4 |\n| DLFE (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode) | 12.8 |\n| PCPL (VCTree-ResNeXt-101-FPN backbone; SGCls mode) | 12.7 |\n| CogTree (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode) | 12.1 |\n| TDE (VCTree-ResNeXt-101-FPN backbone; SGCls mode) | 11.2 |\n| IETrans (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode) | 10.9 |\n| TDE (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode) | 9.9 |\n| PCPL (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode) | 9.9 |\n| TDE (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode) | 9.7 |\n| DLFE (VCTree-ResNeXt-101-FPN backbone; SGDet mode) | 8.6 |\n| DLFE (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode) | 8.6 |\n| PCPL (VCTree-ResNeXt-101-FPN backbone; SGDet mode) | 8.1 |\n| PCPL (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode) | 8.0 |\n| CogTree (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode) | 7.9 |\n| CogTree (VCTree-ResNeXt-101-FPN backbone; SGDet mode) | 7.8 |\n| TDE (VCTree-ResNeXt-101-FPN backbone; SGDet mode) | 6.8 |",
  "src_docs": [
   "2107.02112",
   "2009.00893",
   "2002.11949",
   "2203.09811",
   "2009.07526",
   "2203.11654"
  ],
  "updated_answer": [
   [
    "2203.11654",
    "IETrans (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode)",
    "28.9",
    28.9
   ],
   [
    "2107.02112",
    "DLFE (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode)",
    "22.1",
    22.1
   ],
   [
    "2107.02112",
    "DLFE (VCTree-ResNeXt-101-FPN backbone; PredCls mode)",
    "20.8",
    20.8
   ],
   [
    "2009.00893",
    "PCPL (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode)",
    "19.3",
    19.3
   ],
   [
    "2009.00893",
    "PCPL (VCTree-ResNeXt-101-FPN backbone; PredCls mode)",
    "18.7",
    18.7
   ],
   [
    "2203.11654",
    "IETrans (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode)",
    "17.5",
    17.5
   ],
   [
    "2107.02112",
    "DLFE (VCTree-ResNeXt-101-FPN backbone; SGCls mode)",
    "15.8",
    15.8
   ],
   [
    "2002.11949",
    "TDE (VCTree-ResNeXt-101-FPN backbone; PredCls mode)",
    "19.2",
    19.2
   ],
   [
    "2002.11949",
    "TDE (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode)",
    "17.4",
    17.4
   ],
   [
    "2107.02112",
    "DLFE (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode)",
    "12.8",
    12.8
   ],
   [
    "2009.00893",
    "PCPL (VCTree-ResNeXt-101-FPN backbone; SGCls mode)",
    "12.7",
    12.7
   ],
   [
    "2203.11654",
    "IETrans (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode)",
    "10.9",
    10.9
   ],
   [
    "2009.00893",
    "PCPL (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode)",
    "9.9",
    9.9
   ],
   [
    "2002.11949",
    "TDE (VCTree-ResNeXt-101-FPN backbone; SGCls mode)",
    "11.2",
    11.2
   ],
   [
    "2107.02112",
    "DLFE (VCTree-ResNeXt-101-FPN backbone; SGDet mode)",
    "8.6",
    8.6
   ],
   [
    "2107.02112",
    "DLFE (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode)",
    "8.6",
    8.6
   ],
   [
    "2002.11949",
    "TDE (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode)",
    "9.9",
    9.9
   ],
   [
    "2009.00893",
    "PCPL (VCTree-ResNeXt-101-FPN backbone; SGDet mode)",
    "8.1",
    8.1
   ],
   [
    "2009.00893",
    "PCPL (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode)",
    "8.0",
    8.0
   ],
   [
    "2002.11949",
    "TDE (VCTree-ResNeXt-101-FPN backbone; SGDet mode)",
    "6.8",
    6.8
   ],
   [
    "2002.11949",
    "TDE (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode)",
    "9.7",
    9.7
   ],
   [
    "2203.09811",
    "SHA-GCL (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode)",
    "35.6",
    35.6
   ],
   [
    "2009.07526",
    "CogTree (VCTree-ResNeXt-101-FPN backbone; PredCls mode)",
    "22.0",
    22.0
   ],
   [
    "2009.07526",
    "CogTree (MOTIFS-ResNeXt-101-FPN backbone; PredCls mode)",
    "20.9",
    20.9
   ],
   [
    "2009.07526",
    "CogTree (VCTree-ResNeXt-101-FPN backbone; SGCls mode)",
    "15.4",
    15.4
   ],
   [
    "2009.07526",
    "CogTree (MOTIFS-ResNeXt-101-FPN backbone; SGCls mode)",
    "12.1",
    12.1
   ],
   [
    "2009.07526",
    "CogTree (MOTIFS-ResNeXt-101-FPN backbone; SGDet mode)",
    "7.9",
    7.9
   ],
   [
    "2009.07526",
    "CogTree (VCTree-ResNeXt-101-FPN backbone; SGDet mode)",
    "7.8",
    7.8
   ]
  ],
  "meta_info": {
   "datasets": "Visual Genome",
   "datasets_short": "Visual Genome",
   "task": "Unbiased Scene Graph Generation",
   "metric": "mR@20"
  },
  "updated_answer2": [
   [
    "2203.09811",
    "SHA+GCL",
    "35.6",
    35.6
   ],
   [
    "2203.11654",
    "IETrans",
    "28.9",
    28.9
   ],
   [
    "2107.02112",
    "DLFE",
    "22.1",
    22.1
   ],
   [
    "2009.07526",
    "CogTree",
    "22.0",
    22.0
   ],
   [
    "2009.00893",
    "PCPL",
    "19.3",
    19.3
   ],
   [
    "2002.11949",
    "TDE",
    "19.2",
    19.2
   ]
  ]
 },
 "./longdocdata/docs/994.json": {
  "question": "List the performance scores of various methods on the COCO-Stuff Labels-to-Photos (COCO-Stuff) dataset on the Image-to-Image Translation task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| CC-FPSE-AUG | 71.5 |\n| CC-FPSE | 70.7 |\n| SPADE | 67.9 |\n| Pix2PixHD-AUG | 54.1 |\n| pix2pixHD | 45.8 |\n| CRN | 40.4 |",
  "src_docs": [
   "1707.09405",
   "2011.12636",
   "1903.07291",
   "1910.06809",
   "1711.11585"
  ],
  "updated_answer": [
   [
    "2011.12636",
    "CC-FPSE-AUG",
    "71.5",
    71.5
   ],
   [
    "1910.06809",
    "CC-FPSE",
    "70.7%",
    70.7
   ],
   [
    "1903.07291",
    "SPADE",
    "67.9%",
    67.9
   ],
   [
    "2011.12636",
    "Pix2PixHD-AUG",
    "54.1",
    54.1
   ],
   [
    "1707.09405",
    "CRN",
    "40.4%",
    40.4
   ],
   [
    "1711.11585",
    "pix2pixHD",
    "45.8%",
    45.8
   ]
  ],
  "meta_info": {
   "datasets": "COCO-Stuff Labels-to-Photos",
   "datasets_short": "COCO-Stuff",
   "task": "Image-to-Image Translation",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2011.12636",
    "CC-FPSE-AUG",
    "71.5",
    71.5
   ],
   [
    "1910.06809",
    "CC-FPSE",
    "70.7%",
    70.7
   ],
   [
    "1903.07291",
    "SPADE",
    "67.9%",
    67.9
   ],
   [
    "1711.11585",
    "pix2pixHD",
    "45.8%",
    45.8
   ],
   [
    "1707.09405",
    "CRN",
    "40.4%",
    40.4
   ]
  ]
 },
 "./longdocdata/docs/995.json": {
  "question": "List the performance scores of various methods on the COCO-Stuff Labels-to-Photos (COCO-Stuff) dataset on the Image-to-Image Translation task using metric mIoU.",
  "answer": "| Method | mIoU |\n| --- | --- |\n| OASIS | 44.1 |\n| CC-FPSE-AUG | 42.1 |\n| CC-FPSE | 41.6 |\n| SPADE | 37.4 |\n| CRN | 23.7 |\n| Pix2PixHD-AUG | 21.9 |\n| pix2pixHD | 14.6 |\n| USIS | 14.06 |",
  "src_docs": [
   "2012.04781",
   "1707.09405",
   "2011.12636",
   "1903.07291",
   "1910.06809",
   "1711.11585",
   "2109.14715"
  ],
  "updated_answer": [
   [
    "2012.04781",
    "OASIS",
    "44.1",
    44.1
   ],
   [
    "2011.12636",
    "CC-FPSE-AUG",
    "42.1",
    42.1
   ],
   [
    "1910.06809",
    "CC-FPSE",
    "41.6",
    41.6
   ],
   [
    "1903.07291",
    "SPADE",
    "37.4",
    37.4
   ],
   [
    "2109.14715",
    "USIS",
    "14.06",
    14.06
   ],
   [
    "2011.12636",
    "Pix2PixHD-AUG",
    "21.9",
    21.9
   ],
   [
    "1707.09405",
    "CRN",
    "23.7",
    23.7
   ],
   [
    "1711.11585",
    "pix2pixHD",
    "14.6",
    14.6
   ]
  ],
  "meta_info": {
   "datasets": "COCO-Stuff Labels-to-Photos",
   "datasets_short": "COCO-Stuff",
   "task": "Image-to-Image Translation",
   "metric": "mIoU"
  },
  "updated_answer2": [
   [
    "2012.04781",
    "OASIS",
    "44.1",
    44.1
   ],
   [
    "2011.12636",
    "CC-FPSE-AUG",
    "42.1",
    42.1
   ],
   [
    "1910.06809",
    "CC-FPSE",
    "41.6",
    41.6
   ],
   [
    "1903.07291",
    "SPADE",
    "37.4",
    37.4
   ],
   [
    "1707.09405",
    "CRN",
    "23.7",
    23.7
   ],
   [
    "1711.11585",
    "pix2pixHD",
    "14.6",
    14.6
   ],
   [
    "2109.14715",
    "USIS",
    "14.06",
    14.06
   ]
  ]
 },
 "./longdocdata/docs/997.json": {
  "question": "List the performance scores of various methods on the COCO-Stuff 64x64 (COCO-Stuff) dataset on the Layout-to-Image Generation task using metric Inception Score.",
  "answer": "| Method | Inception Score |\n| --- | --- |\n| OC-GAN | 10.8 |\n| SOARISG | 10.3 |\n| LostGAN | 9.8 |\n| Layout2Im | 9.1 |\n| SG2Im | 7.3 |",
  "src_docs": [
   "1811.11389",
   "2003.07449",
   "1909.05379",
   "1804.01622",
   "1908.07500"
  ],
  "updated_answer": [
   [
    "2003.07449",
    "OC-GAN",
    "10.8",
    10.8
   ],
   [
    "1908.07500",
    "LostGAN",
    "9.8",
    9.8
   ],
   [
    "1811.11389",
    "Layout2Im",
    "9.1",
    9.1
   ],
   [
    "1909.05379",
    "SOARISG",
    "10.3",
    10.3
   ],
   [
    "1804.01622",
    "SG2Im",
    "7.3",
    7.3
   ]
  ],
  "meta_info": {
   "datasets": "COCO-Stuff 64x64",
   "datasets_short": "COCO-Stuff",
   "task": "Layout-to-Image Generation",
   "metric": "Inception Score"
  },
  "updated_answer2": [
   [
    "2003.07449",
    "OC-GAN",
    "10.8",
    10.8
   ],
   [
    "1909.05379",
    "SOARISG",
    "10.3",
    10.3
   ],
   [
    "1908.07500",
    "LostGAN",
    "9.8",
    9.8
   ],
   [
    "1811.11389",
    "Layout2Im",
    "9.1",
    9.1
   ],
   [
    "1804.01622",
    "SG2Im",
    "7.3",
    7.3
   ]
  ]
 },
 "./longdocdata/docs/999.json": {
  "question": "List the performance scores of various methods on the COCO-Stuff 128x128 (COCO-Stuff) dataset on the Layout-to-Image Generation task using metric FID.",
  "answer": "| Method | FID |\n| --- | --- |\n| CAL2IM | 22.32 |\n| LostGAN-V2 | 24.76 |\n| LostGAN | 29.65 |\n| OC-GAN | 36.31 |\n| SOARISG | 59.5 |",
  "src_docs": [
   "2003.07449",
   "1909.05379",
   "2003.11571",
   "2103.11897",
   "1908.07500"
  ],
  "updated_answer": [
   [
    "2103.11897",
    "CAL2IM",
    "22.32",
    22.32
   ],
   [
    "2003.11571",
    "LostGAN-V2",
    "24.76",
    24.76
   ],
   [
    "1908.07500",
    "LostGAN",
    "29.65",
    29.65
   ],
   [
    "2003.07449",
    "OC-GAN",
    "36.31",
    36.31
   ],
   [
    "1909.05379",
    "SOARISG",
    "59.5",
    59.5
   ]
  ],
  "meta_info": {
   "datasets": "COCO-Stuff 128x128",
   "datasets_short": "COCO-Stuff",
   "task": "Layout-to-Image Generation",
   "metric": "FID"
  },
  "updated_answer2": [
   [
    "2103.11897",
    "CAL2IM",
    "22.32",
    22.32
   ],
   [
    "2003.11571",
    "LostGAN-V2",
    "24.76",
    24.76
   ],
   [
    "1908.07500",
    "LostGAN",
    "29.65",
    29.65
   ],
   [
    "2003.07449",
    "OC-GAN",
    "36.31",
    36.31
   ],
   [
    "1909.05379",
    "SOARISG",
    "59.5",
    59.5
   ]
  ]
 },
 "./longdocdata/docs/1001.json": {
  "question": "List the performance scores of various methods on the COCO-Stuff 128x128 (COCO-Stuff) dataset on the Layout-to-Image Generation task using metric Inception Score.",
  "answer": "| Method | Inception Score |\n| --- | --- |\n| CAL2IM | 15.62 |\n| OC-GAN | 14.6 |\n| LostGAN-V2 | 14.21 |\n| LostGAN | 13.8 |\n| SOARISG | 12.5 |",
  "src_docs": [
   "2003.07449",
   "1909.05379",
   "2003.11571",
   "2103.11897",
   "1908.07500"
  ],
  "updated_answer": [
   [
    "2103.11897",
    "CAL2IM",
    "15.62",
    15.62
   ],
   [
    "2003.11571",
    "LostGAN-V2",
    "14.21",
    14.21
   ],
   [
    "1908.07500",
    "LostGAN",
    "13.8",
    13.8
   ],
   [
    "2003.07449",
    "OC-GAN",
    "14.6",
    14.6
   ],
   [
    "1909.05379",
    "SOARISG",
    "12.5",
    12.5
   ]
  ],
  "meta_info": {
   "datasets": "COCO-Stuff 128x128",
   "datasets_short": "COCO-Stuff",
   "task": "Layout-to-Image Generation",
   "metric": "Inception Score"
  },
  "updated_answer2": [
   [
    "2103.11897",
    "CAL2IM",
    "15.62",
    15.62
   ],
   [
    "2003.07449",
    "OC-GAN",
    "14.6",
    14.6
   ],
   [
    "2003.11571",
    "LostGAN-V2",
    "14.21",
    14.21
   ],
   [
    "1908.07500",
    "LostGAN",
    "13.8",
    13.8
   ],
   [
    "1909.05379",
    "SOARISG",
    "12.5",
    12.5
   ]
  ]
 },
 "./longdocdata/docs/1004.json": {
  "question": "List the performance scores of various methods on the COCO-Stuff-27 (COCO-Stuff) dataset on the Unsupervised Semantic Segmentation task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| ViCE | 64.75 |\n| STEGO | 56.9 |\n| PiCIE+H | 50.0 |\n| PiCIE | 48.1 |\n| Ours (SlotCon) | 42.36 |\n| IIC | 21.8 |",
  "src_docs": [
   "2103.17070",
   "2205.15288",
   "1807.06653",
   "2203.08414",
   "2111.12460"
  ],
  "updated_answer": [
   [
    "2111.12460",
    "ViCE",
    "64.75",
    64.75
   ],
   [
    "2203.08414",
    "STEGO",
    "56.9",
    56.9
   ],
   [
    "2103.17070",
    "PiCIE+H",
    "50.0",
    50.0
   ],
   [
    "2103.17070",
    "PiCIE",
    "48.1",
    48.1
   ],
   [
    "2205.15288",
    "Ours (SlotCon)",
    "42.36",
    42.36
   ],
   [
    "1807.06653",
    "IIC",
    "21.8",
    21.8
   ]
  ],
  "meta_info": {
   "datasets": "COCO-Stuff-27",
   "datasets_short": "COCO-Stuff",
   "task": "Unsupervised Semantic Segmentation",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2111.12460",
    "ViCE",
    "64.75",
    64.75
   ],
   [
    "2203.08414",
    "STEGO",
    "56.9",
    56.9
   ],
   [
    "2103.17070",
    "PiCIE+H",
    "50.0",
    50.0
   ],
   [
    "2205.15288",
    "Ours (SlotCon)",
    "42.36",
    42.36
   ],
   [
    "1807.06653",
    "IIC",
    "21.8",
    21.8
   ]
  ]
 },
 "./longdocdata/docs/1016.json": {
  "question": "List the performance scores of various methods on the iLIDS-VID (iLIDS-VID) dataset on the Person Re-Identification task using metric Rank-1.",
  "answer": "| Method | Rank-1 |\n| --- | --- |\n| FGReID | 91.5 |\n| STRF | 89.3 |\n| AGRL | 83.7 |\n| uPMnet | 63.1 |\n| TKP | 54.6 |\n| UTAL | 35.1 |",
  "src_docs": [
   "1908.03885",
   "1903.00535",
   "1909.02240",
   "2011.13475",
   "2111.05170",
   "2107.11878"
  ],
  "updated_answer": [
   [
    "2011.13475",
    "FGReID",
    "91.5",
    91.5
   ],
   [
    "2107.11878",
    "STRF",
    "89.3",
    89.3
   ],
   [
    "1909.02240",
    "AGRL",
    "83.7",
    83.7
   ],
   [
    "2111.05170",
    "uPMnet",
    "63.1",
    63.1
   ],
   [
    "1908.03885",
    "TKP",
    "54.6",
    54.6
   ],
   [
    "1903.00535",
    "UTAL",
    "35.1",
    35.1
   ]
  ],
  "meta_info": {
   "datasets": "iLIDS-VID",
   "datasets_short": "iLIDS-VID",
   "task": "Person Re-Identification",
   "metric": "Rank-1"
  },
  "updated_answer2": [
   [
    "2011.13475",
    "FGReID",
    "91.5",
    91.5
   ],
   [
    "2107.11878",
    "STRF",
    "89.3",
    89.3
   ],
   [
    "1909.02240",
    "AGRL",
    "83.7",
    83.7
   ],
   [
    "2111.05170",
    "uPMnet",
    "63.1",
    63.1
   ],
   [
    "1908.03885",
    "TKP",
    "54.6",
    54.6
   ],
   [
    "1903.00535",
    "UTAL",
    "35.1",
    35.1
   ]
  ]
 },
 
 "./longdocdata/docs/1065.json": {
  "question": "List the performance scores of various methods on the PRID2011 (PRID2011) dataset on the Person Re-Identification task using metric Rank-20.",
  "answer": "| Method | Rank-20 |\n| --- | --- |\n| FGReID | 100.0 |\n| uPMnet | 100.0 |\n| AGRL | 99.8 |\n| DAL | 99.6 |\n| DGM+MLAPG+ | 99.0 |\n| TAUDL | 98.9 |\n| DGM+IDE+ | 96.4 |\n| UTAL | 96.2 |",
  "src_docs": [
   "1903.00535",
   "1809.02874",
   "1808.07301",
   "1909.02240",
   "2011.13475",
   "2111.05170",
   "1709.09297"
  ],
  "updated_answer": [
   [
    "2011.13475",
    "FGReID",
    "100",
    100.0
   ],
   [
    "1909.02240",
    "AGRL",
    "99.8",
    99.8
   ],
   [
    "2111.05170",
    "uPMnet",
    "100.0",
    100.0
   ],
   [
    "1808.07301",
    "DAL",
    "99.6",
    99.6
   ],
   [
    "1709.09297",
    "DGM+MLAPG+",
    "99.0",
    99.0
   ],
   [
    "1709.09297",
    "DGM+IDE+",
    "96.4",
    96.4
   ],
   [
    "1903.00535",
    "UTAL",
    "96.2",
    96.2
   ],
   [
    "1809.02874",
    "TAUDL",
    "98.9",
    98.9
   ]
  ],
  "meta_info": {
   "datasets": "PRID2011",
   "datasets_short": "PRID2011",
   "task": "Person Re-Identification",
   "metric": "Rank-20"
  },
  "updated_answer2": [
   [
    "2011.13475",
    "FGReID",
    "100",
    100.0
   ],
   [
    "2111.05170",
    "uPMnet",
    "100.0",
    100.0
   ],
   [
    "1909.02240",
    "AGRL",
    "99.8",
    99.8
   ],
   [
    "1808.07301",
    "DAL",
    "99.6",
    99.6
   ],
   [
    "1709.09297",
    "DGM+MLAPG+",
    "99.0",
    99.0
   ],
   [
    "1809.02874",
    "TAUDL",
    "98.9",
    98.9
   ],
   [
    "1903.00535",
    "UTAL",
    "96.2",
    96.2
   ]
  ]
 },
 "./longdocdata/docs/1096.json": {
  "question": "List the performance scores of various methods on the VehicleID Medium (VehicleID) dataset on the Vehicle Re-Identification task using metric Rank-1.",
  "answer": "| Method | Rank-1 |\n| --- | --- |\n| Recall@k Surrogate loss (ViT-B/16) | 95.2 |\n| Recall@k Surrogate loss (ResNet-50) | 94.6 |\n| PNP Loss | 94.2 |\n| RPTM | 93.3 |\n| Smooth-AP | 93.3 |\n| ANet | 82.8 |\n| vehiclenet | 81.35 |\n| CAL | 78.2 |",
  "src_docs": [
   "2108.11179",
   "2004.06305",
   "2108.08728",
   "2110.07933",
   "2102.04640",
   "2102.03898",
   "2007.12163"
  ],
  "updated_answer": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "95.2",
    95.2
   ],
   [
    "2108.11179",
    "Recall@k Surrogate loss (ResNet-50)",
    "94.6",
    94.6
   ],
   [
    "2102.04640",
    "PNP Loss",
    "94.2",
    94.2
   ],
   [
    "2110.07933",
    "RPTM",
    "93.3",
    93.3
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "93.3",
    93.3
   ],
   [
    "2102.03898",
    "ANet",
    "82.8",
    82.8
   ],
   [
    "2004.06305",
    "vehiclenet",
    "81.35",
    81.35
   ],
   [
    "2108.08728",
    "CAL",
    "78.2",
    78.2
   ]
  ],
  "meta_info": {
   "datasets": "VehicleID Medium",
   "datasets_short": "VehicleID",
   "task": "Vehicle Re-Identification",
   "metric": "Rank-1"
  },
  "updated_answer2": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "95.2",
    95.2
   ],
   [
    "2102.04640",
    "PNP Loss",
    "94.2",
    94.2
   ],
   [
    "2110.07933",
    "RPTM",
    "93.3",
    93.3
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "93.3",
    93.3
   ],
   [
    "2102.03898",
    "ANet",
    "82.8",
    82.8
   ],
   [
    "2004.06305",
    "vehiclenet",
    "81.35",
    81.35
   ],
   [
    "2108.08728",
    "CAL",
    "78.2",
    78.2
   ]
  ]
 },
 "./longdocdata/docs/1097.json": {
  "question": "List the performance scores of various methods on the VehicleID Medium (VehicleID) dataset on the Vehicle Re-Identification task using metric Rank-5.",
  "answer": "| Method | Rank-5 |\n| --- | --- |\n| Recall@k Surrogate loss (ViT-B/16) | 97.2 |\n| Recall@k Surrogate loss (ResNet-50) | 96.9 |\n| PNP Loss | 96.9 |\n| RPTM | 96.5 |\n| Smooth-AP | 96.4 |\n| ANet | 96.2 |",
  "src_docs": [
   "2108.11179",
   "2110.07933",
   "2102.04640",
   "2102.03898",
   "2007.12163"
  ],
  "updated_answer": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "97.2",
    97.2
   ],
   [
    "2108.11179",
    "Recall@k Surrogate loss (ResNet-50)",
    "96.9",
    96.9
   ],
   [
    "2102.04640",
    "PNP Loss",
    "96.9",
    96.9
   ],
   [
    "2110.07933",
    "RPTM",
    "96.5",
    96.5
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "96.4",
    96.4
   ],
   [
    "2102.03898",
    "ANet",
    "96.2",
    96.2
   ]
  ],
  "meta_info": {
   "datasets": "VehicleID Medium",
   "datasets_short": "VehicleID",
   "task": "Vehicle Re-Identification",
   "metric": "Rank-5"
  },
  "updated_answer2": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "97.2",
    97.2
   ],
   [
    "2102.04640",
    "PNP Loss",
    "96.9",
    96.9
   ],
   [
    "2110.07933",
    "RPTM",
    "96.5",
    96.5
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "96.4",
    96.4
   ],
   [
    "2102.03898",
    "ANet",
    "96.2",
    96.2
   ]
  ]
 },
 "./longdocdata/docs/1101.json": {
  "question": "List the performance scores of various methods on the VehicleID Large (VehicleID) dataset on the Vehicle Re-Identification task using metric Rank-1.",
  "answer": "| Method | Rank-1 |\n| --- | --- |\n| Recall@k Surrogate loss (ViT-B/16) | 94.7 |\n| Recall@k Surrogate loss (ResNet-50) | 93.8 |\n| PNP Loss | 93.2 |\n| RPTM | 92.9 |\n| Smooth-AP | 91.9 |\n| ANet | 80.5 |\n| vehiclenet | 79.46 |\n| CAL | 75.1 |",
  "src_docs": [
   "2108.11179",
   "2004.06305",
   "2108.08728",
   "2110.07933",
   "2102.04640",
   "2102.03898",
   "2007.12163"
  ],
  "updated_answer": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "94.7",
    94.7
   ],
   [
    "2108.11179",
    "Recall@k Surrogate loss (ResNet-50)",
    "93.8",
    93.8
   ],
   [
    "2102.04640",
    "PNP Loss",
    "93.2",
    93.2
   ],
   [
    "2110.07933",
    "RPTM",
    "92.9",
    92.9
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "91.9",
    91.9
   ],
   [
    "2102.03898",
    "ANet",
    "80.5",
    80.5
   ],
   [
    "2004.06305",
    "vehiclenet",
    "79.46",
    79.46
   ],
   [
    "2108.08728",
    "CAL",
    "75.1",
    75.1
   ]
  ],
  "meta_info": {
   "datasets": "VehicleID Large",
   "datasets_short": "VehicleID",
   "task": "Vehicle Re-Identification",
   "metric": "Rank-1"
  },
  "updated_answer2": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "94.7",
    94.7
   ],
   [
    "2102.04640",
    "PNP Loss",
    "93.2",
    93.2
   ],
   [
    "2110.07933",
    "RPTM",
    "92.9",
    92.9
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "91.9",
    91.9
   ],
   [
    "2102.03898",
    "ANet",
    "80.5",
    80.5
   ],
   [
    "2004.06305",
    "vehiclenet",
    "79.46",
    79.46
   ],
   [
    "2108.08728",
    "CAL",
    "75.1",
    75.1
   ]
  ]
 },
 "./longdocdata/docs/1102.json": {
  "question": "List the performance scores of various methods on the VehicleID Large (VehicleID) dataset on the Vehicle Re-Identification task using metric Rank-5.",
  "answer": "| Method | Rank-5 |\n| --- | --- |\n| Recall@k Surrogate loss (ViT-B/16) | 97.1 |\n| Recall@k Surrogate loss (ResNet-50) | 96.6 |\n| PNP Loss | 96.6 |\n| RPTM | 96.3 |\n| Smooth-AP | 96.2 |\n| ANet | 94.6 |",
  "src_docs": [
   "2108.11179",
   "2110.07933",
   "2102.04640",
   "2102.03898",
   "2007.12163"
  ],
  "updated_answer": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "97.1",
    97.1
   ],
   [
    "2108.11179",
    "Recall@k Surrogate loss (ResNet-50)",
    "96.6",
    96.6
   ],
   [
    "2102.04640",
    "PNP Loss",
    "96.6",
    96.6
   ],
   [
    "2110.07933",
    "RPTM",
    "96.3",
    96.3
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "96.2",
    96.2
   ],
   [
    "2102.03898",
    "ANet",
    "94.6",
    94.6
   ]
  ],
  "meta_info": {
   "datasets": "VehicleID Large",
   "datasets_short": "VehicleID",
   "task": "Vehicle Re-Identification",
   "metric": "Rank-5"
  },
  "updated_answer2": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "97.1",
    97.1
   ],
   [
    "2102.04640",
    "PNP Loss",
    "96.6",
    96.6
   ],
   [
    "2110.07933",
    "RPTM",
    "96.3",
    96.3
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "96.2",
    96.2
   ],
   [
    "2102.03898",
    "ANet",
    "94.6",
    94.6
   ]
  ]
 },
 "./longdocdata/docs/1106.json": {
  "question": "List the performance scores of various methods on the VehicleID Small (VehicleID) dataset on the Vehicle Re-Identification task using metric Rank-1.",
  "answer": "| Method | Rank-1 |\n| --- | --- |\n| Recall@k Surrogate loss (ViT-B/16) | 96.2 |\n| Recall@k Surrogate loss (ResNet-50) | 95.7 |\n| PNP Loss | 95.5 |\n| RPTM | 95.5 |\n| Smooth-AP | 94.9 |\n| ANet | 87.9 |\n| vehiclenet | 83.64 |\n| CAL | 82.5 |",
  "src_docs": [
   "2108.11179",
   "2004.06305",
   "2108.08728",
   "2110.07933",
   "2102.04640",
   "2102.03898",
   "2007.12163"
  ],
  "updated_answer": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "96.2",
    96.2
   ],
   [
    "2108.11179",
    "Recall@k Surrogate loss (ResNet-50)",
    "95.7",
    95.7
   ],
   [
    "2102.04640",
    "PNP Loss",
    "95.5",
    95.5
   ],
   [
    "2110.07933",
    "RPTM",
    "95.5",
    95.5
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "94.9",
    94.9
   ],
   [
    "2102.03898",
    "ANet",
    "87.9",
    87.9
   ],
   [
    "2004.06305",
    "vehiclenet",
    "83.64",
    83.64
   ],
   [
    "2108.08728",
    "CAL",
    "82.5",
    82.5
   ]
  ],
  "meta_info": {
   "datasets": "VehicleID Small",
   "datasets_short": "VehicleID",
   "task": "Vehicle Re-Identification",
   "metric": "Rank-1"
  },
  "updated_answer2": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "96.2",
    96.2
   ],
   [
    "2102.04640",
    "PNP Loss",
    "95.5",
    95.5
   ],
   [
    "2110.07933",
    "RPTM",
    "95.5",
    95.5
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "94.9",
    94.9
   ],
   [
    "2102.03898",
    "ANet",
    "87.9",
    87.9
   ],
   [
    "2004.06305",
    "vehiclenet",
    "83.64",
    83.64
   ],
   [
    "2108.08728",
    "CAL",
    "82.5",
    82.5
   ]
  ]
 },
 "./longdocdata/docs/1107.json": {
  "question": "List the performance scores of various methods on the VehicleID Small (VehicleID) dataset on the Vehicle Re-Identification task using metric Rank-5.",
  "answer": "| Method | Rank-5 |\n| --- | --- |\n| Recall@k Surrogate loss (ViT-B/16) | 98.0 |\n| Recall@k Surrogate loss (ResNet-50) | 97.9 |\n| PNP Loss | 97.8 |\n| ANet | 97.8 |\n| Smooth-AP | 97.6 |\n| RPTM | 97.4 |",
  "src_docs": [
   "2108.11179",
   "2110.07933",
   "2102.04640",
   "2102.03898",
   "2007.12163"
  ],
  "updated_answer": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "98.0",
    98.0
   ],
   [
    "2108.11179",
    "Recall@k Surrogate loss (ResNet-50)",
    "97.9",
    97.9
   ],
   [
    "2102.04640",
    "PNP Loss",
    "97.8",
    97.8
   ],
   [
    "2110.07933",
    "RPTM",
    "97.4",
    97.4
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "97.6",
    97.6
   ],
   [
    "2102.03898",
    "ANet",
    "97.8",
    97.8
   ]
  ],
  "meta_info": {
   "datasets": "VehicleID Small",
   "datasets_short": "VehicleID",
   "task": "Vehicle Re-Identification",
   "metric": "Rank-5"
  },
  "updated_answer2": [
   [
    "2108.11179",
    "Recall@k Surrogate loss (ViT-B/16)",
    "98.0",
    98.0
   ],
   [
    "2102.04640",
    "PNP Loss",
    "97.8",
    97.8
   ],
   [
    "2102.03898",
    "ANet",
    "97.8",
    97.8
   ],
   [
    "2007.12163",
    "Smooth-AP",
    "97.6",
    97.6
   ],
   [
    "2110.07933",
    "RPTM",
    "97.4",
    97.4
   ]
  ]
 },
 "./longdocdata/docs/1125.json": {
  "question": "List the performance scores of various methods on the CUB-200-2011 (CUB-200-2011) dataset on the Zero-Shot Learning task using metric average top-1 classification accuracy.",
  "answer": "| Method | average top-1 classification accuracy |\n| --- | --- |\n| Composer | 69.4 |\n| ZSL_TF-VAEGAN | 64.9 |\n| f-VAEGAN-D2 | 61.0 |\n| TCN | 59.5 |\n| LisGAN | 58.8 |\n| Cycle-WGAN | 58.6 |\n| f-CLSWGAN | 57.3 |",
  "src_docs": [
   "1903.10132",
   "2105.10438",
   "2003.07833",
   "1904.04092",
   "1712.00981",
   "1908.05832",
   "1808.00136"
  ],
  "updated_answer": [
   [
    "2105.10438",
    "Composer",
    "69.4",
    69.4
   ],
   [
    "2003.07833",
    "ZSL_TF-VAEGAN",
    "64.9",
    64.9
   ],
   [
    "1903.10132",
    "f-VAEGAN-D2",
    "61.0",
    61.0
   ],
   [
    "1908.05832",
    "TCN",
    "59.5",
    59.5
   ],
   [
    "1904.04092",
    "LisGAN",
    "58.8",
    58.8
   ],
   [
    "1808.00136",
    "Cycle-WGAN",
    "58.6",
    58.6
   ],
   [
    "1712.00981",
    "f-CLSWGAN",
    "57.3",
    57.3
   ]
  ],
  "meta_info": {
   "datasets": "CUB-200-2011",
   "datasets_short": "CUB-200-2011",
   "task": "Zero-Shot Learning",
   "metric": "average top-1 classification accuracy"
  },
  "updated_answer2": [
   [
    "2105.10438",
    "Composer",
    "69.4",
    69.4
   ],
   [
    "2003.07833",
    "ZSL_TF-VAEGAN",
    "64.9",
    64.9
   ],
   [
    "1903.10132",
    "f-VAEGAN-D2",
    "61.0",
    61.0
   ],
   [
    "1908.05832",
    "TCN",
    "59.5",
    59.5
   ],
   [
    "1904.04092",
    "LisGAN",
    "58.8",
    58.8
   ],
   [
    "1808.00136",
    "Cycle-WGAN",
    "58.6",
    58.6
   ],
   [
    "1712.00981",
    "f-CLSWGAN",
    "57.3",
    57.3
   ]
  ]
 },
 "./longdocdata/docs/1190.json": {
  "question": "List the performance scores of various methods on the CoQA (CoQA) dataset on the Question Answering task using metric Overall.",
  "answer": "| Method | Overall |\n| --- | --- |\n| GPT-3 175B (Few-Shot) | 85.0 |\n| BERT Large Augmented (single model) | 81.1 |\n| SDNet (ensemble) | 79.3 |\n| BERT-base finetune (single model) | 78.1 |\n| SDNet (single model) | 76.6 |\n| FlowQA (single model) | 75.0 |\n| BiDAF++ (single model) | 67.8 |\n| DrQA + seq2seq with copy attention (single model) | 65.1 |\n| Vanilla DrQA (single model) | 52.6 |",
  "src_docs": [
   "1810.06683",
   "1809.10735",
   "2005.14165",
   "1810.04805",
   "1808.07042",
   "1812.03593"
  ],
  "updated_answer": [
   [
    "1810.04805",
    "BERT Large Augmented (single model)",
    "81.1",
    81.1
   ],
   [
    "1810.04805",
    "BERT-base finetune (single model)",
    "78.1",
    78.1
   ],
   [
    "1809.10735",
    "BiDAF++ (single model)",
    "67.8",
    67.8
   ],
   [
    "1808.07042",
    "DrQA + seq2seq with copy attention (single model)",
    "65.1",
    65.1
   ],
   [
    "1808.07042",
    "Vanilla DrQA (single model)",
    "52.6",
    52.6
   ],
   [
    "1810.06683",
    "FlowQA (single model)",
    "75.0",
    75.0
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "85",
    85.0
   ],
   [
    "1812.03593",
    "SDNet (ensemble)",
    "79.3",
    79.3
   ],
   [
    "1812.03593",
    "SDNet (single model)",
    "76.6",
    76.6
   ]
  ],
  "meta_info": {
   "datasets": "CoQA",
   "datasets_short": "CoQA",
   "task": "Question Answering",
   "metric": "Overall"
  },
  "updated_answer2": [
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "85",
    85.0
   ],
   [
    "1810.04805",
    "BERT Large Augmented (single model)",
    "81.1",
    81.1
   ],
   [
    "1812.03593",
    "SDNet (ensemble)",
    "79.3",
    79.3
   ],
   [
    "1810.06683",
    "FlowQA (single model)",
    "75.0",
    75.0
   ],
   [
    "1809.10735",
    "BiDAF++ (single model)",
    "67.8",
    67.8
   ],
   [
    "1808.07042",
    "DrQA + seq2seq with copy attention (single model)",
    "65.1",
    65.1
   ]
  ]
 },
 "./longdocdata/docs/1204.json": {
  "question": "List the performance scores of various methods on the NewsQA (NewsQA) dataset on the Question Answering task using metric F1.",
  "answer": "| Method | F1 |\n| --- | --- |\n| SpanBERT | 73.6 |\n| LinkBERT (large) | 72.6 |\n| DecaProp | 66.3 |\n| AMANDA | 63.7 |\n| MINIMAL(Dyn) | 63.2 |\n| FastQAExt | 56.1 |",
  "src_docs": [
   "1907.10529",
   "1805.08092",
   "1801.08290",
   "2203.15827",
   "1811.04210",
   "1703.04816"
  ],
  "updated_answer": [
   [
    "1811.04210",
    "DecaProp",
    "66.3",
    66.3
   ],
   [
    "1805.08092",
    "MINIMAL(Dyn)",
    "63.2",
    63.2
   ],
   [
    "1801.08290",
    "AMANDA",
    "63.7",
    63.7
   ],
   [
    "1703.04816",
    "FastQAExt",
    "56.1",
    56.1
   ],
   [
    "1907.10529",
    "SpanBERT",
    "73.6",
    73.6
   ],
   [
    "2203.15827",
    "LinkBERT (large)",
    "72.6",
    72.6
   ]
  ],
  "meta_info": {
   "datasets": "NewsQA",
   "datasets_short": "NewsQA",
   "task": "Question Answering",
   "metric": "F1"
  },
  "updated_answer2": [
   [
    "1907.10529",
    "SpanBERT",
    "73.6",
    73.6
   ],
   [
    "2203.15827",
    "LinkBERT (large)",
    "72.6",
    72.6
   ],
   [
    "1811.04210",
    "DecaProp",
    "66.3",
    66.3
   ],
   [
    "1801.08290",
    "AMANDA",
    "63.7",
    63.7
   ],
   [
    "1805.08092",
    "MINIMAL(Dyn)",
    "63.2",
    63.2
   ],
   [
    "1703.04816",
    "FastQAExt",
    "56.1",
    56.1
   ]
  ]
 },
 "./longdocdata/docs/1219.json": {
  "question": "List the performance scores of various methods on the SQuAD1.1 (SQuAD) dataset on the Question Generation task using metric BLEU-4.",
  "answer": "| Method | BLEU-4 |\n| --- | --- |\n| ERNIE-GENLARGE (beam size=5) | 25.41 |\n| UniLMv2 | 24.43 |\n| ProphetNet | 23.91 |\n| UniLM | 22.78 |\n| Selector & NQG++ | 15.874 |\n| RNN +attn +copy | 13.5 |\n| NQG++ | 13.27 |",
  "src_docs": [
   "2001.11314",
   "2002.12804",
   "1905.03197",
   "1909.01953",
   "1902.11049",
   "2001.04063",
   "1704.01792"
  ],
  "updated_answer": [
   [
    "2001.11314",
    "ERNIE-GENLARGE (beam size=5)",
    "25.41",
    25.41
   ],
   [
    "2002.12804",
    "UniLMv2",
    "24.43",
    24.43
   ],
   [
    "2001.04063",
    "ProphetNet",
    "23.91",
    23.91
   ],
   [
    "1905.03197",
    "UniLM",
    "22.78",
    22.78
   ],
   [
    "1909.01953",
    "Selector & NQG++",
    "15.874",
    15.874
   ],
   [
    "1902.11049",
    "RNN +attn +copy",
    "13.5",
    13.5
   ],
   [
    "1704.01792",
    "NQG++",
    "13.27",
    13.27
   ]
  ],
  "meta_info": {
   "datasets": "SQuAD1.1",
   "datasets_short": "SQuAD",
   "task": "Question Generation",
   "metric": "BLEU-4"
  },
  "updated_answer2": [
   [
    "2001.11314",
    "ERNIE-GENLARGE (beam size=5)",
    "25.41",
    25.41
   ],
   [
    "2002.12804",
    "UniLMv2",
    "24.43",
    24.43
   ],
   [
    "2001.04063",
    "ProphetNet",
    "23.91",
    23.91
   ],
   [
    "1905.03197",
    "UniLM",
    "22.78",
    22.78
   ],
   [
    "1909.01953",
    "Selector & NQG++",
    "15.874",
    15.874
   ],
   [
    "1902.11049",
    "RNN +attn +copy",
    "13.5",
    13.5
   ],
   [
    "1704.01792",
    "NQG++",
    "13.27",
    13.27
   ]
  ]
 },
 "./longdocdata/docs/1236.json": {
  "question": "List the performance scores of various methods on the NarrativeQA (NarrativeQA) dataset on the Question Answering task using metric BLEU-1.",
  "answer": "| Method | BLEU-1 |\n| --- | --- |\n| Oracle IR Models | 54.6 |\n| Masque (NarrativeQA + MS MARCO) | 54.11 |\n| Masque (NarrativeQA only) | 48.7 |\n| DecaProp | 44.35 |\n| MHPGM + NOIC | 43.63 |\n| FiD+Distil | 35.3 |\n| BiDAF | 33.45 |",
  "src_docs": [
   "2012.04584",
   "1809.06309",
   "1901.02262",
   "1712.07040",
   "1611.01603",
   "1811.04210"
  ],
  "updated_answer": [
   [
    "1901.02262",
    "Masque (NarrativeQA + MS MARCO)",
    "54.11",
    54.11
   ],
   [
    "1901.02262",
    "Masque (NarrativeQA only)",
    "48.7",
    48.7
   ],
   [
    "1811.04210",
    "DecaProp",
    "44.35",
    44.35
   ],
   [
    "1809.06309",
    "MHPGM + NOIC",
    "43.63",
    43.63
   ],
   [
    "1611.01603",
    "BiDAF",
    "33.45",
    33.45
   ],
   [
    "2012.04584",
    "FiD+Distil",
    "35.3",
    35.3
   ],
   [
    "1712.07040",
    "Oracle IR Models",
    "54.60/55.55",
    54.6
   ]
  ],
  "meta_info": {
   "datasets": "NarrativeQA",
   "datasets_short": "NarrativeQA",
   "task": "Question Answering",
   "metric": "BLEU-1"
  },
  "updated_answer2": [
   [
    "1712.07040",
    "Oracle IR Models",
    "54.60/55.55",
    54.6
   ],
   [
    "1901.02262",
    "Masque (NarrativeQA + MS MARCO)",
    "54.11",
    54.11
   ],
   [
    "1811.04210",
    "DecaProp",
    "44.35",
    44.35
   ],
   [
    "1809.06309",
    "MHPGM + NOIC",
    "43.63",
    43.63
   ],
   [
    "2012.04584",
    "FiD+Distil",
    "35.3",
    35.3
   ],
   [
    "1611.01603",
    "BiDAF",
    "33.45",
    33.45
   ]
  ]
 },
 "./longdocdata/docs/1237.json": {
  "question": "List the performance scores of various methods on the NarrativeQA (NarrativeQA) dataset on the Question Answering task using metric Rouge-L.",
  "answer": "| Method | Rouge-L |\n| --- | --- |\n| Masque (NarrativeQA + MS MARCO) | 59.87 |\n| BERT-QA with Hard EM objective | 58.8 |\n| Masque (NarrativeQA only) | 54.74 |\n| DecaProp | 44.69 |\n| MHPGM + NOIC | 44.16 |\n| BiDAF | 36.74 |\n| FiD+Distil | 32.0 |",
  "src_docs": [
   "1909.04849",
   "1809.06309",
   "1901.02262",
   "2012.04584",
   "1611.01603",
   "1811.04210"
  ],
  "updated_answer": [
   [
    "1901.02262",
    "Masque (NarrativeQA + MS MARCO)",
    "59.87",
    59.87
   ],
   [
    "1909.04849",
    "BERT-QA with Hard EM objective",
    "58.8",
    58.8
   ],
   [
    "1901.02262",
    "Masque (NarrativeQA only)",
    "54.74",
    54.74
   ],
   [
    "1811.04210",
    "DecaProp",
    "44.69",
    44.69
   ],
   [
    "1809.06309",
    "MHPGM + NOIC",
    "44.16",
    44.16
   ],
   [
    "1611.01603",
    "BiDAF",
    "36.74",
    36.74
   ],
   [
    "2012.04584",
    "FiD+Distil",
    "32",
    32.0
   ]
  ],
  "meta_info": {
   "datasets": "NarrativeQA",
   "datasets_short": "NarrativeQA",
   "task": "Question Answering",
   "metric": "Rouge-L"
  },
  "updated_answer2": [
   [
    "1901.02262",
    "Masque (NarrativeQA + MS MARCO)",
    "59.87",
    59.87
   ],
   [
    "1909.04849",
    "BERT-QA with Hard EM objective",
    "58.8",
    58.8
   ],
   [
    "1811.04210",
    "DecaProp",
    "44.69",
    44.69
   ],
   [
    "1809.06309",
    "MHPGM + NOIC",
    "44.16",
    44.16
   ],
   [
    "1611.01603",
    "BiDAF",
    "36.74",
    36.74
   ],
   [
    "2012.04584",
    "FiD+Distil",
    "32",
    32.0
   ]
  ]
 },
 "./longdocdata/docs/1238.json": {
  "question": "List the performance scores of various methods on the NarrativeQA (NarrativeQA) dataset on the Question Answering task using metric BLEU-4.",
  "answer": "| Method | BLEU-4 |\n| --- | --- |\n| Masque (NarrativeQA + MS MARCO) | 30.43 |\n| DecaProp | 27.61 |\n| Oracle IR Models | 26.71 |\n| MHPGM + NOIC | 21.07 |\n| Masque (NarrativeQA only) | 20.98 |\n| BiDAF | 15.69 |\n| FiD+Distil | 7.5 |",
  "src_docs": [
   "1809.06309",
   "1901.02262",
   "2012.04584",
   "1712.07040",
   "1611.01603",
   "1811.04210"
  ],
  "updated_answer": [
   [
    "1901.02262",
    "Masque (NarrativeQA + MS MARCO)",
    "30.43",
    30.43
   ],
   [
    "1901.02262",
    "Masque (NarrativeQA only)",
    "20.98",
    20.98
   ],
   [
    "1811.04210",
    "DecaProp",
    "27.61",
    27.61
   ],
   [
    "1809.06309",
    "MHPGM + NOIC",
    "21.07",
    21.07
   ],
   [
    "1611.01603",
    "BiDAF",
    "15.69",
    15.69
   ],
   [
    "2012.04584",
    "FiD+Distil",
    "7.5",
    7.5
   ],
   [
    "1712.07040",
    "Oracle IR Models",
    "26.71/27.78",
    26.71
   ]
  ],
  "meta_info": {
   "datasets": "NarrativeQA",
   "datasets_short": "NarrativeQA",
   "task": "Question Answering",
   "metric": "BLEU-4"
  },
  "updated_answer2": [
   [
    "1901.02262",
    "Masque (NarrativeQA + MS MARCO)",
    "30.43",
    30.43
   ],
   [
    "1811.04210",
    "DecaProp",
    "27.61",
    27.61
   ],
   [
    "1712.07040",
    "Oracle IR Models",
    "26.71/27.78",
    26.71
   ],
   [
    "1809.06309",
    "MHPGM + NOIC",
    "21.07",
    21.07
   ],
   [
    "1611.01603",
    "BiDAF",
    "15.69",
    15.69
   ],
   [
    "2012.04584",
    "FiD+Distil",
    "7.5",
    7.5
   ]
  ]
 },
 "./longdocdata/docs/1249.json": {
  "question": "List the performance scores of various methods on the MultiRC (SuperGLUE) dataset on the Question Answering task using metric F1.",
  "answer": "| Method | F1 |\n| --- | --- |\n| PaLM 540B (finetuned)  | 90.1 |\n| DeBERTa-1.5B | 88.2 |\n| T5-11B | 88.1 |\n| FLAN 137B (zero-shot) | 77.5 |\n| GPT-3 175B (Few-Shot) | 75.4 |\n| KELM (finetuning BERT-large based single model) | 70.8 |\n| BERT-large(single model) | 70.0 |",
  "src_docs": [
   "2204.02311",
   "2006.03654",
   "2005.14165",
   "1810.04805",
   "1910.10683",
   "2109.01652",
   "2109.04223"
  ],
  "updated_answer": [
   [
    "2204.02311",
    "PaLM 540B (finetuned) ",
    "90.1",
    90.1
   ],
   [
    "2006.03654",
    "DeBERTa-1.5B",
    "88.2",
    88.2
   ],
   [
    "1910.10683",
    "T5-11B",
    "88.1",
    88.1
   ],
   [
    "2109.01652",
    "FLAN 137B (zero-shot)",
    "77.5",
    77.5
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "75.4",
    75.4
   ],
   [
    "2109.04223",
    "KELM (finetuning BERT-large based single model)",
    "70.8",
    70.8
   ],
   [
    "1810.04805",
    "BERT-large(single model)",
    "70.0",
    70.0
   ]
  ],
  "meta_info": {
   "datasets": "MultiRC",
   "datasets_short": "SuperGLUE",
   "task": "Question Answering",
   "metric": "F1"
  },
  "updated_answer2": [
   [
    "2204.02311",
    "PaLM 540B (finetuned) ",
    "90.1",
    90.1
   ],
   [
    "2006.03654",
    "DeBERTa-1.5B",
    "88.2",
    88.2
   ],
   [
    "1910.10683",
    "T5-11B",
    "88.1",
    88.1
   ],
   [
    "2109.01652",
    "FLAN 137B (zero-shot)",
    "77.5",
    77.5
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "75.4",
    75.4
   ],
   [
    "2109.04223",
    "KELM (finetuning BERT-large based single model)",
    "70.8",
    70.8
   ],
   [
    "1810.04805",
    "BERT-large(single model)",
    "70.0",
    70.0
   ]
  ]
 },
 "./longdocdata/docs/1265.json": {
  "question": "List the performance scores of various methods on the RACE (RACE) dataset on the Question Answering task using metric RACE-m.",
  "answer": "| Method | RACE-m |\n| --- | --- |\n| XLNet | 85.45 |\n| OCN_large | 76.7 |\n| DCMN_large  | 73.4 |\n| BiAttention MRU | 60.2 |\n| GPT-3 175B (Few-Shot) | 58.1 |",
  "src_docs": [
   "1906.08237",
   "2005.14165",
   "1901.09381",
   "1803.09074",
   "1903.03033"
  ],
  "updated_answer": [
   [
    "1906.08237",
    "XLNet",
    "85.45",
    85.45
   ],
   [
    "1903.03033",
    "OCN_large",
    "76.7",
    76.7
   ],
   [
    "1901.09381",
    "DCMN_large ",
    "73.4",
    73.4
   ],
   [
    "1803.09074",
    "BiAttention MRU",
    "60.2",
    60.2
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "58.1",
    58.1
   ]
  ],
  "meta_info": {
   "datasets": "RACE",
   "datasets_short": "RACE",
   "task": "Question Answering",
   "metric": "RACE-m"
  },
  "updated_answer2": [
   [
    "1906.08237",
    "XLNet",
    "85.45",
    85.45
   ],
   [
    "1903.03033",
    "OCN_large",
    "76.7",
    76.7
   ],
   [
    "1901.09381",
    "DCMN_large ",
    "73.4",
    73.4
   ],
   [
    "1803.09074",
    "BiAttention MRU",
    "60.2",
    60.2
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "58.1",
    58.1
   ]
  ]
 },
 "./longdocdata/docs/1267.json": {
  "question": "List the performance scores of various methods on the RACE (RACE) dataset on the Reading Comprehension task using metric Accuracy (Middle).",
  "answer": "| Method | Accuracy (Middle) |\n| --- | --- |\n| Megatron-BERT (ensemble) | 93.1 |\n| Megatron-BERT | 91.8 |\n| B10-10-10 | 88.8 |\n| ALBERTxxlarge+DUMA(ensemble) | 88.7 |\n| XLNet | 88.6 |\n| RoBERTa | 86.5 |\n| PaLM 540B (zero-shot) | 68.1 |\n| PaLM 62B (zero-shot) | 64.3 |\n| GPT-3 175B (zero-shot) | 58.4 |\n| PaLM 8B (zero-shot) | 57.9 |",
  "src_docs": [
   "1906.08237",
   "1907.11692",
   "2204.02311",
   "1909.08053",
   "2006.03236",
   "2005.14165",
   "2001.09415"
  ],
  "updated_answer": [
   [
    "1909.08053",
    "Megatron-BERT (ensemble)",
    "93.1",
    93.1
   ],
   [
    "2001.09415",
    "ALBERTxxlarge+DUMA(ensemble)",
    "88.7",
    88.7
   ],
   [
    "1909.08053",
    "Megatron-BERT",
    "91.8",
    91.8
   ],
   [
    "2006.03236",
    "B10-10-10",
    "88.8",
    88.8
   ],
   [
    "1907.11692",
    "RoBERTa",
    "86.5",
    86.5
   ],
   [
    "1906.08237",
    "XLNet",
    "88.6",
    88.6
   ],
   [
    "2204.02311",
    "PaLM 540B (zero-shot)",
    "68.1",
    68.1
   ],
   [
    "2204.02311",
    "PaLM 62B (zero-shot)",
    "64.3",
    64.3
   ],
   [
    "2005.14165",
    "GPT-3 175B (zero-shot)",
    "58.4",
    58.4
   ],
   [
    "2204.02311",
    "PaLM 8B (zero-shot)",
    "57.9",
    57.9
   ]
  ],
  "meta_info": {
   "datasets": "RACE",
   "datasets_short": "RACE",
   "task": "Reading Comprehension",
   "metric": "Accuracy (Middle)"
  },
  "updated_answer2": [
   [
    "1909.08053",
    "Megatron-BERT (ensemble)",
    "93.1",
    93.1
   ],
   [
    "2006.03236",
    "B10-10-10",
    "88.8",
    88.8
   ],
   [
    "2001.09415",
    "ALBERTxxlarge+DUMA(ensemble)",
    "88.7",
    88.7
   ],
   [
    "1906.08237",
    "XLNet",
    "88.6",
    88.6
   ],
   [
    "1907.11692",
    "RoBERTa",
    "86.5",
    86.5
   ],
   [
    "2204.02311",
    "PaLM 540B (zero-shot)",
    "68.1",
    68.1
   ],
   [
    "2005.14165",
    "GPT-3 175B (zero-shot)",
    "58.4",
    58.4
   ]
  ]
 },
 "./longdocdata/docs/1268.json": {
  "question": "List the performance scores of various methods on the RACE (RACE) dataset on the Reading Comprehension task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| ALBERT (Ensemble) | 91.4 |\n| Megatron-BERT (ensemble) | 90.9 |\n| ALBERTxxlarge+DUMA(ensemble) | 89.8 |\n| Megatron-BERT | 89.5 |\n| DeBERTalarge | 86.8 |\n| B10-10-10 | 85.7 |\n| RoBERTa | 83.2 |\n| HAT (Encoder) | 67.3 |",
  "src_docs": [
   "1907.11692",
   "2006.03654",
   "1909.08053",
   "2104.07545",
   "2006.03236",
   "2001.09415",
   "2011.03292"
  ],
  "updated_answer": [
   [
    "2011.03292",
    "ALBERT (Ensemble)",
    "91.4",
    91.4
   ],
   [
    "1909.08053",
    "Megatron-BERT (ensemble)",
    "90.9",
    90.9
   ],
   [
    "2001.09415",
    "ALBERTxxlarge+DUMA(ensemble)",
    "89.8",
    89.8
   ],
   [
    "1909.08053",
    "Megatron-BERT",
    "89.5",
    89.5
   ],
   [
    "2006.03654",
    "DeBERTalarge",
    "86.8",
    86.8
   ],
   [
    "2006.03236",
    "B10-10-10",
    "85.7",
    85.7
   ],
   [
    "1907.11692",
    "RoBERTa",
    "83.2",
    83.2
   ],
   [
    "2104.07545",
    "HAT (Encoder)",
    "67.3",
    67.3
   ]
  ],
  "meta_info": {
   "datasets": "RACE",
   "datasets_short": "RACE",
   "task": "Reading Comprehension",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2011.03292",
    "ALBERT (Ensemble)",
    "91.4",
    91.4
   ],
   [
    "1909.08053",
    "Megatron-BERT (ensemble)",
    "90.9",
    90.9
   ],
   [
    "2001.09415",
    "ALBERTxxlarge+DUMA(ensemble)",
    "89.8",
    89.8
   ],
   [
    "2006.03654",
    "DeBERTalarge",
    "86.8",
    86.8
   ],
   [
    "2006.03236",
    "B10-10-10",
    "85.7",
    85.7
   ],
   [
    "1907.11692",
    "RoBERTa",
    "83.2",
    83.2
   ],
   [
    "2104.07545",
    "HAT (Encoder)",
    "67.3",
    67.3
   ]
  ]
 },
 "./longdocdata/docs/1269.json": {
  "question": "List the performance scores of various methods on the RACE (RACE) dataset on the Reading Comprehension task using metric Accuracy (High).",
  "answer": "| Method | Accuracy (High) |\n| --- | --- |\n| ALBERTxxlarge+DUMA(ensemble) | 92.6 |\n| Megatron-BERT (ensemble) | 90.0 |\n| Megatron-BERT | 88.6 |\n| B10-10-10 | 84.4 |\n| XLNet | 84.0 |\n| RoBERTa | 81.3 |\n| PaLM 540B (zero-shot) | 49.1 |\n| PaLM 62B (zero-shot) | 47.5 |\n| GPT-3 175B (zero-shot) | 45.5 |\n| PaLM 8B (zero-shot) | 42.3 |",
  "src_docs": [
   "1906.08237",
   "1907.11692",
   "2204.02311",
   "1909.08053",
   "2006.03236",
   "2005.14165",
   "2001.09415"
  ],
  "updated_answer": [
   [
    "1909.08053",
    "Megatron-BERT (ensemble)",
    "90.0",
    90.0
   ],
   [
    "2001.09415",
    "ALBERTxxlarge+DUMA(ensemble)",
    "92.6",
    92.6
   ],
   [
    "1909.08053",
    "Megatron-BERT",
    "88.6",
    88.6
   ],
   [
    "2006.03236",
    "B10-10-10",
    "84.4",
    84.4
   ],
   [
    "1907.11692",
    "RoBERTa",
    "81.3",
    81.3
   ],
   [
    "1906.08237",
    "XLNet",
    "84.0",
    84.0
   ],
   [
    "2204.02311",
    "PaLM 540B (zero-shot)",
    "49.1",
    49.1
   ],
   [
    "2204.02311",
    "PaLM 62B (zero-shot)",
    "47.5",
    47.5
   ],
   [
    "2005.14165",
    "GPT-3 175B (zero-shot)",
    "45.5",
    45.5
   ],
   [
    "2204.02311",
    "PaLM 8B (zero-shot)",
    "42.3",
    42.3
   ]
  ],
  "meta_info": {
   "datasets": "RACE",
   "datasets_short": "RACE",
   "task": "Reading Comprehension",
   "metric": "Accuracy (High)"
  },
  "updated_answer2": [
   [
    "2001.09415",
    "ALBERTxxlarge+DUMA(ensemble)",
    "92.6",
    92.6
   ],
   [
    "1909.08053",
    "Megatron-BERT (ensemble)",
    "90.0",
    90.0
   ],
   [
    "2006.03236",
    "B10-10-10",
    "84.4",
    84.4
   ],
   [
    "1906.08237",
    "XLNet",
    "84.0",
    84.0
   ],
   [
    "1907.11692",
    "RoBERTa",
    "81.3",
    81.3
   ],
   [
    "2204.02311",
    "PaLM 540B (zero-shot)",
    "49.1",
    49.1
   ],
   [
    "2005.14165",
    "GPT-3 175B (zero-shot)",
    "45.5",
    45.5
   ]
  ]
 },
 "./longdocdata/docs/1273.json": {
  "question": "List the performance scores of various methods on the Wizard-of-Oz (Wizard-of-Oz) dataset on the Dialogue State Tracking task using metric Request.",
  "answer": "| Method | Request |\n| --- | --- |\n| BERT-based tracker | 97.6 |\n| GCE | 97.4 |\n| Zhong et al. | 97.1 |\n| G-SAT | 96.9 |\n| Neural belief tracker | 96.5 |",
  "src_docs": [
   "1812.00899",
   "1910.12995",
   "1805.09655",
   "1606.03777",
   "1910.09942"
  ],
  "updated_answer": [
   [
    "1910.12995",
    "BERT-based tracker",
    "97.6",
    97.6
   ],
   [
    "1910.09942",
    "G-SAT",
    "96.9",
    96.9
   ],
   [
    "1812.00899",
    "GCE",
    "97.4",
    97.4
   ],
   [
    "1805.09655",
    "Zhong et al.",
    "97.1",
    97.1
   ],
   [
    "1606.03777",
    "Neural belief tracker",
    "96.5",
    96.5
   ]
  ],
  "meta_info": {
   "datasets": "Wizard-of-Oz",
   "datasets_short": "Wizard-of-Oz",
   "task": "Dialogue State Tracking",
   "metric": "Request"
  },
  "updated_answer2": [
   [
    "1910.12995",
    "BERT-based tracker",
    "97.6",
    97.6
   ],
   [
    "1812.00899",
    "GCE",
    "97.4",
    97.4
   ],
   [
    "1805.09655",
    "Zhong et al.",
    "97.1",
    97.1
   ],
   [
    "1910.09942",
    "G-SAT",
    "96.9",
    96.9
   ],
   [
    "1606.03777",
    "Neural belief tracker",
    "96.5",
    96.5
   ]
  ]
 },
 "./longdocdata/docs/1317.json": {
  "question": "List the performance scores of various methods on the Annotated Faces in the Wild (AFW) dataset on the Face Detection task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| SRN | 0.9987 |\n| HyperFace-ResNet | 0.994 |\n| LRN + RSA | 0.9917 |\n| FaceBoxes | 0.9891 |\n| STN | 0.9835 |\n| DPM | 0.9721 |\n| Conv3D | 0.9597 |",
  "src_docs": [
   "1603.01249",
   "1606.00850",
   "1408.1656",
   "1708.05234",
   "1607.05477",
   "1809.02693",
   "1707.09531"
  ],
  "updated_answer": [
   [
    "1809.02693",
    "SRN",
    "0.9987",
    0.9987
   ],
   [
    "1603.01249",
    "HyperFace-ResNet",
    "0.9940",
    0.994
   ],
   [
    "1707.09531",
    "LRN + RSA",
    "0.9917",
    0.9917
   ],
   [
    "1708.05234",
    "FaceBoxes",
    "0.9891",
    0.9891
   ],
   [
    "1607.05477",
    "STN",
    "0.9835",
    0.9835
   ],
   [
    "1408.1656",
    "DPM",
    "0.9721",
    0.9721
   ],
   [
    "1606.00850",
    "Conv3D",
    "0.9597",
    0.9597
   ]
  ],
  "meta_info": {
   "datasets": "Annotated Faces in the Wild",
   "datasets_short": "AFW",
   "task": "Face Detection",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "1809.02693",
    "SRN",
    "0.9987",
    0.9987
   ],
   [
    "1603.01249",
    "HyperFace-ResNet",
    "0.9940",
    0.994
   ],
   [
    "1707.09531",
    "LRN + RSA",
    "0.9917",
    0.9917
   ],
   [
    "1708.05234",
    "FaceBoxes",
    "0.9891",
    0.9891
   ],
   [
    "1607.05477",
    "STN",
    "0.9835",
    0.9835
   ],
   [
    "1408.1656",
    "DPM",
    "0.9721",
    0.9721
   ],
   [
    "1606.00850",
    "Conv3D",
    "0.9597",
    0.9597
   ]
  ]
 },
 "./longdocdata/docs/1351.json": {
  "question": "List the performance scores of various methods on the SentEval (SentEval) dataset on the Semantic Textual Similarity task using metric MRPC.",
  "answer": "| Method | MRPC |\n| --- | --- |\n| XLNet-Large | 93.0 |\n| MT-DNN-ensemble | 92.7 |\n| Snorkel MeTaL(ensemble) | 91.5 |\n| GenSen | 78.6 |\n| InferSent | 76.2 |",
  "src_docs": [
   "1906.08237",
   "1810.02840",
   "1705.02364",
   "1904.09482",
   "1804.00079"
  ],
  "updated_answer": [
   [
    "1804.00079",
    "GenSen",
    "78.6/84.4",
    78.6
   ],
   [
    "1705.02364",
    "InferSent",
    "76.2/83.1",
    76.2
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "91.5/88.5",
    91.5
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "92.7/90.3",
    92.7
   ],
   [
    "1906.08237",
    "XLNet-Large",
    "93.0/90.7",
    93.0
   ]
  ],
  "meta_info": {
   "datasets": "SentEval",
   "datasets_short": "SentEval",
   "task": "Semantic Textual Similarity",
   "metric": "MRPC"
  },
  "updated_answer2": [
   [
    "1906.08237",
    "XLNet-Large",
    "93.0/90.7",
    93.0
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "92.7/90.3",
    92.7
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "91.5/88.5",
    91.5
   ],
   [
    "1804.00079",
    "GenSen",
    "78.6/84.4",
    78.6
   ],
   [
    "1705.02364",
    "InferSent",
    "76.2/83.1",
    76.2
   ]
  ]
 },
 "./longdocdata/docs/1352.json": {
  "question": "List the performance scores of various methods on the SentEval (SentEval) dataset on the Semantic Textual Similarity task using metric SICK-R.",
  "answer": "| Method | SICK-R |\n| --- | --- |\n| GenSen | 0.888 |\n| InferSent | 0.884 |\n| Snorkel MeTaL(ensemble) | 0.0 |\n| MT-DNN-ensemble | 0.0 |\n| XLNet-Large | 0.0 |",
  "src_docs": [
   "1906.08237",
   "1810.02840",
   "1705.02364",
   "1904.09482",
   "1804.00079"
  ],
  "updated_answer": [
   [
    "1804.00079",
    "GenSen",
    "0.888",
    0.888
   ],
   [
    "1705.02364",
    "InferSent",
    "0.884",
    0.884
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "-",
    0
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "-",
    0
   ],
   [
    "1906.08237",
    "XLNet-Large",
    "-",
    0
   ]
  ],
  "meta_info": {
   "datasets": "SentEval",
   "datasets_short": "SentEval",
   "task": "Semantic Textual Similarity",
   "metric": "SICK-R"
  },
  "updated_answer2": [
   [
    "1804.00079",
    "GenSen",
    "0.888",
    0.888
   ],
   [
    "1705.02364",
    "InferSent",
    "0.884",
    0.884
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "-",
    0
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "-",
    0
   ],
   [
    "1906.08237",
    "XLNet-Large",
    "-",
    0
   ]
  ]
 },
 "./longdocdata/docs/1353.json": {
  "question": "List the performance scores of various methods on the SentEval (SentEval) dataset on the Semantic Textual Similarity task using metric SICK-E.",
  "answer": "| Method | SICK-E |\n| --- | --- |\n| GenSen | 87.8 |\n| InferSent | 86.3 |\n| Snorkel MeTaL(ensemble) | 0.0 |\n| MT-DNN-ensemble | 0.0 |\n| XLNet-Large | 0.0 |",
  "src_docs": [
   "1906.08237",
   "1810.02840",
   "1705.02364",
   "1904.09482",
   "1804.00079"
  ],
  "updated_answer": [
   [
    "1804.00079",
    "GenSen",
    "87.8",
    87.8
   ],
   [
    "1705.02364",
    "InferSent",
    "86.3",
    86.3
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "-",
    0
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "-",
    0
   ],
   [
    "1906.08237",
    "XLNet-Large",
    "-",
    0
   ]
  ],
  "meta_info": {
   "datasets": "SentEval",
   "datasets_short": "SentEval",
   "task": "Semantic Textual Similarity",
   "metric": "SICK-E"
  },
  "updated_answer2": [
   [
    "1804.00079",
    "GenSen",
    "87.8",
    87.8
   ],
   [
    "1705.02364",
    "InferSent",
    "86.3",
    86.3
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "-",
    0
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "-",
    0
   ],
   [
    "1906.08237",
    "XLNet-Large",
    "-",
    0
   ]
  ]
 },
 "./longdocdata/docs/1354.json": {
  "question": "List the performance scores of various methods on the SentEval (SentEval) dataset on the Semantic Textual Similarity task using metric STS.",
  "answer": "| Method | STS |\n| --- | --- |\n| XLNet-Large | 91.6 |\n| MT-DNN-ensemble | 91.1 |\n| Snorkel MeTaL(ensemble) | 90.1 |\n| GenSen | 78.9 |\n| InferSent | 75.8 |",
  "src_docs": [
   "1906.08237",
   "1810.02840",
   "1705.02364",
   "1904.09482",
   "1804.00079"
  ],
  "updated_answer": [
   [
    "1804.00079",
    "GenSen",
    "78.9/78.6",
    78.9
   ],
   [
    "1705.02364",
    "InferSent",
    "75.8/75.5",
    75.8
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "90.1/89.7*",
    90.1
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "91.1/90.7*",
    91.1
   ],
   [
    "1906.08237",
    "XLNet-Large",
    "91.6/91.1*",
    91.6
   ]
  ],
  "meta_info": {
   "datasets": "SentEval",
   "datasets_short": "SentEval",
   "task": "Semantic Textual Similarity",
   "metric": "STS"
  },
  "updated_answer2": [
   [
    "1906.08237",
    "XLNet-Large",
    "91.6/91.1*",
    91.6
   ],
   [
    "1904.09482",
    "MT-DNN-ensemble",
    "91.1/90.7*",
    91.1
   ],
   [
    "1810.02840",
    "Snorkel MeTaL(ensemble)",
    "90.1/89.7*",
    90.1
   ],
   [
    "1804.00079",
    "GenSen",
    "78.9/78.6",
    78.9
   ],
   [
    "1705.02364",
    "InferSent",
    "75.8/75.5",
    75.8
   ]
  ]
 },
 "./longdocdata/docs/1355.json": {
  "question": "List the performance scores of various methods on the SentEval (SentEval) dataset on the Linear-Probe Classification task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| cpt-text XL-supervised | 92.2 |\n| cpt-text XL-unsupervised | 91.8 |\n| SimCSE-supervised | 90.23 |\n| DECLUTR | 88.3 |\n| Sentence-BERT: | 87.7 |\n| SimCSE-unsupervised | 87.6 |\n| BERT | 84.9 |",
  "src_docs": [
   "1908.10084",
   "2104.08821",
   "2006.03659",
   "1810.04805",
   "2201.10005"
  ],
  "updated_answer": [
   [
    "2201.10005",
    "cpt-text XL-supervised",
    "92.2",
    92.2
   ],
   [
    "2201.10005",
    "cpt-text XL-unsupervised",
    "91.8",
    91.8
   ],
   [
    "2104.08821",
    "SimCSE-supervised",
    "90.23",
    90.23
   ],
   [
    "2006.03659",
    "DECLUTR",
    "88.3",
    88.3
   ],
   [
    "1908.10084",
    "Sentence-BERT:",
    "87.7",
    87.7
   ],
   [
    "2104.08821",
    "SimCSE-unsupervised",
    "87.6",
    87.6
   ],
   [
    "1810.04805",
    "BERT",
    "84.9",
    84.9
   ]
  ],
  "meta_info": {
   "datasets": "SentEval",
   "datasets_short": "SentEval",
   "task": "Linear-Probe Classification",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2201.10005",
    "cpt-text XL-supervised",
    "92.2",
    92.2
   ],
   [
    "2104.08821",
    "SimCSE-supervised",
    "90.23",
    90.23
   ],
   [
    "2006.03659",
    "DECLUTR",
    "88.3",
    88.3
   ],
   [
    "1908.10084",
    "Sentence-BERT:",
    "87.7",
    87.7
   ],
   [
    "1810.04805",
    "BERT",
    "84.9",
    84.9
   ]
  ]
 },
 "./longdocdata/docs/1362.json": {
  "question": "List the performance scores of various methods on the ESC-50 (ESC-50) dataset on the Self-Supervised Audio Classification task using metric Top-1 Accuracy.",
  "answer": "| Method | Top-1 Accuracy |\n| --- | --- |\n| BraVe:V-FA (TSM-50x2) | 91.1 |\n| CrissCross (AudioSet) | 90.5 |\n| AVID | 89.2 |\n| CrissCross (Kinetics400) | 86.8 |\n| MMV | 85.6 |\n| XDC | 85.4 |\n| AVTS | 80.6 |",
  "src_docs": [
   "2111.05329",
   "2004.12943",
   "1807.00230",
   "1911.12667",
   "2103.16559",
   "2006.16228"
  ],
  "updated_answer": [
   [
    "2103.16559",
    "BraVe:V-FA (TSM-50x2)",
    "91.1",
    91.1
   ],
   [
    "2111.05329",
    "CrissCross (AudioSet)",
    "90.5",
    90.5
   ],
   [
    "2004.12943",
    "AVID",
    "89.2",
    89.2
   ],
   [
    "2111.05329",
    "CrissCross (Kinetics400)",
    "86.8",
    86.8
   ],
   [
    "2006.16228",
    "MMV",
    "85.6",
    85.6
   ],
   [
    "1911.12667",
    "XDC",
    "85.4",
    85.4
   ],
   [
    "1807.00230",
    "AVTS",
    "80.6",
    80.6
   ]
  ],
  "meta_info": {
   "datasets": "ESC-50",
   "datasets_short": "ESC-50",
   "task": "Self-Supervised Audio Classification",
   "metric": "Top-1 Accuracy"
  },
  "updated_answer2": [
   [
    "2103.16559",
    "BraVe:V-FA (TSM-50x2)",
    "91.1",
    91.1
   ],
   [
    "2111.05329",
    "CrissCross (AudioSet)",
    "90.5",
    90.5
   ],
   [
    "2004.12943",
    "AVID",
    "89.2",
    89.2
   ],
   [
    "2006.16228",
    "MMV",
    "85.6",
    85.6
   ],
   [
    "1911.12667",
    "XDC",
    "85.4",
    85.4
   ],
   [
    "1807.00230",
    "AVTS",
    "80.6",
    80.6
   ]
  ]
 },
 "./longdocdata/docs/1365.json": {
  "question": "List the performance scores of various methods on the ESC-50 (ESC-50) dataset on the Audio Classification task using metric PRE-TRAINING DATASET.",
  "answer": "| Method | PRE-TRAINING DATASET |\n| --- | --- |\n| HTS-AT | 0.0 |\n| EAT-M | 0.0 |\n| Audio Spectrogram Transformer | 0.0 |\n| EAT-S | 0.0 |\n| SepTr | 0.0 |\n| XDC | 0.0 |\n| XDC | 0.0 |",
  "src_docs": [
   "2104.01778",
   "2204.11479",
   "2203.09581",
   "2202.00874",
   "1911.12667"
  ],
  "updated_answer": [
   [
    "2202.00874",
    "HTS-AT",
    "AudioSet",
    0
   ],
   [
    "2204.11479",
    "EAT-M",
    "AudioSet",
    0
   ],
   [
    "2104.01778",
    "Audio Spectrogram Transformer",
    "AudioSet, ImageNet",
    0
   ],
   [
    "2204.11479",
    "EAT-S",
    "AudioSet",
    0
   ],
   [
    "2203.09581",
    "SepTr",
    "-",
    0
   ],
   [
    "1911.12667",
    "XDC",
    "IG-Random",
    0
   ],
   [
    "1911.12667",
    "XDC",
    "AudioSet",
    0
   ]
  ],
  "meta_info": {
   "datasets": "ESC-50",
   "datasets_short": "ESC-50",
   "task": "Audio Classification",
   "metric": "PRE-TRAINING DATASET"
  },
  "updated_answer2": [
   [
    "2202.00874",
    "HTS-AT",
    "AudioSet",
    0
   ],
   [
    "2204.11479",
    "EAT-M",
    "AudioSet",
    0
   ],
   [
    "2204.11479",
    "EAT-S",
    "AudioSet",
    0
   ],
   [
    "2104.01778",
    "Audio Spectrogram Transformer",
    "AudioSet, ImageNet",
    0
   ],
   [
    "2203.09581",
    "SepTr",
    "-",
    0
   ],
   [
    "1911.12667",
    "XDC",
    "IG-Random",
    0
   ]
  ]
 },
 "./longdocdata/docs/1386.json": {
  "question": "List the performance scores of various methods on the ATIS (ATIS) dataset on the Intent Detection task using metric F1.",
  "answer": "| Method | F1 |\n| --- | --- |\n| DeepStruct multi-task w/ finetune | 97.8 |\n| DeepStruct multi-task | 97.3 |\n| Stack-Propagation (+BERT) | 96.1 |\n| Attention Encoder-Decoder NN | 95.87 |\n| Context Encoder | 95.8 |\n| SF-ID (BLSTM) network | 95.8 |\n| Joint model with recurrent slot label context | 94.64 |\n| DELTA (BLSTM-CRF) | 0.952 |",
  "src_docs": [
   "1909.02188",
   "2205.10475",
   "1907.00390",
   "1911.01680",
   "1609.01462",
   "1908.01853",
   "1609.01454"
  ],
  "updated_answer": [
   [
    "1609.01454",
    "Attention Encoder-Decoder NN",
    "95.87",
    95.87
   ],
   [
    "1609.01462",
    "Joint model with recurrent slot label context",
    "94.64",
    94.64
   ],
   [
    "1907.00390",
    "SF-ID (BLSTM) network",
    "95.80",
    95.8
   ],
   [
    "1909.02188",
    "Stack-Propagation (+BERT)",
    "96.10",
    96.1
   ],
   [
    "1908.01853",
    "DELTA (BLSTM-CRF)",
    "0.952",
    0.952
   ],
   [
    "2205.10475",
    "DeepStruct multi-task w/ finetune",
    "97.8",
    97.8
   ],
   [
    "2205.10475",
    "DeepStruct multi-task",
    "97.3",
    97.3
   ],
   [
    "1911.01680",
    "Context Encoder",
    "95.80",
    95.8
   ]
  ],
  "meta_info": {
   "datasets": "ATIS",
   "datasets_short": "ATIS",
   "task": "Intent Detection",
   "metric": "F1"
  },
  "updated_answer2": [
   [
    "2205.10475",
    "DeepStruct multi-task w/ finetune",
    "97.8",
    97.8
   ],
   [
    "1909.02188",
    "Stack-Propagation (+BERT)",
    "96.10",
    96.1
   ],
   [
    "1609.01454",
    "Attention Encoder-Decoder NN",
    "95.87",
    95.87
   ],
   [
    "1907.00390",
    "SF-ID (BLSTM) network",
    "95.80",
    95.8
   ],
   [
    "1911.01680",
    "Context Encoder",
    "95.80",
    95.8
   ],
   [
    "1609.01462",
    "Joint model with recurrent slot label context",
    "94.64",
    94.64
   ],
   [
    "1908.01853",
    "DELTA (BLSTM-CRF)",
    "0.952",
    0.952
   ]
  ]
 },
 "./longdocdata/docs/1391.json": {
  "question": "List the performance scores of various methods on the ActivityNet (ActivityNet) dataset on the Video Retrieval task using metric text-to-video Median Rank.",
  "answer": "| Method | text-to-video Median Rank |\n| --- | --- |\n| HunYuan_tvr | 1.0 |\n| CAMoE | 1.0 |\n| CenterCLIP (ViT-B/16) | 2.0 |\n| CLIP4Clip | 2.0 |\n| TACo | 3.0 |\n| MMT-Pretrained | 3.3 |\n| MMT | 5.0 |\n| Collaborative Experts | 6.0 |",
  "src_docs": [
   "2007.10639",
   "1907.13487",
   "2204.03382",
   "2108.09980",
   "2205.00823",
   "2109.04290",
   "2104.08860"
  ],
  "updated_answer": [
   [
    "2204.03382",
    "HunYuan_tvr",
    "1",
    1.0
   ],
   [
    "2109.04290",
    "CAMoE",
    "1",
    1.0
   ],
   [
    "2205.00823",
    "CenterCLIP (ViT-B/16)",
    "2",
    2.0
   ],
   [
    "2104.08860",
    "CLIP4Clip",
    "2",
    2.0
   ],
   [
    "2108.09980",
    "TACo",
    "3.0",
    3.0
   ],
   [
    "2007.10639",
    "MMT-Pretrained",
    "3.3",
    3.3
   ],
   [
    "2007.10639",
    "MMT",
    "5",
    5.0
   ],
   [
    "1907.13487",
    "Collaborative Experts",
    "6",
    6.0
   ]
  ],
  "meta_info": {
   "datasets": "ActivityNet",
   "datasets_short": "ActivityNet",
   "task": "Video Retrieval",
   "metric": "text-to-video Median Rank"
  },
  "updated_answer2": [
   [
    "2204.03382",
    "HunYuan_tvr",
    "1",
    1.0
   ],
   [
    "2109.04290",
    "CAMoE",
    "1",
    1.0
   ],
   [
    "2205.00823",
    "CenterCLIP (ViT-B/16)",
    "2",
    2.0
   ],
   [
    "2104.08860",
    "CLIP4Clip",
    "2",
    2.0
   ],
   [
    "2108.09980",
    "TACo",
    "3.0",
    3.0
   ],
   [
    "2007.10639",
    "MMT-Pretrained",
    "3.3",
    3.3
   ],
   [
    "1907.13487",
    "Collaborative Experts",
    "6",
    6.0
   ]
  ]
 },
 "./longdocdata/docs/1395.json": {
  "question": "List the performance scores of various methods on the ActivityNet (ActivityNet) dataset on the Video Retrieval task using metric text-to-video Mean Rank.",
  "answer": "| Method | text-to-video Mean Rank |\n| --- | --- |\n| HunYuan_tvr | 4.0 |\n| CenterCLIP (ViT-B/16) | 5.7 |\n| CAMoE | 6.3 |\n| CLIP4Clip | 7.5 |\n| MMT-Pretrained | 16.0 |\n| MMT | 20.8 |\n| Collaborative Experts | 23.1 |",
  "src_docs": [
   "2007.10639",
   "1907.13487",
   "2204.03382",
   "2205.00823",
   "2109.04290",
   "2104.08860"
  ],
  "updated_answer": [
   [
    "2204.03382",
    "HunYuan_tvr",
    "4.0",
    4.0
   ],
   [
    "2109.04290",
    "CAMoE",
    "6.3",
    6.3
   ],
   [
    "2205.00823",
    "CenterCLIP (ViT-B/16)",
    "5.7",
    5.7
   ],
   [
    "2104.08860",
    "CLIP4Clip",
    "7.5",
    7.5
   ],
   [
    "2007.10639",
    "MMT-Pretrained",
    "16",
    16.0
   ],
   [
    "2007.10639",
    "MMT",
    "20.8",
    20.8
   ],
   [
    "1907.13487",
    "Collaborative Experts",
    "23.1",
    23.1
   ]
  ],
  "meta_info": {
   "datasets": "ActivityNet",
   "datasets_short": "ActivityNet",
   "task": "Video Retrieval",
   "metric": "text-to-video Mean Rank"
  },
  "updated_answer2": [
   [
    "2204.03382",
    "HunYuan_tvr",
    "4.0",
    4.0
   ],
   [
    "2205.00823",
    "CenterCLIP (ViT-B/16)",
    "5.7",
    5.7
   ],
   [
    "2109.04290",
    "CAMoE",
    "6.3",
    6.3
   ],
   [
    "2104.08860",
    "CLIP4Clip",
    "7.5",
    7.5
   ],
   [
    "2007.10639",
    "MMT-Pretrained",
    "16",
    16.0
   ],
   [
    "1907.13487",
    "Collaborative Experts",
    "23.1",
    23.1
   ]
  ]
 },
 "./longdocdata/docs/1438.json": {
  "question": "List the performance scores of various methods on the ShanghaiTech A (ShanghaiTech) dataset on the Crowd Counting task using metric MSE.",
  "answer": "| Method | MSE |\n| --- | --- |\n| P2PNet | 85.06 |\n| SPANet | 92.5 |\n| M-SFANet+M-SegNet | 94.48 |\n| FusionCount | 101.2 |\n| Cascaded-MTL | 152.4 |",
  "src_docs": [
   "1909.07057",
   "2003.05586",
   "2202.13660",
   "1707.09605",
   "2107.12746"
  ],
  "updated_answer": [
   [
    "2107.12746",
    "P2PNet",
    "85.06",
    85.06
   ],
   [
    "2003.05586",
    "M-SFANet+M-SegNet",
    "94.48",
    94.48
   ],
   [
    "1909.07057",
    "SPANet",
    "92.5",
    92.5
   ],
   [
    "2202.13660",
    "FusionCount",
    "101.2",
    101.2
   ],
   [
    "1707.09605",
    "Cascaded-MTL",
    "152.4",
    152.4
   ]
  ],
  "meta_info": {
   "datasets": "ShanghaiTech A",
   "datasets_short": "ShanghaiTech",
   "task": "Crowd Counting",
   "metric": "MSE"
  },
  "updated_answer2": [
   [
    "2107.12746",
    "P2PNet",
    "85.06",
    85.06
   ],
   [
    "1909.07057",
    "SPANet",
    "92.5",
    92.5
   ],
   [
    "2003.05586",
    "M-SFANet+M-SegNet",
    "94.48",
    94.48
   ],
   [
    "2202.13660",
    "FusionCount",
    "101.2",
    101.2
   ],
   [
    "1707.09605",
    "Cascaded-MTL",
    "152.4",
    152.4
   ]
  ]
 },
 "./longdocdata/docs/1451.json": {
  "question": "List the performance scores of various methods on the AVA (Aesthetic Visual Analysis) dataset on the Aesthetics Quality Assessment task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| A-Lamp | 82.5 |\n| Pool-3FC | 81.7 |\n| NIMA | 81.5 |\n| MTRLCNN | 79.1 |\n| ADB-CNN | 77.3 |",
  "src_docs": [
   "1704.00248",
   "1904.01382",
   "1604.04970",
   "1709.05424",
   "1606.01621"
  ],
  "updated_answer": [
   [
    "1704.00248",
    "A-Lamp",
    "82.5%",
    82.5
   ],
   [
    "1904.01382",
    "Pool-3FC",
    "81.7%",
    81.7
   ],
   [
    "1709.05424",
    "NIMA",
    "81.5%",
    81.5
   ],
   [
    "1604.04970",
    "MTRLCNN",
    "79.1%",
    79.1
   ],
   [
    "1606.01621",
    "ADB-CNN",
    "77.3%",
    77.3
   ]
  ],
  "meta_info": {
   "datasets": "AVA",
   "datasets_short": "Aesthetic Visual Analysis",
   "task": "Aesthetics Quality Assessment",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "1704.00248",
    "A-Lamp",
    "82.5%",
    82.5
   ],
   [
    "1904.01382",
    "Pool-3FC",
    "81.7%",
    81.7
   ],
   [
    "1709.05424",
    "NIMA",
    "81.5%",
    81.5
   ],
   [
    "1604.04970",
    "MTRLCNN",
    "79.1%",
    79.1
   ],
   [
    "1606.01621",
    "ADB-CNN",
    "77.3%",
    77.3
   ]
  ]
 },
 "./longdocdata/docs/1453.json": {
  "question": "List the performance scores of various methods on the EPIC-KITCHENS-55 (EPIC-KITCHENS-55) dataset on the Egocentric Activity Recognition task using metric Actions Top-1 (S2).",
  "answer": "| Method | Actions Top-1 (S2) |\n| --- | --- |\n| DEEP-HAL with ODF+SDF (AssembleNet++) | 27.3 |\n| R(2+1)D-152-SE (ig) | 25.6 |\n| LFB Max | 21.2 |\n| RULSTM | 19.49 |\n| TBN | 19.06 |\n| R(2+1)D-34 (kinetics) | 16.8 |\n| LSTA | 16.63 |",
  "src_docs": [
   "2001.04627",
   "1908.08498",
   "1905.09035",
   "1811.10698",
   "1812.05038",
   "1905.00561"
  ],
  "updated_answer": [
   [
    "2001.04627",
    "DEEP-HAL with ODF+SDF (AssembleNet++)",
    "27.3",
    27.3
   ],
   [
    "1908.08498",
    "TBN",
    "19.06",
    19.06
   ],
   [
    "1905.09035",
    "RULSTM",
    "19.49",
    19.49
   ],
   [
    "1812.05038",
    "LFB Max",
    "21.2",
    21.2
   ],
   [
    "1905.00561",
    "R(2+1)D-152-SE (ig)",
    "25.6",
    25.6
   ],
   [
    "1905.00561",
    "R(2+1)D-34 (kinetics)",
    "16.8",
    16.8
   ],
   [
    "1811.10698",
    "LSTA",
    "16.63",
    16.63
   ]
  ],
  "meta_info": {
   "datasets": "EPIC-KITCHENS-55",
   "datasets_short": "EPIC-KITCHENS-55",
   "task": "Egocentric Activity Recognition",
   "metric": "Actions Top-1 (S2)"
  },
  "updated_answer2": [
   [
    "2001.04627",
    "DEEP-HAL with ODF+SDF (AssembleNet++)",
    "27.3",
    27.3
   ],
   [
    "1905.00561",
    "R(2+1)D-152-SE (ig)",
    "25.6",
    25.6
   ],
   [
    "1812.05038",
    "LFB Max",
    "21.2",
    21.2
   ],
   [
    "1905.09035",
    "RULSTM",
    "19.49",
    19.49
   ],
   [
    "1908.08498",
    "TBN",
    "19.06",
    19.06
   ],
   [
    "1811.10698",
    "LSTA",
    "16.63",
    16.63
   ]
  ]
 },
 "./longdocdata/docs/1456.json": {
  "question": "List the performance scores of various methods on the Charades (Charades) dataset on the Weakly Supervised Object Detection task using metric MAP.",
  "answer": "| Method | MAP |\n| --- | --- |\n| Spatial Prior | 10.03 |\n| PCL | 2.83 |\n| TD-LSTM | 1.98 |\n| ContextLocNet | 1.12 |\n| R*CNN | 0.99 |\n| WSDDN | 0.65 |",
  "src_docs": [
   "1609.04331",
   "1904.01665",
   "1505.01197",
   "1807.03342",
   "1511.02853",
   "1708.00666"
  ],
  "updated_answer": [
   [
    "1904.01665",
    "Spatial Prior",
    "10.03",
    10.03
   ],
   [
    "1807.03342",
    "PCL",
    "2.83",
    2.83
   ],
   [
    "1708.00666",
    "TD-LSTM",
    "1.98",
    1.98
   ],
   [
    "1609.04331",
    "ContextLocNet",
    "1.12",
    1.12
   ],
   [
    "1505.01197",
    "R*CNN",
    "0.99",
    0.99
   ],
   [
    "1511.02853",
    "WSDDN",
    "0.65",
    0.65
   ]
  ],
  "meta_info": {
   "datasets": "Charades",
   "datasets_short": "Charades",
   "task": "Weakly Supervised Object Detection",
   "metric": "MAP"
  },
  "updated_answer2": [
   [
    "1904.01665",
    "Spatial Prior",
    "10.03",
    10.03
   ],
   [
    "1807.03342",
    "PCL",
    "2.83",
    2.83
   ],
   [
    "1708.00666",
    "TD-LSTM",
    "1.98",
    1.98
   ],
   [
    "1609.04331",
    "ContextLocNet",
    "1.12",
    1.12
   ],
   [
    "1505.01197",
    "R*CNN",
    "0.99",
    0.99
   ],
   [
    "1511.02853",
    "WSDDN",
    "0.65",
    0.65
   ]
  ]
 },
 "./longdocdata/docs/1514.json": {
  "question": "List the performance scores of various methods on the GTAV-to-Cityscapes Labels (GTA5) dataset on the Semantic Segmentation task using metric mIoU.",
  "answer": "| Method | mIoU |\n| --- | --- |\n| HRDA | 73.8 |\n| SePiCo | 70.3 |\n| DAFormer + ProCST | 69.4 |\n| DAFormer | 68.3 |\n| TransDA-B | 63.9 |\n| ProDA+CRA | 58.6 |\n| ProDA | 57.5 |",
  "src_docs": [
   "2204.13132",
   "2204.08808",
   "2101.10979",
   "2111.14887",
   "2203.07988",
   "2204.11891",
   "2109.06422"
  ],
  "updated_answer": [
   [
    "2204.13132",
    "HRDA",
    "73.8",
    73.8
   ],
   [
    "2204.08808",
    "SePiCo",
    "70.3",
    70.3
   ],
   [
    "2204.11891",
    "DAFormer + ProCST",
    "69.4",
    69.4
   ],
   [
    "2111.14887",
    "DAFormer",
    "68.3",
    68.3
   ],
   [
    "2203.07988",
    "TransDA-B",
    "63.9",
    63.9
   ],
   [
    "2109.06422",
    "ProDA+CRA",
    "58.6",
    58.6
   ],
   [
    "2101.10979",
    "ProDA",
    "57.5",
    57.5
   ]
  ],
  "meta_info": {
   "datasets": "GTAV-to-Cityscapes Labels",
   "datasets_short": "GTA5",
   "task": "Semantic Segmentation",
   "metric": "mIoU"
  },
  "updated_answer2": [
   [
    "2204.13132",
    "HRDA",
    "73.8",
    73.8
   ],
   [
    "2204.08808",
    "SePiCo",
    "70.3",
    70.3
   ],
   [
    "2204.11891",
    "DAFormer + ProCST",
    "69.4",
    69.4
   ],
   [
    "2111.14887",
    "DAFormer",
    "68.3",
    68.3
   ],
   [
    "2203.07988",
    "TransDA-B",
    "63.9",
    63.9
   ],
   [
    "2109.06422",
    "ProDA+CRA",
    "58.6",
    58.6
   ],
   [
    "2101.10979",
    "ProDA",
    "57.5",
    57.5
   ]
  ]
 },
 "./longdocdata/docs/1521.json": {
  "question": "List the performance scores of various methods on the MovieLens 20M (MovieLens) dataset on the Recommendation Systems task using metric nDCG@100.",
  "answer": "| Method | nDCG@100 |\n| --- | --- |\n| VASP | 0.448 |\n| H+Vamp Gated | 0.44522 |\n| RecVAE | 0.442 |\n| RaCT | 0.434 |\n| Mult-VAE PR | 0.426 |\n| EASE | 0.42 |\n| Mult-DAE | 0.419 |",
  "src_docs": [
   "2102.05774",
   "1912.11160",
   "1802.05814",
   "1906.04281",
   "1911.00936",
   "1905.03375"
  ],
  "updated_answer": [
   [
    "2102.05774",
    "VASP",
    "0.448",
    0.448
   ],
   [
    "1911.00936",
    "H+Vamp Gated",
    "0.44522",
    0.44522
   ],
   [
    "1912.11160",
    "RecVAE",
    "0.442",
    0.442
   ],
   [
    "1906.04281",
    "RaCT",
    "0.434",
    0.434
   ],
   [
    "1802.05814",
    "Mult-VAE PR",
    "0.426",
    0.426
   ],
   [
    "1905.03375",
    "EASE",
    "0.420",
    0.42
   ],
   [
    "1802.05814",
    "Mult-DAE",
    "0.419",
    0.419
   ]
  ],
  "meta_info": {
   "datasets": "MovieLens 20M",
   "datasets_short": "MovieLens",
   "task": "Recommendation Systems",
   "metric": "nDCG@100"
  },
  "updated_answer2": [
   [
    "2102.05774",
    "VASP",
    "0.448",
    0.448
   ],
   [
    "1911.00936",
    "H+Vamp Gated",
    "0.44522",
    0.44522
   ],
   [
    "1912.11160",
    "RecVAE",
    "0.442",
    0.442
   ],
   [
    "1906.04281",
    "RaCT",
    "0.434",
    0.434
   ],
   [
    "1802.05814",
    "Mult-VAE PR",
    "0.426",
    0.426
   ],
   [
    "1905.03375",
    "EASE",
    "0.420",
    0.42
   ]
  ]
 },
 "./longdocdata/docs/1546.json": {
  "question": "List the performance scores of various methods on the MovieLens 1M (MovieLens) dataset on the Recommendation Systems task using metric HR@10.",
  "answer": "| Method | HR@10 |\n| --- | --- |\n| KTUP (soft) | 0.8903 |\n| SASRec | 0.8245 |\n| HyperML | 0.7563 |\n| LRML | 0.7397 |\n| Ekar* | 0.1994 |",
  "src_docs": [
   "1808.09781",
   "1902.06236",
   "1707.05176",
   "1906.09506",
   "1809.01703"
  ],
  "updated_answer": [
   [
    "1808.09781",
    "SASRec",
    "0.8245",
    0.8245
   ],
   [
    "1809.01703",
    "HyperML",
    "0.7563",
    0.7563
   ],
   [
    "1707.05176",
    "LRML",
    "0.7397",
    0.7397
   ],
   [
    "1906.09506",
    "Ekar*",
    "0.1994",
    0.1994
   ],
   [
    "1902.06236",
    "KTUP (soft)",
    "0.8903",
    0.8903
   ]
  ],
  "meta_info": {
   "datasets": "MovieLens 1M",
   "datasets_short": "MovieLens",
   "task": "Recommendation Systems",
   "metric": "HR@10"
  },
  "updated_answer2": [
   [
    "1902.06236",
    "KTUP (soft)",
    "0.8903",
    0.8903
   ],
   [
    "1808.09781",
    "SASRec",
    "0.8245",
    0.8245
   ],
   [
    "1809.01703",
    "HyperML",
    "0.7563",
    0.7563
   ],
   [
    "1707.05176",
    "LRML",
    "0.7397",
    0.7397
   ],
   [
    "1906.09506",
    "Ekar*",
    "0.1994",
    0.1994
   ]
  ]
 },
 "./longdocdata/docs/1555.json": {
  "question": "List the performance scores of various methods on the MovieLens 25M (MovieLens) dataset on the Link Prediction task using metric Hits@10.",
  "answer": "| Method | Hits@10 |\n| --- | --- |\n| PEAGAT | 0.8284 |\n| CFKG | 0.8152 |\n| KGAT | 0.8147 |\n| NFM | 0.8132 |\n| NGCF | 0.7807 |\n| KGCN | 0.771 |",
  "src_docs": [
   "1708.05027",
   "1904.12575",
   "1905.08108",
   "1905.07854",
   "2010.11793",
   "1805.03352"
  ],
  "updated_answer": [
   [
    "2010.11793",
    "PEAGAT",
    "0.8284",
    0.8284
   ],
   [
    "1708.05027",
    "NFM",
    "0.8132",
    0.8132
   ],
   [
    "1905.07854",
    "KGAT",
    "0.8147",
    0.8147
   ],
   [
    "1805.03352",
    "CFKG",
    "0.8152",
    0.8152
   ],
   [
    "1905.08108",
    "NGCF",
    "0.7807",
    0.7807
   ],
   [
    "1904.12575",
    "KGCN",
    "0.771",
    0.771
   ]
  ],
  "meta_info": {
   "datasets": "MovieLens 25M",
   "datasets_short": "MovieLens",
   "task": "Link Prediction",
   "metric": "Hits@10"
  },
  "updated_answer2": [
   [
    "2010.11793",
    "PEAGAT",
    "0.8284",
    0.8284
   ],
   [
    "1805.03352",
    "CFKG",
    "0.8152",
    0.8152
   ],
   [
    "1905.07854",
    "KGAT",
    "0.8147",
    0.8147
   ],
   [
    "1708.05027",
    "NFM",
    "0.8132",
    0.8132
   ],
   [
    "1905.08108",
    "NGCF",
    "0.7807",
    0.7807
   ],
   [
    "1904.12575",
    "KGCN",
    "0.771",
    0.771
   ]
  ]
 },
 "./longdocdata/docs/1556.json": {
  "question": "List the performance scores of various methods on the MovieLens 25M (MovieLens) dataset on the Link Prediction task using metric nDCG@10.",
  "answer": "| Method | nDCG@10 |\n| --- | --- |\n| PEAGAT | 0.5475 |\n| NFM | 0.5347 |\n| KGAT | 0.5236 |\n| CFKG | 0.5196 |\n| NGCF | 0.4866 |\n| KGCN | 0.4699 |",
  "src_docs": [
   "1708.05027",
   "1904.12575",
   "1905.08108",
   "1905.07854",
   "2010.11793",
   "1805.03352"
  ],
  "updated_answer": [
   [
    "2010.11793",
    "PEAGAT",
    "0.5475",
    0.5475
   ],
   [
    "1708.05027",
    "NFM",
    "0.5347",
    0.5347
   ],
   [
    "1905.07854",
    "KGAT",
    "0.5236",
    0.5236
   ],
   [
    "1805.03352",
    "CFKG",
    "0.5196",
    0.5196
   ],
   [
    "1905.08108",
    "NGCF",
    "0.4866",
    0.4866
   ],
   [
    "1904.12575",
    "KGCN",
    "0.4699",
    0.4699
   ]
  ],
  "meta_info": {
   "datasets": "MovieLens 25M",
   "datasets_short": "MovieLens",
   "task": "Link Prediction",
   "metric": "nDCG@10"
  },
  "updated_answer2": [
   [
    "2010.11793",
    "PEAGAT",
    "0.5475",
    0.5475
   ],
   [
    "1708.05027",
    "NFM",
    "0.5347",
    0.5347
   ],
   [
    "1905.07854",
    "KGAT",
    "0.5236",
    0.5236
   ],
   [
    "1805.03352",
    "CFKG",
    "0.5196",
    0.5196
   ],
   [
    "1905.08108",
    "NGCF",
    "0.4866",
    0.4866
   ],
   [
    "1904.12575",
    "KGCN",
    "0.4699",
    0.4699
   ]
  ]
 },
 "./longdocdata/docs/1563.json": {
  "question": "List the performance scores of various methods on the Middlebury (Middlebury) dataset on the Video Frame Interpolation task using metric Interpolation Error.",
  "answer": "| Method | Interpolation Error |\n| --- | --- |\n| IFRNet | 4.216 |\n| SoftSplat | 4.223 |\n| BMBC | 4.479 |\n| DAIN | 4.86 |\n| MEMC-NET | 5.24 |\n| ToFlow | 5.49 |\n| SepConv-L1 | 5.61 |",
  "src_docs": [
   "2205.14620",
   "1904.00830",
   "2007.12622",
   "1708.01692",
   "1711.09078",
   "2003.05534",
   "1810.08768"
  ],
  "updated_answer": [
   [
    "2205.14620",
    "IFRNet",
    "4.216",
    4.216
   ],
   [
    "2003.05534",
    "SoftSplat",
    "4.223",
    4.223
   ],
   [
    "2007.12622",
    "BMBC",
    "4.479",
    4.479
   ],
   [
    "1904.00830",
    "DAIN",
    "4.86",
    4.86
   ],
   [
    "1810.08768",
    "MEMC-NET",
    "5.24",
    5.24
   ],
   [
    "1711.09078",
    "ToFlow",
    "5.49",
    5.49
   ],
   [
    "1708.01692",
    "SepConv-L1",
    "5.61",
    5.61
   ]
  ],
  "meta_info": {
   "datasets": "Middlebury",
   "datasets_short": "Middlebury",
   "task": "Video Frame Interpolation",
   "metric": "Interpolation Error"
  },
  "updated_answer2": [
   [
    "2205.14620",
    "IFRNet",
    "4.216",
    4.216
   ],
   [
    "2003.05534",
    "SoftSplat",
    "4.223",
    4.223
   ],
   [
    "2007.12622",
    "BMBC",
    "4.479",
    4.479
   ],
   [
    "1904.00830",
    "DAIN",
    "4.86",
    4.86
   ],
   [
    "1810.08768",
    "MEMC-NET",
    "5.24",
    5.24
   ],
   [
    "1711.09078",
    "ToFlow",
    "5.49",
    5.49
   ],
   [
    "1708.01692",
    "SepConv-L1",
    "5.61",
    5.61
   ]
  ]
 },
 "./longdocdata/docs/1611.json": {
  "question": "List the performance scores of various methods on the Kinetics-600 12 frames, 64x64 (Kinetics) dataset on the Video Prediction task using metric FVD.",
  "answer": "| Method | FVD |\n| --- | --- |\n| TriVD-GAN-FP | 25.74 |\n| CCVS | 55.0 |\n| Video VQ-VAE FVD | 64.3 |\n| DVD-GAN-FP | 69.15 |\n| Video Transformer | 170.0 |\n| LVT | 224.73 |",
  "src_docs": [
   "1907.06571",
   "2003.04035",
   "2006.10704",
   "2103.01950",
   "2107.08037",
   "1906.02634"
  ],
  "updated_answer": [
   [
    "2003.04035",
    "TriVD-GAN-FP",
    "25.74Â±0.66",
    25.74
   ],
   [
    "2107.08037",
    "CCVS",
    "55Â±1",
    55.0
   ],
   [
    "2103.01950",
    "Video VQ-VAE FVD",
    "64.30Â±2.04",
    64.3
   ],
   [
    "1907.06571",
    "DVD-GAN-FP",
    "69.15Â±0.78",
    69.15
   ],
   [
    "1906.02634",
    "Video Transformer",
    "170Â±5",
    170.0
   ],
   [
    "2006.10704",
    "LVT",
    "224.73",
    224.73
   ]
  ],
  "meta_info": {
   "datasets": "Kinetics-600  test set, 12 frames, 64x64",
   "datasets_short": "Kinetics",
   "task": "Video Prediction",
   "metric": "FVD"
  },
  "updated_answer2": [
   [
    "2003.04035",
    "TriVD-GAN-FP",
    "25.74Â±0.66",
    25.74
   ],
   [
    "2107.08037",
    "CCVS",
    "55Â±1",
    55.0
   ],
   [
    "2103.01950",
    "Video VQ-VAE FVD",
    "64.30Â±2.04",
    64.3
   ],
   [
    "1907.06571",
    "DVD-GAN-FP",
    "69.15Â±0.78",
    69.15
   ],
   [
    "1906.02634",
    "Video Transformer",
    "170Â±5",
    170.0
   ],
   [
    "2006.10704",
    "LVT",
    "224.73",
    224.73
   ]
  ]
 },
 
 "./longdocdata/docs/1614.json": {
  "question": "List the performance scores of various methods on the Kinetics-700 (Kinetics-700) dataset on the Action Classification task using metric Top-5 Accuracy.",
  "answer": "| Method | Top-5 Accuracy |\n| --- | --- |\n| MTV-H (WTS 60M) | 96.2 |\n| MaskFeat (no extra data, MViT-L) | 95.7 |\n| CoVeR (JFT-3B) | 94.9 |\n| MViTv2-L (ImageNet-21k pretrain) | 94.9 |\n| CoVeR (JFT-300M) | 94.2 |\n| MViTv2-B | 93.2 |\n| En-VidTr-L | 89.4 |\n| VidTr-L | 89.0 |\n| VidTr-M | 88.3 |\n| VidTr-S | 87.7 |\n| SRTG r3d-101 | 76.82 |\n| SRTG r(2+1)d-50 | 74.62 |\n| SRTG r3d-50 | 74.17 |\n| SRTG r(2+1)d-34 | 73.23 |\n| SRTG r3d-34 | 72.68 |",
  "src_docs": [
   "2112.09133",
   "2201.04288",
   "2112.07175",
   "2104.11746",
   "2006.08247",
   "2112.01526"
  ],
  "updated_answer": [
   [
    "2201.04288",
    "MTV-H (WTS 60M)",
    "96.2",
    96.2
   ],
   [
    "2112.09133",
    "MaskFeat (no extra data, MViT-L)",
    "95.7",
    95.7
   ],
   [
    "2112.07175",
    "CoVeR (JFT-3B)",
    "94.9",
    94.9
   ],
   [
    "2112.01526",
    "MViTv2-L (ImageNet-21k pretrain)",
    "94.9",
    94.9
   ],
   [
    "2112.07175",
    "CoVeR (JFT-300M)",
    "94.2",
    94.2
   ],
   [
    "2112.01526",
    "MViTv2-B",
    "93.2",
    93.2
   ],
   [
    "2104.11746",
    "En-VidTr-L",
    "89.4",
    89.4
   ],
   [
    "2104.11746",
    "VidTr-L",
    "89",
    89.0
   ],
   [
    "2104.11746",
    "VidTr-M",
    "88.3",
    88.3
   ],
   [
    "2104.11746",
    "VidTr-S",
    "87.7",
    87.7
   ],
   [
    "2006.08247",
    "SRTG r3d-101",
    "76.82",
    76.82
   ],
   [
    "2006.08247",
    "SRTG r(2+1)d-50",
    "74.62",
    74.62
   ],
   [
    "2006.08247",
    "SRTG r3d-50",
    "74.17",
    74.17
   ],
   [
    "2006.08247",
    "SRTG r(2+1)d-34",
    "73.23",
    73.23
   ],
   [
    "2006.08247",
    "SRTG r3d-34",
    "72.68",
    72.68
   ]
  ],
  "meta_info": {
   "datasets": "Kinetics-700",
   "datasets_short": "Kinetics-700",
   "task": "Action Classification",
   "metric": "Top-5 Accuracy"
  },
  "updated_answer2": [
   [
    "2201.04288",
    "MTV-H (WTS 60M)",
    "96.2",
    96.2
   ],
   [
    "2112.09133",
    "MaskFeat (no extra data, MViT-L)",
    "95.7",
    95.7
   ],
   [
    "2112.07175",
    "CoVeR (JFT-3B)",
    "94.9",
    94.9
   ],
   [
    "2112.01526",
    "MViTv2-L (ImageNet-21k pretrain)",
    "94.9",
    94.9
   ],
   [
    "2104.11746",
    "En-VidTr-L",
    "89.4",
    89.4
   ],
   [
    "2006.08247",
    "SRTG r3d-101",
    "76.82",
    76.82
   ]
  ]
 },


 "./longdocdata/docs/1626.json": {
  "question": "List the performance scores of various methods on the Volleyball (Volleyball) dataset on the Group Activity Recognition task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| COMPOSER | 94.69 |\n| DIN (VGG16) | 93.6 |\n| POGARS | 93.2 |\n| PoseC3D (Pose-Only) | 91.3 |\n| Zappardino et al. | 91.0 |\n| Zappardino et al. (SSAL) | 89.4 |\n| H-LSTCM | 88.4 |\n| Shu et al. | 83.6 |",
  "src_docs": [
   "1704.03058",
   "1811.00270",
   "2104.13586",
   "2112.05892",
   "2108.11743",
   "2108.04186",
   "2105.06754"
  ],
  "updated_answer": [
   [
    "2112.05892",
    "COMPOSER",
    "94.69",
    94.69
   ],
   [
    "2108.11743",
    "DIN (VGG16)",
    "93.6",
    93.6
   ],
   [
    "2108.04186",
    "POGARS",
    "93.2",
    93.2
   ],
   [
    "2104.13586",
    "PoseC3D (Pose-Only)",
    "91.3",
    91.3
   ],
   [
    "2105.06754",
    "Zappardino et al.",
    "91.0",
    91.0
   ],
   [
    "2105.06754",
    "Zappardino et al. (SSAL)",
    "89.4",
    89.4
   ],
   [
    "1811.00270",
    "H-LSTCM",
    "88.4",
    88.4
   ],
   [
    "1704.03058",
    "Shu et al.",
    "83.6",
    83.6
   ]
  ],
  "meta_info": {
   "datasets": "Volleyball",
   "datasets_short": "Volleyball",
   "task": "Group Activity Recognition",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2112.05892",
    "COMPOSER",
    "94.69",
    94.69
   ],
   [
    "2108.11743",
    "DIN (VGG16)",
    "93.6",
    93.6
   ],
   [
    "2108.04186",
    "POGARS",
    "93.2",
    93.2
   ],
   [
    "2104.13586",
    "PoseC3D (Pose-Only)",
    "91.3",
    91.3
   ],
   [
    "2105.06754",
    "Zappardino et al.",
    "91.0",
    91.0
   ],
   [
    "1811.00270",
    "H-LSTCM",
    "88.4",
    88.4
   ],
   [
    "1704.03058",
    "Shu et al.",
    "83.6",
    83.6
   ]
  ]
 },
 "./longdocdata/docs/1634.json": {
  "question": "List the performance scores of various methods on the NUS-WIDE (NUS-WIDE) dataset on the Multi-Label Classification task using metric MAP.",
  "answer": "| Method | MAP |\n| --- | --- |\n| Q2L-CvT(resolution 384, ImageNet-21K pretrained) | 70.1 |\n| Q2L-TResL(resoluition 448) | 66.3 |\n| TResNet-L (resolution 448) | 65.2 |\n| Q2L-R101(resolution 448) | 65.0 |\n| SRN | 62.0 |\n| MSRN | 61.5 |\n| MS-CMA | 61.4 |\n| S-CLs | 60.1 |",
  "src_docs": [
   "2009.14119",
   "2107.10834",
   "1702.05891",
   "1809.05884",
   "2106.11596",
   "1912.07872"
  ],
  "updated_answer": [
   [
    "2107.10834",
    "Q2L-CvT(resolution 384, ImageNet-21K pretrained)",
    "70.1",
    70.1
   ],
   [
    "2107.10834",
    "Q2L-TResL(resoluition 448)",
    "66.3",
    66.3
   ],
   [
    "2009.14119",
    "TResNet-L (resolution 448)",
    "65.2",
    65.2
   ],
   [
    "2107.10834",
    "Q2L-R101(resolution 448)",
    "65.0",
    65.0
   ],
   [
    "1702.05891",
    "SRN",
    "62.0",
    62.0
   ],
   [
    "2106.11596",
    "MSRN",
    "61.5",
    61.5
   ],
   [
    "1912.07872",
    "MS-CMA",
    "61.4",
    61.4
   ],
   [
    "1809.05884",
    "S-CLs",
    "60.1",
    60.1
   ]
  ],
  "meta_info": {
   "datasets": "NUS-WIDE",
   "datasets_short": "NUS-WIDE",
   "task": "Multi-Label Classification",
   "metric": "MAP"
  },
  "updated_answer2": [
   [
    "2107.10834",
    "Q2L-CvT(resolution 384, ImageNet-21K pretrained)",
    "70.1",
    70.1
   ],
   [
    "2009.14119",
    "TResNet-L (resolution 448)",
    "65.2",
    65.2
   ],
   [
    "1702.05891",
    "SRN",
    "62.0",
    62.0
   ],
   [
    "2106.11596",
    "MSRN",
    "61.5",
    61.5
   ],
   [
    "1912.07872",
    "MS-CMA",
    "61.4",
    61.4
   ],
   [
    "1809.05884",
    "S-CLs",
    "60.1",
    60.1
   ]
  ]
 },
 "./longdocdata/docs/1674.json": {
  "question": "List the performance scores of various methods on the CelebA-HQ 1024x1024 (CelebA-HQ) dataset on the Image Generation task using metric FID.",
  "answer": "| Method | FID |\n| --- | --- |\n| StyleSwin | 4.43 |\n| StyleGAN | 5.06 |\n| MSG-StyleGAN | 6.37 |\n| Polarity-ProGAN | 7.28 |\n| PGGAN | 7.3 |\n| HiT-B | 8.83 |\n| COCO-GAN | 9.49 |",
  "src_docs": [
   "2203.01993",
   "1710.10196",
   "2112.10762",
   "1904.00284",
   "1812.04948",
   "2106.07631",
   "1903.06048"
  ],
  "updated_answer": [
   [
    "2112.10762",
    "StyleSwin",
    "4.43",
    4.43
   ],
   [
    "1812.04948",
    "StyleGAN",
    "5.06",
    5.06
   ],
   [
    "1903.06048",
    "MSG-StyleGAN",
    "6.37",
    6.37
   ],
   [
    "2203.01993",
    "Polarity-ProGAN",
    "7.28",
    7.28
   ],
   [
    "1710.10196",
    "PGGAN",
    "7.3",
    7.3
   ],
   [
    "2106.07631",
    "HiT-B",
    "8.83",
    8.83
   ],
   [
    "1904.00284",
    "COCO-GAN",
    "9.49",
    9.49
   ]
  ],
  "meta_info": {
   "datasets": "CelebA-HQ 1024x1024",
   "datasets_short": "CelebA-HQ",
   "task": "Image Generation",
   "metric": "FID"
  },
  "updated_answer2": [
   [
    "2112.10762",
    "StyleSwin",
    "4.43",
    4.43
   ],
   [
    "1812.04948",
    "StyleGAN",
    "5.06",
    5.06
   ],
   [
    "1903.06048",
    "MSG-StyleGAN",
    "6.37",
    6.37
   ],
   [
    "2203.01993",
    "Polarity-ProGAN",
    "7.28",
    7.28
   ],
   [
    "1710.10196",
    "PGGAN",
    "7.3",
    7.3
   ],
   [
    "2106.07631",
    "HiT-B",
    "8.83",
    8.83
   ],
   [
    "1904.00284",
    "COCO-GAN",
    "9.49",
    9.49
   ]
  ]
 },
 "./longdocdata/docs/1685.json": {
  "question": "List the performance scores of various methods on the CelebA-HQ 128x128 (CelebA-HQ) dataset on the Image Generation task using metric FID.",
  "answer": "| Method | FID |\n| --- | --- |\n| U-Net GAN | 2.03 |\n| COCO-GAN | 5.74 |\n| PA-GAN | 15.4 |\n| CR+LT-SNDCGAN | 16.84 |\n| CR-GAN | 16.97 |\n| SS-GAN (sBN) | 24.36 |\n| QSNGAN | 29.417 |",
  "src_docs": [
   "1811.11212",
   "2002.12655",
   "2104.09630",
   "1910.12027",
   "1904.00284",
   "2010.09893",
   "1901.10422"
  ],
  "updated_answer": [
   [
    "2002.12655",
    "U-Net GAN",
    "2.03",
    2.03
   ],
   [
    "1904.00284",
    "COCO-GAN",
    "5.74",
    5.74
   ],
   [
    "1901.10422",
    "PA-GAN",
    "15.4",
    15.4
   ],
   [
    "2010.09893",
    "CR+LT-SNDCGAN",
    "16.84",
    16.84
   ],
   [
    "1910.12027",
    "CR-GAN",
    "16.97",
    16.97
   ],
   [
    "1811.11212",
    "SS-GAN (sBN)",
    "24.36",
    24.36
   ],
   [
    "2104.09630",
    "QSNGAN",
    "29.417",
    29.417
   ]
  ],
  "meta_info": {
   "datasets": "CelebA-HQ 128x128",
   "datasets_short": "CelebA-HQ",
   "task": "Image Generation",
   "metric": "FID"
  },
  "updated_answer2": [
   [
    "2002.12655",
    "U-Net GAN",
    "2.03",
    2.03
   ],
   [
    "1904.00284",
    "COCO-GAN",
    "5.74",
    5.74
   ],
   [
    "1901.10422",
    "PA-GAN",
    "15.4",
    15.4
   ],
   [
    "2010.09893",
    "CR+LT-SNDCGAN",
    "16.84",
    16.84
   ],
   [
    "1910.12027",
    "CR-GAN",
    "16.97",
    16.97
   ],
   [
    "1811.11212",
    "SS-GAN (sBN)",
    "24.36",
    24.36
   ],
   [
    "2104.09630",
    "QSNGAN",
    "29.417",
    29.417
   ]
  ]
 },
 "./longdocdata/docs/1728.json": {
  "question": "List the performance scores of various methods on the BIWI (BIWI) dataset on the Head Pose Estimation task using metric MAE (trained with other data).",
  "answer": "| Method | MAE (trained with other data) |\n| --- | --- |\n| 6DRepNet | 3.47 |\n| WHENet-V | 3.48 |\n| MNN | 3.66 |\n| img2pose | 3.786 |\n| WHENet | 3.81 |\n| RetinaFace R-50 (5 points) | 4.578 |\n| FAN (12 points) | 7.882 |\n| KEPLER | 13.852 |\n| 3DDFA | 19.068 |",
  "src_docs": [
   "2202.12555",
   "1702.05085",
   "1703.07332",
   "2202.02299",
   "2012.07791",
   "2005.10353",
   "1511.07212"
  ],
  "updated_answer": [
   [
    "2202.12555",
    "6DRepNet",
    "3.47",
    3.47
   ],
   [
    "2005.10353",
    "WHENet-V",
    "3.48",
    3.48
   ],
   [
    "2202.02299",
    "MNN",
    "3.66",
    3.66
   ],
   [
    "2012.07791",
    "img2pose",
    "3.786",
    3.786
   ],
   [
    "2005.10353",
    "WHENet",
    "3.81",
    3.81
   ],
   [
    "2012.07791",
    "RetinaFace R-50 (5 points)",
    "4.578",
    4.578
   ],
   [
    "1703.07332",
    "FAN (12 points)",
    "7.882",
    7.882
   ],
   [
    "1702.05085",
    "KEPLER",
    "13.852",
    13.852
   ],
   [
    "1511.07212",
    "3DDFA",
    "19.068",
    19.068
   ]
  ],
  "meta_info": {
   "datasets": "BIWI",
   "datasets_short": "BIWI",
   "task": "Head Pose Estimation",
   "metric": "MAE (trained with other data)"
  },
  "updated_answer2": [
   [
    "2202.12555",
    "6DRepNet",
    "3.47",
    3.47
   ],
   [
    "2005.10353",
    "WHENet-V",
    "3.48",
    3.48
   ],
   [
    "2202.02299",
    "MNN",
    "3.66",
    3.66
   ],
   [
    "2012.07791",
    "img2pose",
    "3.786",
    3.786
   ],
   [
    "1703.07332",
    "FAN (12 points)",
    "7.882",
    7.882
   ],
   [
    "1702.05085",
    "KEPLER",
    "13.852",
    13.852
   ],
   [
    "1511.07212",
    "3DDFA",
    "19.068",
    19.068
   ]
  ]
 },
 "./longdocdata/docs/1737.json": {
  "question": "List the performance scores of various methods on the YCB-Video (YCB-Video) dataset on the 6D Pose Estimation task using metric ADDS AUC.",
  "answer": "| Method | ADDS AUC |\n| --- | --- |\n| FFB6D | 96.6 |\n| ICG | 96.5 |\n| PVN3D | 96.1 |\n| se3-TrackNet | 95.71 |\n| MaskedFusion | 93.3 |\n| DenseFusion | 93.1 |\n| PoseCNN+ICP | 93.0 |",
  "src_docs": [
   "2103.02242",
   "1911.07771",
   "2203.05334",
   "1711.00199",
   "2007.13866",
   "1911.04231",
   "1901.04780"
  ],
  "updated_answer": [
   [
    "2103.02242",
    "FFB6D",
    "96.6",
    96.6
   ],
   [
    "2203.05334",
    "ICG",
    "96.5",
    96.5
   ],
   [
    "1911.04231",
    "PVN3D",
    "96.1",
    96.1
   ],
   [
    "2007.13866",
    "se3-TrackNet",
    "95.71",
    95.71
   ],
   [
    "1911.07771",
    "MaskedFusion",
    "93.3",
    93.3
   ],
   [
    "1901.04780",
    "DenseFusion",
    "93.1",
    93.1
   ],
   [
    "1711.00199",
    "PoseCNN+ICP",
    "93.0",
    93.0
   ]
  ],
  "meta_info": {
   "datasets": "YCB-Video",
   "datasets_short": "YCB-Video",
   "task": "6D Pose Estimation",
   "metric": "ADDS AUC"
  },
  "updated_answer2": [
   [
    "2103.02242",
    "FFB6D",
    "96.6",
    96.6
   ],
   [
    "2203.05334",
    "ICG",
    "96.5",
    96.5
   ],
   [
    "1911.04231",
    "PVN3D",
    "96.1",
    96.1
   ],
   [
    "2007.13866",
    "se3-TrackNet",
    "95.71",
    95.71
   ],
   [
    "1911.07771",
    "MaskedFusion",
    "93.3",
    93.3
   ],
   [
    "1901.04780",
    "DenseFusion",
    "93.1",
    93.1
   ],
   [
    "1711.00199",
    "PoseCNN+ICP",
    "93.0",
    93.0
   ]
  ]
 },
 "./longdocdata/docs/1758.json": {
  "question": "List the performance scores of various methods on the Darmstadt Noise Dataset (Darmstadt Noise Dataset) dataset on the Color Image Denoising task using metric SSIM (sRGB).",
  "answer": "| Method | SSIM (sRGB) |\n| --- | --- |\n| Image Unprocessing | 0.9641 |\n| PRIDNet (blind) | 0.9528 |\n| RIDNet (blind) | 0.9526 |\n| CBDNet (Blind) | 0.9421 |\n| TWSC | 0.9403 |\n| TNRD | 0.8306 |",
  "src_docs": [
   "1508.02848",
   "1904.07396",
   "1807.04686",
   "1908.00273",
   "1807.04364",
   "1811.11127"
  ],
  "updated_answer": [
   [
    "1811.11127",
    "Image Unprocessing",
    "0.9641",
    0.9641
   ],
   [
    "1908.00273",
    "PRIDNet (blind)",
    "0.9528",
    0.9528
   ],
   [
    "1904.07396",
    "RIDNet (blind)",
    "0.9526",
    0.9526
   ],
   [
    "1807.04686",
    "CBDNet (Blind)",
    "0.9421",
    0.9421
   ],
   [
    "1807.04364",
    "TWSC",
    "0.9403",
    0.9403
   ],
   [
    "1508.02848",
    "TNRD",
    "0.8306",
    0.8306
   ]
  ],
  "meta_info": {
   "datasets": "Darmstadt Noise Dataset",
   "datasets_short": "Darmstadt Noise Dataset",
   "task": "Color Image Denoising",
   "metric": "SSIM (sRGB)"
  },
  "updated_answer2": [
   [
    "1811.11127",
    "Image Unprocessing",
    "0.9641",
    0.9641
   ],
   [
    "1908.00273",
    "PRIDNet (blind)",
    "0.9528",
    0.9528
   ],
   [
    "1904.07396",
    "RIDNet (blind)",
    "0.9526",
    0.9526
   ],
   [
    "1807.04686",
    "CBDNet (Blind)",
    "0.9421",
    0.9421
   ],
   [
    "1807.04364",
    "TWSC",
    "0.9403",
    0.9403
   ],
   [
    "1508.02848",
    "TNRD",
    "0.8306",
    0.8306
   ]
  ]
 },
 "./longdocdata/docs/1759.json": {
  "question": "List the performance scores of various methods on the Darmstadt Noise Dataset (Darmstadt Noise Dataset) dataset on the Color Image Denoising task using metric PSNR (sRGB).",
  "answer": "| Method | PSNR (sRGB) |\n| --- | --- |\n| Image Unprocessing | 40.35 |\n| PRIDNet (blind) | 39.4 |\n| RIDNet (blind) | 39.23 |\n| CBDNet (Blind) | 38.06 |\n| TWSC | 37.94 |\n| TNRD | 33.65 |",
  "src_docs": [
   "1508.02848",
   "1904.07396",
   "1807.04686",
   "1908.00273",
   "1807.04364",
   "1811.11127"
  ],
  "updated_answer": [
   [
    "1811.11127",
    "Image Unprocessing",
    "40.35",
    40.35
   ],
   [
    "1908.00273",
    "PRIDNet (blind)",
    "39.4",
    39.4
   ],
   [
    "1904.07396",
    "RIDNet (blind)",
    "39.23",
    39.23
   ],
   [
    "1807.04686",
    "CBDNet (Blind)",
    "38.06",
    38.06
   ],
   [
    "1807.04364",
    "TWSC",
    "37.94",
    37.94
   ],
   [
    "1508.02848",
    "TNRD",
    "33.65",
    33.65
   ]
  ],
  "meta_info": {
   "datasets": "Darmstadt Noise Dataset",
   "datasets_short": "Darmstadt Noise Dataset",
   "task": "Color Image Denoising",
   "metric": "PSNR (sRGB)"
  },
  "updated_answer2": [
   [
    "1811.11127",
    "Image Unprocessing",
    "40.35",
    40.35
   ],
   [
    "1908.00273",
    "PRIDNet (blind)",
    "39.42",
    39.42
   ],
   [
    "1904.07396",
    "RIDNet (blind)",
    "39.23",
    39.23
   ],
   [
    "1807.04686",
    "CBDNet (Blind)",
    "38.06",
    38.06
   ],
   [
    "1807.04364",
    "TWSC",
    "37.94",
    37.94
   ],
   [
    "1508.02848",
    "TNRD",
    "33.65",
    33.65
   ]
  ]
 },
 "./longdocdata/docs/1829.json": {
  "question": "List the performance scores of various methods on the DAVIS 2017 (DAVIS 2017) dataset on the Interactive Video Object Segmentation task using metric J@60s.",
  "answer": "| Method | J@60s |\n| --- | --- |\n| MiVOS | 0.854 |\n| GIS | 0.829 |\n| AT-Net | 0.79 |\n| GNNannot | 0.767 |\n| MA-Net | 0.761 |\n| FUGVOS | 0.734 |",
  "src_docs": [
   "2103.07941",
   "1904.09791",
   "2104.10386",
   "2003.13246",
   "2007.08139",
   "2103.03821"
  ],
  "updated_answer": [
   [
    "2103.07941",
    "MiVOS",
    "0.854",
    0.854
   ],
   [
    "2104.10386",
    "GIS",
    "0.829",
    0.829
   ],
   [
    "2007.08139",
    "AT-Net",
    "0.790",
    0.79
   ],
   [
    "2103.03821",
    "GNNannot",
    "0.767",
    0.767
   ],
   [
    "2003.13246",
    "MA-Net",
    "0.761",
    0.761
   ],
   [
    "1904.09791",
    "FUGVOS",
    "0.734",
    0.734
   ]
  ],
  "meta_info": {
   "datasets": "DAVIS 2017 validation set",
   "datasets_short": "DAVIS 2017",
   "task": "Interactive Video Object Segmentation",
   "metric": "J@60s"
  },
  "updated_answer2": [
   [
    "2103.07941",
    "MiVOS",
    "0.854",
    0.854
   ],
   [
    "2104.10386",
    "GIS",
    "0.829",
    0.829
   ],
   [
    "2007.08139",
    "AT-Net",
    "0.790",
    0.79
   ],
   [
    "2103.03821",
    "GNNannot",
    "0.767",
    0.767
   ],
   [
    "2003.13246",
    "MA-Net",
    "0.761",
    0.761
   ],
   [
    "1904.09791",
    "FUGVOS",
    "0.734",
    0.734
   ]
  ]
 },
 "./longdocdata/docs/1832.json": {
  "question": "List the performance scores of various methods on the DAVIS 2017 (DAVIS 2017) dataset on the Interactive Video Object Segmentation task using metric AUC-J.",
  "answer": "| Method | AUC-J |\n| --- | --- |\n| MiVOS | 0.849 |\n| GIS | 0.82 |\n| AT-Net | 0.778 |\n| GNNannot | 0.759 |\n| MA-Net | 0.749 |\n| FUGVOS | 0.691 |",
  "src_docs": [
   "2103.07941",
   "1904.09791",
   "2104.10386",
   "2003.13246",
   "2007.08139",
   "2103.03821"
  ],
  "updated_answer": [
   [
    "2103.07941",
    "MiVOS",
    "0.849",
    0.849
   ],
   [
    "2104.10386",
    "GIS",
    "0.820",
    0.82
   ],
   [
    "2007.08139",
    "AT-Net",
    "0.778",
    0.778
   ],
   [
    "2103.03821",
    "GNNannot",
    "0.759",
    0.759
   ],
   [
    "2003.13246",
    "MA-Net",
    "0.749",
    0.749
   ],
   [
    "1904.09791",
    "FUGVOS",
    "0.691",
    0.691
   ]
  ],
  "meta_info": {
   "datasets": "DAVIS 2017",
   "datasets_short": "DAVIS 2017",
   "task": "Interactive Video Object Segmentation",
   "metric": "AUC-J"
  },
  "updated_answer2": [
   [
    "2103.07941",
    "MiVOS",
    "0.849",
    0.849
   ],
   [
    "2104.10386",
    "GIS",
    "0.820",
    0.82
   ],
   [
    "2007.08139",
    "AT-Net",
    "0.778",
    0.778
   ],
   [
    "2103.03821",
    "GNNannot",
    "0.759",
    0.759
   ],
   [
    "2003.13246",
    "MA-Net",
    "0.749",
    0.749
   ],
   [
    "1904.09791",
    "FUGVOS",
    "0.691",
    0.691
   ]
  ]
 },
 
 
 
 
 "./longdocdata/docs/2004.json": {
  "question": "List the performance scores of various methods on the CIFAR-100 (CIFAR-100) dataset on the Image Clustering task using metric ARI.",
  "answer": "| Method | ARI |\n| --- | --- |\n| SPICE* | 0.422 |\n| IMC-SwAV (Best) | 0.361 |\n| IMC-SwAV (Avg+-) | 0.337 |\n| SCAN | 0.333 |\n| ConCURL | 0.303 |\n| SCAN (Avg) | 0.301 |\n| CC | 0.266 |\n| IDFD | 0.264 |",
  "src_docs": [
   "2105.01289",
   "2009.09687",
   "2103.07368",
   "2103.09382",
   "2106.00131",
   "2005.12320"
  ],
  "updated_answer": [
   [
    "2103.09382",
    "SPICE*",
    "0.422",
    0.422
   ],
   [
    "2103.07368",
    "IMC-SwAV (Best)",
    "0.361",
    0.361
   ],
   [
    "2103.07368",
    "IMC-SwAV (Avg+-)",
    "0.337",
    0.337
   ],
   [
    "2005.12320",
    "SCAN",
    "0.333",
    0.333
   ],
   [
    "2105.01289",
    "ConCURL",
    "0.303",
    0.303
   ],
   [
    "2005.12320",
    "SCAN (Avg)",
    "0.301",
    0.301
   ],
   [
    "2009.09687",
    "CC",
    "0.266",
    0.266
   ],
   [
    "2106.00131",
    "IDFD",
    "0.264",
    0.264
   ]
  ],
  "meta_info": {
   "datasets": "CIFAR-100",
   "datasets_short": "CIFAR-100",
   "task": "Image Clustering",
   "metric": "ARI"
  },
  "updated_answer2": [
   [
    "2103.09382",
    "SPICE*",
    "0.422",
    0.422
   ],
   [
    "2103.07368",
    "IMC-SwAV (Best)",
    "0.361",
    0.361
   ],
   [
    "2005.12320",
    "SCAN",
    "0.333",
    0.333
   ],
   [
    "2105.01289",
    "ConCURL",
    "0.303",
    0.303
   ],
   [
    "2009.09687",
    "CC",
    "0.266",
    0.266
   ],
   [
    "2106.00131",
    "IDFD",
    "0.264",
    0.264
   ]
  ]
 },


 "./longdocdata/docs/2081.json": {
  "question": "List the performance scores of various methods on the Human3.6M (Human3.6M) dataset on the Unsupervised 3D Human Pose Estimation task using metric MPJPE.",
  "answer": "| Method | MPJPE |\n| --- | --- |\n| ElePose | 64.0 |\n| HDVR | 82.1 |\n| ITES-TS | 85.3 |\n| Non-Local Latent Relation Distillation | 97.8 |\n| SVMAC | 98.3 |\n| PGNIS | 99.2 |\n| Uncertainty-Aware Adaptation | 103.2 |",
  "src_docs": [
   "2004.04400",
   "2204.01971",
   "2112.07088",
   "2109.09166",
   "2012.09398",
   "2203.15293",
   "2106.05616"
  ],
  "updated_answer": [
   [
    "2112.07088",
    "ElePose",
    "64.0",
    64.0
   ],
   [
    "2109.09166",
    "HDVR",
    "82.1",
    82.1
   ],
   [
    "2012.09398",
    "ITES-TS",
    "85.3",
    85.3
   ],
   [
    "2204.01971",
    "Non-Local Latent Relation Distillation",
    "97.8",
    97.8
   ],
   [
    "2106.05616",
    "SVMAC",
    "98.3",
    98.3
   ],
   [
    "2004.04400",
    "PGNIS",
    "99.2",
    99.2
   ],
   [
    "2203.15293",
    "Uncertainty-Aware Adaptation",
    "103.2",
    103.2
   ]
  ],
  "meta_info": {
   "datasets": "Human3.6M",
   "datasets_short": "Human3.6M",
   "task": "Unsupervised 3D Human Pose Estimation",
   "metric": "MPJPE"
  },
  "updated_answer2": [
   [
    "2112.07088",
    "ElePose",
    "64.0",
    64.0
   ],
   [
    "2109.09166",
    "HDVR",
    "82.1",
    82.1
   ],
   [
    "2012.09398",
    "ITES-TS",
    "85.3",
    85.3
   ],
   [
    "2204.01971",
    "Non-Local Latent Relation Distillation",
    "97.8",
    97.8
   ],
   [
    "2106.05616",
    "SVMAC",
    "98.3",
    98.3
   ],
   [
    "2004.04400",
    "PGNIS",
    "99.2",
    99.2
   ],
   [
    "2203.15293",
    "Uncertainty-Aware Adaptation",
    "103.2",
    103.2
   ]
  ]
 },

 "./longdocdata/docs/2099.json": {
  "question": "List the performance scores of various methods on the Human3.6M (Human3.6M) dataset on the Human Pose Forecasting task using metric MMFDE.",
  "answer": "| Method | MMFDE |\n| --- | --- |\n| GSPS | 525.0 |\n| BoM | 544.0 |\n| GMVAE | 566.0 |\n| Pose-Knows | 569.0 |\n| DSF | 599.0 |\n| HP-GAN | 858.0 |\n| MT-VAE | 883.0 |",
  "src_docs": [
   "2108.08422",
   "1808.04545",
   "1611.02648",
   "1907.04967",
   "1806.07772",
   "1711.09561",
   "1705.00053"
  ],
  "updated_answer": [
   [
    "2108.08422",
    "GSPS",
    "525",
    525.0
   ],
   [
    "1907.04967",
    "DSF",
    "599",
    599.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "858",
    858.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "566",
    566.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "569",
    569.0
   ],
   [
    "1806.07772",
    "BoM",
    "544",
    544.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "883",
    883.0
   ]
  ],
  "meta_info": {
   "datasets": "Human3.6M",
   "datasets_short": "Human3.6M",
   "task": "Human Pose Forecasting",
   "metric": "MMFDE"
  },
  "updated_answer2": [
   [
    "2108.08422",
    "GSPS",
    "525",
    525.0
   ],
   [
    "1806.07772",
    "BoM",
    "544",
    544.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "566",
    566.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "569",
    569.0
   ],
   [
    "1907.04967",
    "DSF",
    "599",
    599.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "858",
    858.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "883",
    883.0
   ]
  ]
 },
 "./longdocdata/docs/2100.json": {
  "question": "List the performance scores of various methods on the Human3.6M (Human3.6M) dataset on the Human Pose Forecasting task using metric APD.",
  "answer": "| Method | APD |\n| --- | --- |\n| GSPS | 14757.0 |\n| DSF | 9330.0 |\n| HP-GAN | 7214.0 |\n| GMVAE | 6769.0 |\n| Pose-Knows | 6723.0 |\n| BoM | 6265.0 |\n| MT-VAE | 403.0 |",
  "src_docs": [
   "2108.08422",
   "1808.04545",
   "1611.02648",
   "1907.04967",
   "1806.07772",
   "1711.09561",
   "1705.00053"
  ],
  "updated_answer": [
   [
    "2108.08422",
    "GSPS",
    "14757",
    14757.0
   ],
   [
    "1907.04967",
    "DSF",
    "9330",
    9330.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "7214",
    7214.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "6769",
    6769.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "6723",
    6723.0
   ],
   [
    "1806.07772",
    "BoM",
    "6265",
    6265.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "403",
    403.0
   ]
  ],
  "meta_info": {
   "datasets": "Human3.6M",
   "datasets_short": "Human3.6M",
   "task": "Human Pose Forecasting",
   "metric": "APD"
  },
  "updated_answer2": [
   [
    "2108.08422",
    "GSPS",
    "14.757",
    14757.0
   ],
   [
    "1907.04967",
    "DSF",
    "9.330",
    9330.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "7.214",
    7214.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "6.769",
    6769.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "6.723",
    6723.0
   ],
   [
    "1806.07772",
    "BoM",
    "6.265",
    6265.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "0.403",
    403.0
   ]
  ]
 },
 "./longdocdata/docs/2101.json": {
  "question": "List the performance scores of various methods on the Human3.6M (Human3.6M) dataset on the Human Pose Forecasting task using metric MMADE.",
  "answer": "| Method | MMADE |\n| --- | --- |\n| GSPS | 476.0 |\n| BoM | 514.0 |\n| Pose-Knows | 522.0 |\n| GMVAE | 524.0 |\n| DSF | 550.0 |\n| MT-VAE | 716.0 |\n| HP-GAN | 847.0 |",
  "src_docs": [
   "2108.08422",
   "1808.04545",
   "1611.02648",
   "1907.04967",
   "1806.07772",
   "1711.09561",
   "1705.00053"
  ],
  "updated_answer": [
   [
    "2108.08422",
    "GSPS",
    "476",
    476.0
   ],
   [
    "1907.04967",
    "DSF",
    "550",
    550.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "847",
    847.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "524",
    524.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "522",
    522.0
   ],
   [
    "1806.07772",
    "BoM",
    "514",
    514.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "716",
    716.0
   ]
  ],
  "meta_info": {
   "datasets": "Human3.6M",
   "datasets_short": "Human3.6M",
   "task": "Human Pose Forecasting",
   "metric": "MMADE"
  },
  "updated_answer2": [
   [
    "2108.08422",
    "GSPS",
    "476",
    476.0
   ],
   [
    "1806.07772",
    "BoM",
    "514",
    514.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "522",
    522.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "524",
    524.0
   ],
   [
    "1907.04967",
    "DSF",
    "550",
    550.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "716",
    716.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "847",
    847.0
   ]
  ]
 },
 
 "./longdocdata/docs/2105.json": {
  "question": "List the performance scores of various methods on the Human3.6M (Human3.6M) dataset on the Human Pose Forecasting task using metric FDE.",
  "answer": "| Method | FDE |\n| --- | --- |\n| GSPS | 496.0 |\n| BoM | 533.0 |\n| GMVAE | 555.0 |\n| Pose-Knows | 560.0 |\n| DSF | 592.0 |\n| MT-VAE | 595.0 |\n| HP-GAN | 867.0 |",
  "src_docs": [
   "2108.08422",
   "1808.04545",
   "1611.02648",
   "1907.04967",
   "1806.07772",
   "1711.09561",
   "1705.00053"
  ],
  "updated_answer": [
   [
    "2108.08422",
    "GSPS",
    "496",
    496.0
   ],
   [
    "1907.04967",
    "DSF",
    "592",
    592.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "867",
    867.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "555",
    555.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "560",
    560.0
   ],
   [
    "1806.07772",
    "BoM",
    "533",
    533.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "595",
    595.0
   ]
  ],
  "meta_info": {
   "datasets": "Human3.6M",
   "datasets_short": "Human3.6M",
   "task": "Human Pose Forecasting",
   "metric": "FDE"
  },
  "updated_answer2": [
   [
    "2108.08422",
    "GSPS",
    "496",
    496.0
   ],
   [
    "1806.07772",
    "BoM",
    "533",
    533.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "555",
    555.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "560",
    560.0
   ],
   [
    "1907.04967",
    "DSF",
    "592",
    592.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "595",
    595.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "867",
    867.0
   ]
  ]
 },
 "./longdocdata/docs/2108.json": {
  "question": "List the performance scores of various methods on the Human3.6M (Human3.6M) dataset on the Human Pose Forecasting task using metric ADE.",
  "answer": "| Method | ADE |\n| --- | --- |\n| GSPS | 389.0 |\n| BoM | 448.0 |\n| MT-VAE | 457.0 |\n| GMVAE | 461.0 |\n| Pose-Knows | 461.0 |\n| DSF | 493.0 |\n| HP-GAN | 858.0 |",
  "src_docs": [
   "2108.08422",
   "1808.04545",
   "1611.02648",
   "1907.04967",
   "1806.07772",
   "1711.09561",
   "1705.00053"
  ],
  "updated_answer": [
   [
    "2108.08422",
    "GSPS",
    "389",
    389.0
   ],
   [
    "1907.04967",
    "DSF",
    "493",
    493.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "858",
    858.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "461",
    461.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "461",
    461.0
   ],
   [
    "1806.07772",
    "BoM",
    "448",
    448.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "457",
    457.0
   ]
  ],
  "meta_info": {
   "datasets": "Human3.6M",
   "datasets_short": "Human3.6M",
   "task": "Human Pose Forecasting",
   "metric": "ADE"
  },
  "updated_answer2": [
   [
    "2108.08422",
    "GSPS",
    "389",
    389.0
   ],
   [
    "1806.07772",
    "BoM",
    "448",
    448.0
   ],
   [
    "1808.04545",
    "MT-VAE",
    "457",
    457.0
   ],
   [
    "1611.02648",
    "GMVAE",
    "461",
    461.0
   ],
   [
    "1705.00053",
    "Pose-Knows",
    "461",
    461.0
   ],
   [
    "1907.04967",
    "DSF",
    "493",
    493.0
   ],
   [
    "1711.09561",
    "HP-GAN",
    "858",
    858.0
   ]
  ]
 },
 "./longdocdata/docs/2234.json": {
  "question": "List the performance scores of various methods on the WebNLG Full (WebNLG) dataset on the Data-to-Text Generation task using metric BLEU.",
  "answer": "| Method | BLEU |\n| --- | --- |\n| Control Prefixes (A1, A2, T5-large) | 62.27 |\n| Control Prefixes (A1, T5-large) | 61.94 |\n| T5-large + Wiki + Position | 60.56 |\n| T5-large | 59.7 |\n| T5-Large | 57.1 |\n| HTLM (prefix 0.1%) | 56.3 |\n| DATATUNER_NO_FC | 52.9 |\n| Transformer (Pipeline) | 51.68 |",
  "src_docs": [
   "1908.09022",
   "2107.06955",
   "2004.06577",
   "2007.08426",
   "2005.10433",
   "2105.08021",
   "2110.08329"
  ],
  "updated_answer": [
   [
    "2110.08329",
    "Control Prefixes (A1, A2, T5-large)",
    "62.27 ",
    62.27
   ],
   [
    "2110.08329",
    "Control Prefixes (A1, T5-large)",
    "61.94",
    61.94
   ],
   [
    "2105.08021",
    "T5-large + Wiki + Position",
    "60.56",
    60.56
   ],
   [
    "2007.08426",
    "T5-large",
    "59.70",
    59.7
   ],
   [
    "2005.10433",
    "T5-Large",
    "57.1",
    57.1
   ],
   [
    "2107.06955",
    "HTLM (prefix 0.1%)",
    "56.3",
    56.3
   ],
   [
    "2004.06577",
    "DATATUNER_NO_FC",
    "52.9",
    52.9
   ],
   [
    "1908.09022",
    "Transformer (Pipeline)",
    "51.68",
    51.68
   ]
  ],
  "meta_info": {
   "datasets": "WebNLG Full",
   "datasets_short": "WebNLG",
   "task": "Data-to-Text Generation",
   "metric": "BLEU"
  },
  "updated_answer2": [
   [
    "2110.08329",
    "Control Prefixes (A1, A2, T5-large)",
    "62.27 ",
    62.27
   ],
   [
    "2105.08021",
    "T5-large + Wiki + Position",
    "60.56",
    60.56
   ],
   [
    "2007.08426",
    "T5-large",
    "59.70",
    59.7
   ],
   [
    "2005.10433",
    "T5-Large",
    "57.1",
    57.1
   ],
   [
    "2107.06955",
    "HTLM (prefix 0.1%)",
    "56.3",
    56.3
   ],
   [
    "2004.06577",
    "DATATUNER_NO_FC",
    "52.9",
    52.9
   ],
   [
    "1908.09022",
    "Transformer (Pipeline)",
    "51.68",
    51.68
   ]
  ]
 },
 "./longdocdata/docs/2282.json": {
  "question": "List the performance scores of various methods on the AFLW2000-3D (AFLW2000-3D) dataset on the 3D Face Reconstruction task using metric Mean NME .",
  "answer": "| Method | Mean NME  |\n| --- | --- |\n| SADRNet | 3.25 |\n| VGG-F | 3.42 |\n| B-spline FFD | 3.51 |\n| 3DDFA-V2 | 3.56 |\n| PRN | 3.9625 |\n| 3DDFA | 5.3695 |\n| DeFA | 5.6454 |",
  "src_docs": [
   "2103.16554",
   "2106.03021",
   "2105.14857",
   "1709.01442",
   "2009.09960",
   "1803.07835",
   "1511.07212"
  ],
  "updated_answer": [
   [
    "2106.03021",
    "SADRNet",
    "3.25%",
    3.25
   ],
   [
    "2103.16554",
    "VGG-F",
    "3.42%",
    3.42
   ],
   [
    "2105.14857",
    "B-spline FFD",
    "3.51%",
    3.51
   ],
   [
    "2009.09960",
    "3DDFA-V2",
    "3.56%",
    3.56
   ],
   [
    "1803.07835",
    "PRN",
    "3.9625%",
    3.9625
   ],
   [
    "1511.07212",
    "3DDFA",
    "5.3695%",
    5.3695
   ],
   [
    "1709.01442",
    "DeFA",
    "5.6454%",
    5.6454
   ]
  ],
  "meta_info": {
   "datasets": "AFLW2000-3D",
   "datasets_short": "AFLW2000-3D",
   "task": "3D Face Reconstruction",
   "metric": "Mean NME "
  },
  "updated_answer2": [
   [
    "2106.03021",
    "SADRNet",
    "3.25%",
    3.25
   ],
   [
    "2103.16554",
    "VGG-F",
    "3.42%",
    3.42
   ],
   [
    "2105.14857",
    "B-spline FFD",
    "3.51%",
    3.51
   ],
   [
    "2009.09960",
    "3DDFA-V2",
    "3.56%",
    3.56
   ],
   [
    "1803.07835",
    "PRN",
    "3.9625%",
    3.9625
   ],
   [
    "1511.07212",
    "3DDFA",
    "5.3695%",
    5.3695
   ],
   [
    "1709.01442",
    "DeFA",
    "5.6454%",
    5.6454
   ]
  ]
 },
 "./longdocdata/docs/2307.json": {
  "question": "List the performance scores of various methods on the Caltech-101 (Caltech-101) dataset on the Fine-Grained Image Classification task using metric Top-1 Error Rate.",
  "answer": "| Method | Top-1 Error Rate |\n| --- | --- |\n| Wide-ResNet-101 (Spinal FC) | 2.68 |\n| Wide-ResNet-101 | 2.89 |\n| ResNeXt-101-32x8d | 4.42 |\n| TWIST (ResNet-50 ) | 6.5 |\n| VGG-19bn (Spinal FC) | 6.84 |\n| Âµ2Net (ViT-L/16) | 7.0 |\n| NNCLR | 8.7 |\n| SEER (RegNet10B - linear eval) | 9.0 |\n| AutoAugment | 13.07 |",
  "src_docs": [
   "2202.08360",
   "2205.12755",
   "2007.03347",
   "2108.13576",
   "2110.07402",
   "2104.14548",
   "1805.09501"
  ],
  "updated_answer": [
   [
    "2007.03347",
    "Wide-ResNet-101 (Spinal FC)",
    "2.68%",
    2.68
   ],
   [
    "2007.03347",
    "Wide-ResNet-101",
    "2.89%",
    2.89
   ],
   [
    "2108.13576",
    "ResNeXt-101-32x8d",
    "4.42%",
    4.42
   ],
   [
    "2110.07402",
    "TWIST (ResNet-50 )",
    "6.5%",
    6.5
   ],
   [
    "2007.03347",
    "VGG-19bn (Spinal FC)",
    "6.84%",
    6.84
   ],
   [
    "2205.12755",
    "Âµ2Net (ViT-L/16)",
    "7%",
    7.0
   ],
   [
    "2104.14548",
    "NNCLR",
    "8.7%",
    8.7
   ],
   [
    "2202.08360",
    "SEER (RegNet10B - linear eval)",
    "9.0%",
    9.0
   ],
   [
    "1805.09501",
    "AutoAugment",
    "13.07%",
    13.07
   ]
  ],
  "meta_info": {
   "datasets": "Caltech-101",
   "datasets_short": "Caltech-101",
   "task": "Fine-Grained Image Classification",
   "metric": "Top-1 Error Rate"
  },
  "updated_answer2": [
   [
    "2007.03347",
    "Wide-ResNet-101 (Spinal FC)",
    "2.68%",
    2.68
   ],
   [
    "2108.13576",
    "ResNeXt-101-32x8d",
    "4.42%",
    4.42
   ],
   [
    "2110.07402",
    "TWIST (ResNet-50 )",
    "6.5%",
    6.5
   ],
   [
    "2205.12755",
    "Âµ2Net (ViT-L/16)",
    "7%",
    7.0
   ],
   [
    "2104.14548",
    "NNCLR",
    "8.7%",
    8.7
   ],
   [
    "2202.08360",
    "SEER (RegNet10B - linear eval)",
    "9.0%",
    9.0
   ],
   [
    "1805.09501",
    "AutoAugment",
    "13.07%",
    13.07
   ]
  ]
 },
 "./longdocdata/docs/2479.json": {
  "question": "List the performance scores of various methods on the Citeseer (Citeseer) dataset on the Link Prediction task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| VGNAE | 97.1 |\n| GNAE | 97.0 |\n| Graph InfoClust (GIC) | 96.8 |\n| Walkpooling | 96.04 |\n| sGraphite-VAE | 95.4 |\n| S-VGAE | 95.2 |\n| ARGE | 93.0 |\n| Node Feature Agg + Similarity Metric | 91.8 |",
  "src_docs": [
   "2009.06946",
   "1803.10459",
   "1910.02548",
   "1802.04407",
   "2110.04375",
   "1804.00891",
   "2108.08046"
  ],
  "updated_answer": [
   [
    "2108.08046",
    "VGNAE",
    "97.1",
    97.1
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "96.8",
    96.8
   ],
   [
    "2108.08046",
    "GNAE",
    "97",
    97.0
   ],
   [
    "2110.04375",
    "Walkpooling",
    "96.04",
    96.04
   ],
   [
    "1804.00891",
    "S-VGAE",
    "95.2",
    95.2
   ],
   [
    "1803.10459",
    "sGraphite-VAE",
    "95.4%",
    95.4
   ],
   [
    "1802.04407",
    "ARGE",
    "93",
    93.0
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "91.8%",
    91.8
   ]
  ],
  "meta_info": {
   "datasets": "Citeseer",
   "datasets_short": "Citeseer",
   "task": "Link Prediction",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "2108.08046",
    "VGNAE",
    "97.1",
    97.1
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "96.8",
    96.8
   ],
   [
    "2110.04375",
    "Walkpooling",
    "96.04",
    96.04
   ],
   [
    "1803.10459",
    "sGraphite-VAE",
    "95.4%",
    95.4
   ],
   [
    "1804.00891",
    "S-VGAE",
    "95.2",
    95.2
   ],
   [
    "1802.04407",
    "ARGE",
    "93",
    93.0
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "91.8%",
    91.8
   ]
  ]
 },
 "./longdocdata/docs/2481.json": {
  "question": "List the performance scores of various methods on the Citeseer (Citeseer) dataset on the Link Prediction task using metric AUC.",
  "answer": "| Method | AUC |\n| --- | --- |\n| VGNAE | 97.0 |\n| Graph InfoClust (GIC) | 97.0 |\n| GNAE | 96.5 |\n| Walkpooling | 95.94 |\n| S-VGAE | 94.7 |\n| sGraphite-VAE | 94.1 |\n| ARGE | 91.9 |\n| Node Feature Agg + Similarity Metric | 90.9 |",
  "src_docs": [
   "2009.06946",
   "1803.10459",
   "1910.02548",
   "1802.04407",
   "2110.04375",
   "1804.00891",
   "2108.08046"
  ],
  "updated_answer": [
   [
    "2108.08046",
    "VGNAE",
    "97",
    97.0
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "97",
    97.0
   ],
   [
    "2108.08046",
    "GNAE",
    "96.5",
    96.5
   ],
   [
    "2110.04375",
    "Walkpooling",
    "95.94",
    95.94
   ],
   [
    "1804.00891",
    "S-VGAE",
    "94.7",
    94.7
   ],
   [
    "1803.10459",
    "sGraphite-VAE",
    "94.1%",
    94.1
   ],
   [
    "1802.04407",
    "ARGE",
    "91.9",
    91.9
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "90.9%",
    90.9
   ]
  ],
  "meta_info": {
   "datasets": "Citeseer",
   "datasets_short": "Citeseer",
   "task": "Link Prediction",
   "metric": "AUC"
  },
  "updated_answer2": [
   [
    "2108.08046",
    "VGNAE",
    "97",
    97.0
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "97",
    97.0
   ],
   [
    "2110.04375",
    "Walkpooling",
    "95.94",
    95.94
   ],
   [
    "1804.00891",
    "S-VGAE",
    "94.7",
    94.7
   ],
   [
    "1803.10459",
    "sGraphite-VAE",
    "94.1%",
    94.1
   ],
   [
    "1802.04407",
    "ARGE",
    "91.9",
    91.9
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "90.9%",
    90.9
   ]
  ]
 },
 "./longdocdata/docs/2523.json": {
  "question": "List the performance scores of various methods on the Cora Full-supervised (Cora) dataset on the Node Classification task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| GCNII | 88.49 |\n| IncepGCN+DropEdge | 88.2 |\n| FDGATII | 87.7867 |\n| ASGCN | 87.44 |\n| FastGCN | 85.0 |\n| GraphSAGE | 82.2 |\n| GraphMix (GCN) | 61.8 |",
  "src_docs": [
   "2007.02133",
   "1801.10247",
   "2110.11464",
   "1809.05343",
   "1706.02216",
   "1909.11715",
   "1907.10903"
  ],
  "updated_answer": [
   [
    "2007.02133",
    "GCNII",
    "88.49%",
    88.49
   ],
   [
    "1907.10903",
    "IncepGCN+DropEdge",
    "88.2%",
    88.2
   ],
   [
    "2110.11464",
    "FDGATII",
    "87.7867%",
    87.7867
   ],
   [
    "1809.05343",
    "ASGCN",
    "87.44Â±0.0034%",
    87.44
   ],
   [
    "1801.10247",
    "FastGCN",
    "85.00%",
    85.0
   ],
   [
    "1706.02216",
    "GraphSAGE",
    "82.20%",
    82.2
   ],
   [
    "1909.11715",
    "GraphMix (GCN)",
    "61.8%",
    61.8
   ]
  ],
  "meta_info": {
   "datasets": "Cora Full-supervised",
   "datasets_short": "Cora",
   "task": "Node Classification",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2007.02133",
    "GCNII",
    "88.49%",
    88.49
   ],
   [
    "1907.10903",
    "IncepGCN+DropEdge",
    "88.2%",
    88.2
   ],
   [
    "2110.11464",
    "FDGATII",
    "87.7867%",
    87.7867
   ],
   [
    "1809.05343",
    "ASGCN",
    "87.44Â±0.0034%",
    87.44
   ],
   [
    "1801.10247",
    "FastGCN",
    "85.00%",
    85.0
   ],
   [
    "1706.02216",
    "GraphSAGE",
    "82.20%",
    82.2
   ],
   [
    "1909.11715",
    "GraphMix (GCN)",
    "61.8%",
    61.8
   ]
  ]
 },
 "./longdocdata/docs/2539.json": {
  "question": "List the performance scores of various methods on the Pubmed (Pubmed) dataset on the Link Prediction task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| Walkpooling | 98.7 |\n| VGNAE | 97.6 |\n| GNAE | 97.5 |\n| ARGE | 97.1 |\n| sGraphite-VAE | 96.3 |\n| S-VGAE | 96.0 |\n| Node Feature Agg + Similarity Metric | 94.2 |\n| Graph InfoClust (GIC) | 93.5 |",
  "src_docs": [
   "2009.06946",
   "1910.02548",
   "1802.04407",
   "2110.04375",
   "1804.00891",
   "1905.08509",
   "2108.08046"
  ],
  "updated_answer": [
   [
    "2110.04375",
    "Walkpooling",
    "98.7%",
    98.7
   ],
   [
    "2108.08046",
    "VGNAE",
    "97.6%",
    97.6
   ],
   [
    "2108.08046",
    "GNAE",
    "97.5%",
    97.5
   ],
   [
    "1802.04407",
    "ARGE",
    "97.1%",
    97.1
   ],
   [
    "1804.00891",
    "S-VGAE",
    "96.0%",
    96.0
   ],
   [
    "1905.08509",
    "sGraphite-VAE",
    "96.3%",
    96.3
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "94.2%",
    94.2
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "93.5%",
    93.5
   ]
  ],
  "meta_info": {
   "datasets": "Pubmed",
   "datasets_short": "Pubmed",
   "task": "Link Prediction",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "2110.04375",
    "Walkpooling",
    "98.7%",
    98.7
   ],
   [
    "2108.08046",
    "VGNAE",
    "97.6%",
    97.6
   ],
   [
    "1802.04407",
    "ARGE",
    "97.1%",
    97.1
   ],
   [
    "1905.08509",
    "sGraphite-VAE",
    "96.3%",
    96.3
   ],
   [
    "1804.00891",
    "S-VGAE",
    "96.0%",
    96.0
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "94.2%",
    94.2
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "93.5%",
    93.5
   ]
  ]
 },
 "./longdocdata/docs/2541.json": {
  "question": "List the performance scores of various methods on the Pubmed (Pubmed) dataset on the Link Prediction task using metric AUC.",
  "answer": "| Method | AUC |\n| --- | --- |\n| Walkpooling | 98.7 |\n| VGNAE | 97.6 |\n| GNAE | 97.5 |\n| ARGE | 96.8 |\n| S-VGAE | 96.0 |\n| sGraphite-VAE | 94.8 |\n| Node Feature Agg + Similarity Metric | 94.5 |\n| Graph InfoClust (GIC) | 93.7 |",
  "src_docs": [
   "2009.06946",
   "1910.02548",
   "1802.04407",
   "2110.04375",
   "1804.00891",
   "1905.08509",
   "2108.08046"
  ],
  "updated_answer": [
   [
    "2110.04375",
    "Walkpooling",
    "98.7%",
    98.7
   ],
   [
    "2108.08046",
    "VGNAE",
    "97.6%",
    97.6
   ],
   [
    "2108.08046",
    "GNAE",
    "97.5%",
    97.5
   ],
   [
    "1802.04407",
    "ARGE",
    "96.8%",
    96.8
   ],
   [
    "1804.00891",
    "S-VGAE",
    "96.0%",
    96.0
   ],
   [
    "1905.08509",
    "sGraphite-VAE",
    "94.8%",
    94.8
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "94.5%",
    94.5
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "93.7%",
    93.7
   ]
  ],
  "meta_info": {
   "datasets": "Pubmed",
   "datasets_short": "Pubmed",
   "task": "Link Prediction",
   "metric": "AUC"
  },
  "updated_answer2": [
   [
    "2110.04375",
    "Walkpooling",
    "98.7%",
    98.7
   ],
   [
    "2108.08046",
    "VGNAE",
    "97.6%",
    97.6
   ],
   [
    "1802.04407",
    "ARGE",
    "96.8%",
    96.8
   ],
   [
    "1804.00891",
    "S-VGAE",
    "96.0%",
    96.0
   ],
   [
    "1905.08509",
    "sGraphite-VAE",
    "94.8%",
    94.8
   ],
   [
    "1910.02548",
    "Node Feature Agg + Similarity Metric",
    "94.5%",
    94.5
   ],
   [
    "2009.06946",
    "Graph InfoClust (GIC)",
    "93.7%",
    93.7
   ]
  ]
 },
 "./longdocdata/docs/2586.json": {
  "question": "List the performance scores of various methods on the JIGSAWS (JIGSAWS) dataset on the Action Segmentation task using metric Edit Distance.",
  "answer": "| Method | Edit Distance |\n| --- | --- |\n| MRG-Net | 89.3 |\n| RL+Tree | 88.53 |\n| RL (full) | 87.96 |\n| TricorNet | 86.8 |\n| SDL+SC-CRF | 86.21 |\n| TCN | 83.1 |\n| ST-CNN+Seg | 66.56 |",
  "src_docs": [
   "1608.08242",
   "2011.01619",
   "2002.08718",
   "1806.08089",
   "1801.09571",
   "1705.07818",
   "1602.02995"
  ],
  "updated_answer": [
   [
    "2011.01619",
    "MRG-Net",
    "89.3",
    89.3
   ],
   [
    "2002.08718",
    "RL+Tree",
    "88.53",
    88.53
   ],
   [
    "1806.08089",
    "RL (full)",
    "87.96",
    87.96
   ],
   [
    "1705.07818",
    "TricorNet",
    "86.8",
    86.8
   ],
   [
    "1801.09571",
    "SDL+SC-CRF",
    "86.21",
    86.21
   ],
   [
    "1608.08242",
    "TCN",
    "83.1",
    83.1
   ],
   [
    "1602.02995",
    "ST-CNN+Seg",
    "66.56",
    66.56
   ]
  ],
  "meta_info": {
   "datasets": "JIGSAWS",
   "datasets_short": "JIGSAWS",
   "task": "Action Segmentation",
   "metric": "Edit Distance"
  },
  "updated_answer2": [
   [
    "2011.01619",
    "MRG-Net",
    "89.3",
    89.3
   ],
   [
    "2002.08718",
    "RL+Tree",
    "88.53",
    88.53
   ],
   [
    "1806.08089",
    "RL (full)",
    "87.96",
    87.96
   ],
   [
    "1705.07818",
    "TricorNet",
    "86.8",
    86.8
   ],
   [
    "1801.09571",
    "SDL+SC-CRF",
    "86.21",
    86.21
   ],
   [
    "1608.08242",
    "TCN",
    "83.1",
    83.1
   ],
   [
    "1602.02995",
    "ST-CNN+Seg",
    "66.56",
    66.56
   ]
  ]
 },
 "./longdocdata/docs/2622.json": {
  "question": "List the performance scores of various methods on the ACE 2004 (ACE 2004) dataset on the Nested Mention Recognition task using metric F1.",
  "answer": "| Method | F1 |\n| --- | --- |\n| BoningKnife | 86.41 |\n| BERT-MRC | 85.98 |\n| Second-best learning and decoding | 85.82 |\n| seq2seq+BERT+Flair | 84.4 |\n| MGNER | 79.5 |\n| Neural segmental hypergraphs | 75.1 |\n| Neural transition-based model | 73.1 |",
  "src_docs": [
   "1910.11476",
   "1909.02250",
   "1908.06926",
   "1810.01817",
   "1810.01808",
   "1906.08449",
   "2107.09429"
  ],
  "updated_answer": [
   [
    "2107.09429",
    "BoningKnife",
    "86.41",
    86.41
   ],
   [
    "1910.11476",
    "BERT-MRC",
    "85.98",
    85.98
   ],
   [
    "1909.02250",
    "Second-best learning and decoding",
    "85.82",
    85.82
   ],
   [
    "1908.06926",
    "seq2seq+BERT+Flair",
    "84.40",
    84.4
   ],
   [
    "1906.08449",
    "MGNER",
    "79.5",
    79.5
   ],
   [
    "1810.01817",
    "Neural segmental hypergraphs",
    "75.1",
    75.1
   ],
   [
    "1810.01808",
    "Neural transition-based model",
    "73.1",
    73.1
   ]
  ],
  "meta_info": {
   "datasets": "ACE 2004",
   "datasets_short": "ACE 2004",
   "task": "Nested Mention Recognition",
   "metric": "F1"
  },
  "updated_answer2": [
   [
    "2107.09429",
    "BoningKnife",
    "86.41",
    86.41
   ],
   [
    "1910.11476",
    "BERT-MRC",
    "85.98",
    85.98
   ],
   [
    "1909.02250",
    "Second-best learning and decoding",
    "85.82",
    85.82
   ],
   [
    "1908.06926",
    "seq2seq+BERT+Flair",
    "84.40",
    84.4
   ],
   [
    "1906.08449",
    "MGNER",
    "79.5",
    79.5
   ],
   [
    "1810.01817",
    "Neural segmental hypergraphs",
    "75.1",
    75.1
   ],
   [
    "1810.01808",
    "Neural transition-based model",
    "73.1",
    73.1
   ]
  ]
 },
 "./longdocdata/docs/2638.json": {
  "question": "List the performance scores of various methods on the ACE 2005 (ACE 2005) dataset on the Relation Extraction task using metric RE+ Micro F1.",
  "answer": "| Method | RE+ Micro F1 |\n| --- | --- |\n| PL-Marker | 71.1 |\n| PFN | 66.8 |\n| Table-Sequence | 64.3 |\n| TriMF | 62.77 |\n| TablERT | 62.4 |\n| Multi-turn QA | 60.2 |\n| SPTree | 55.6 |",
  "src_docs": [
   "2010.03851",
   "2101.10213",
   "1905.05529",
   "1601.00770",
   "2010.07522",
   "2108.12202",
   "2109.06067"
  ],
  "updated_answer": [
   [
    "2109.06067",
    "PL-Marker",
    "71.1",
    71.1
   ],
   [
    "2010.03851",
    "Table-Sequence",
    "64.3",
    64.3
   ],
   [
    "2101.10213",
    "TriMF",
    "62.77",
    62.77
   ],
   [
    "2010.07522",
    "TablERT",
    "62.4",
    62.4
   ],
   [
    "2108.12202",
    "PFN",
    "66.8",
    66.8
   ],
   [
    "1905.05529",
    "Multi-turn QA",
    "60.2",
    60.2
   ],
   [
    "1601.00770",
    "SPTree",
    "55.6",
    55.6
   ]
  ],
  "meta_info": {
   "datasets": "ACE 2005",
   "datasets_short": "ACE 2005",
   "task": "Relation Extraction",
   "metric": "RE+ Micro F1"
  },
  "updated_answer2": [
   [
    "2109.06067",
    "PL-Marker",
    "71.1",
    71.1
   ],
   [
    "2108.12202",
    "PFN",
    "66.8",
    66.8
   ],
   [
    "2010.03851",
    "Table-Sequence",
    "64.3",
    64.3
   ],
   [
    "2101.10213",
    "TriMF",
    "62.77",
    62.77
   ],
   [
    "2010.07522",
    "TablERT",
    "62.4",
    62.4
   ],
   [
    "1905.05529",
    "Multi-turn QA",
    "60.2",
    60.2
   ],
   [
    "1601.00770",
    "SPTree",
    "55.6",
    55.6
   ]
  ]
 },
 "./longdocdata/docs/2699.json": {
  "question": "List the performance scores of various methods on the Natural Questions (Natural Questions) dataset on the Passage Retrieval task using metric Precision@100.",
  "answer": "| Method | Precision@100 |\n| --- | --- |\n| DPR-PAQ | 89.22 |\n| SPAR | 88.8 |\n| RocketQA | 88.5 |\n| DPR+ELECTRA-large-extreader-reranker | 88.25 |\n| DPR+RoBERTa-base-crossencoder-reranker | 88.03 |\n| ANCE | 87.5 |\n| DPR | 86.0 |\n| BM25+RM3 | 79.6 |",
  "src_docs": [
   "2004.04906",
   "2007.00808",
   "2109.03502",
   "2009.08553",
   "2010.08191",
   "2110.06918",
   "2107.13602"
  ],
  "updated_answer": [
   [
    "2107.13602",
    "DPR-PAQ",
    "89.22",
    89.22
   ],
   [
    "2110.06918",
    "SPAR",
    "88.8",
    88.8
   ],
   [
    "2010.08191",
    "RocketQA",
    "88.5",
    88.5
   ],
   [
    "2109.03502",
    "DPR+ELECTRA-large-extreader-reranker",
    "88.25",
    88.25
   ],
   [
    "2109.03502",
    "DPR+RoBERTa-base-crossencoder-reranker",
    "88.03",
    88.03
   ],
   [
    "2007.00808",
    "ANCE",
    "87.5",
    87.5
   ],
   [
    "2004.04906",
    "DPR",
    "86",
    86.0
   ],
   [
    "2009.08553",
    "BM25+RM3",
    "79.6",
    79.6
   ]
  ],
  "meta_info": {
   "datasets": "Natural Questions",
   "datasets_short": "Natural Questions",
   "task": "Passage Retrieval",
   "metric": "Precision@100"
  },
  "updated_answer2": [
   [
    "2107.13602",
    "DPR-PAQ",
    "89.22",
    89.22
   ],
   [
    "2110.06918",
    "SPAR",
    "88.8",
    88.8
   ],
   [
    "2010.08191",
    "RocketQA",
    "88.5",
    88.5
   ],
   [
    "2109.03502",
    "DPR+ELECTRA-large-extreader-reranker",
    "88.25",
    88.25
   ],
   [
    "2007.00808",
    "ANCE",
    "87.5",
    87.5
   ],
   [
    "2004.04906",
    "DPR",
    "86",
    86.0
   ],
   [
    "2009.08553",
    "BM25+RM3",
    "79.6",
    79.6
   ]
  ]
 },
 "./longdocdata/docs/2720.json": {
  "question": "List the performance scores of various methods on the BC5CDR-chemical (BC5CDR) dataset on the Named Entity Recognition (NER) task using metric F1.",
  "answer": "| Method | F1 |\n| --- | --- |\n| Spark NLP | 94.88 |\n| SciFive-Large | 94.76 |\n| HGN | 94.59 |\n| BioLinkBERT (large) | 94.04 |\n| NCBI_BERT(base) (P) | 93.5 |\n| KeBioLM | 93.3 |\n| BioMegatron | 92.9 |",
  "src_docs": [
   "2011.06315",
   "1906.05474",
   "2104.10344",
   "2106.03598",
   "2010.06060",
   "2205.07177",
   "2203.15827"
  ],
  "updated_answer": [
   [
    "2011.06315",
    "Spark NLP",
    "94.88",
    94.88
   ],
   [
    "2106.03598",
    "SciFive-Large",
    "94.76",
    94.76
   ],
   [
    "2205.07177",
    "HGN",
    "94.59",
    94.59
   ],
   [
    "2203.15827",
    "BioLinkBERT (large)",
    "94.04",
    94.04
   ],
   [
    "1906.05474",
    "NCBI_BERT(base) (P)",
    "93.5",
    93.5
   ],
   [
    "2104.10344",
    "KeBioLM",
    "93.3",
    93.3
   ],
   [
    "2010.06060",
    "BioMegatron",
    "92.9",
    92.9
   ]
  ],
  "meta_info": {
   "datasets": "BC5CDR-chemical",
   "datasets_short": "BC5CDR",
   "task": "Named Entity Recognition (NER)",
   "metric": "F1"
  },
  "updated_answer2": [
   [
    "2011.06315",
    "Spark NLP",
    "94.88",
    94.88
   ],
   [
    "2106.03598",
    "SciFive-Large",
    "94.76",
    94.76
   ],
   [
    "2205.07177",
    "HGN",
    "94.59",
    94.59
   ],
   [
    "2203.15827",
    "BioLinkBERT (large)",
    "94.04",
    94.04
   ],
   [
    "1906.05474",
    "NCBI_BERT(base) (P)",
    "93.5",
    93.5
   ],
   [
    "2104.10344",
    "KeBioLM",
    "93.3",
    93.3
   ],
   [
    "2010.06060",
    "BioMegatron",
    "92.9",
    92.9
   ]
  ]
 },
 "./longdocdata/docs/2733.json": {
  "question": "List the performance scores of various methods on the SciERC (SciERC) dataset on the Joint Entity and Relation Extraction task using metric Cross Sentence.",
  "answer": "| Method | Cross Sentence |\n| --- | --- |\n| PL-Marker | 0 |\n| SpERT (with overlap) | 0 |\n| Ours: cross-sentence | 0 |\n| DyGIE++ | 0 |\n| DyGIE | 0 |\n| SciIE | 0 |\n| SpERT | 0 |\n| PFN | 0 |",
  "src_docs": [
   "2010.12812",
   "1904.03296",
   "2108.12202",
   "2109.06067",
   "1808.09602",
   "1909.07755",
   "1909.03546"
  ],
  "updated_answer": [
   [
    "2109.06067",
    "PL-Marker",
    "Yes",
    0
   ],
   [
    "1909.07755",
    "SpERT (with overlap)",
    "No",
    0
   ],
   [
    "2010.12812",
    "Ours: cross-sentence",
    "Yes",
    0
   ],
   [
    "1909.03546",
    "DyGIE++",
    "Yes",
    0
   ],
   [
    "1904.03296",
    "DyGIE",
    "Yes",
    0
   ],
   [
    "1808.09602",
    "SciIE",
    "No",
    0
   ],
   [
    "1909.07755",
    "SpERT",
    "No",
    0
   ],
   [
    "2108.12202",
    "PFN",
    "No",
    0
   ]
  ],
  "meta_info": {
   "datasets": "SciERC",
   "datasets_short": "SciERC",
   "task": "Joint Entity and Relation Extraction",
   "metric": "Cross Sentence"
  },
  "updated_answer2": [
   [
    "2109.06067",
    "PL-Marker",
    "Yes",
    0
   ],
   [
    "1909.07755",
    "SpERT (with overlap)",
    "No",
    0
   ],
   [
    "1909.07755",
    "SpERT",
    "No",
    0
   ],
   [
    "2010.12812",
    "Ours: cross-sentence",
    "Yes",
    0
   ],
   [
    "1909.03546",
    "DyGIE++",
    "Yes",
    0
   ],
   [
    "1904.03296",
    "DyGIE",
    "Yes",
    0
   ],
   [
    "1808.09602",
    "SciIE",
    "No",
    0
   ],
   [
    "2108.12202",
    "PFN",
    "No",
    0
   ]
  ]
 },
 "./longdocdata/docs/2734.json": {
  "question": "List the performance scores of various methods on the SciERC (SciERC) dataset on the Joint Entity and Relation Extraction task using metric Entity F1.",
  "answer": "| Method | Entity F1 |\n| --- | --- |\n| SpERT | 70.33 |\n| SpERT (with overlap) | 70.3 |\n| PL-Marker | 69.9 |\n| Ours: cross-sentence | 68.9 |\n| DyGIE++ | 67.5 |\n| PFN | 66.8 |\n| DyGIE | 65.2 |\n| SciIE | 64.2 |",
  "src_docs": [
   "2010.12812",
   "1904.03296",
   "2108.12202",
   "2109.06067",
   "1808.09602",
   "1909.07755",
   "1909.03546"
  ],
  "updated_answer": [
   [
    "2109.06067",
    "PL-Marker",
    "69.9",
    69.9
   ],
   [
    "1909.07755",
    "SpERT (with overlap)",
    "70.3",
    70.3
   ],
   [
    "2010.12812",
    "Ours: cross-sentence",
    "68.9",
    68.9
   ],
   [
    "1909.03546",
    "DyGIE++",
    "67.50",
    67.5
   ],
   [
    "1904.03296",
    "DyGIE",
    "65.2",
    65.2
   ],
   [
    "1808.09602",
    "SciIE",
    "64.20",
    64.2
   ],
   [
    "1909.07755",
    "SpERT",
    "70.33",
    70.33
   ],
   [
    "2108.12202",
    "PFN",
    "66.8",
    66.8
   ]
  ],
  "meta_info": {
   "datasets": "SciERC",
   "datasets_short": "SciERC",
   "task": "Joint Entity and Relation Extraction",
   "metric": "Entity F1"
  },
  "updated_answer2": [
   [
    "1909.07755",
    "SpERT",
    "70.33",
    70.33
   ],
   [
    "2109.06067",
    "PL-Marker",
    "69.9",
    69.9
   ],
   [
    "2010.12812",
    "Ours: cross-sentence",
    "68.9",
    68.9
   ],
   [
    "1909.03546",
    "DyGIE++",
    "67.50",
    67.5
   ],
   [
    "2108.12202",
    "PFN",
    "66.8",
    66.8
   ],
   [
    "1904.03296",
    "DyGIE",
    "65.2",
    65.2
   ],
   [
    "1808.09602",
    "SciIE",
    "64.20",
    64.2
   ]
  ]
 },
 "./longdocdata/docs/2757.json": {
  "question": "List the performance scores of various methods on the OCHuman (OCHuman) dataset on the Multi-Person Pose Estimation task using metric AP50.",
  "answer": "| Method | AP50 |\n| --- | --- |\n| MIPNet (gt-bb) | 89.7 |\n| TransPose-H | 82.7 |\n| HRFormer-B | 81.4 |\n| SPM | 67.5 |\n| CrowdPose | 40.8 |\n| SimplePose | 37.4 |\n| Mask R-CNN | 33.2 |",
  "src_docs": [
   "1703.06870",
   "2110.09408",
   "1812.00324",
   "2101.11223",
   "1804.06208",
   "1908.09220",
   "2012.14214"
  ],
  "updated_answer": [
   [
    "2101.11223",
    "MIPNet (gt-bb)",
    "89.7",
    89.7
   ],
   [
    "2012.14214",
    "TransPose-H",
    "82.7",
    82.7
   ],
   [
    "2110.09408",
    "HRFormer-B",
    "81.4",
    81.4
   ],
   [
    "1908.09220",
    "SPM",
    "67.5",
    67.5
   ],
   [
    "1812.00324",
    "CrowdPose",
    "40.8",
    40.8
   ],
   [
    "1804.06208",
    "SimplePose",
    "37.4",
    37.4
   ],
   [
    "1703.06870",
    "Mask R-CNN",
    "33.2",
    33.2
   ]
  ],
  "meta_info": {
   "datasets": "OCHuman",
   "datasets_short": "OCHuman",
   "task": "Multi-Person Pose Estimation",
   "metric": "AP50"
  },
  "updated_answer2": [
   [
    "2101.11223",
    "MIPNet (gt-bb)",
    "89.7",
    89.7
   ],
   [
    "2012.14214",
    "TransPose-H",
    "82.7",
    82.7
   ],
   [
    "2110.09408",
    "HRFormer-B",
    "81.4",
    81.4
   ],
   [
    "1908.09220",
    "SPM",
    "67.5",
    67.5
   ],
   [
    "1812.00324",
    "CrowdPose",
    "40.8",
    40.8
   ],
   [
    "1804.06208",
    "SimplePose",
    "37.4",
    37.4
   ],
   [
    "1703.06870",
    "Mask R-CNN",
    "33.2",
    33.2
   ]
  ]
 },
 "./longdocdata/docs/2759.json": {
  "question": "List the performance scores of various methods on the OCHuman (OCHuman) dataset on the Multi-Person Pose Estimation task using metric AP75.",
  "answer": "| Method | AP75 |\n| --- | --- |\n| MIPNet (gt-bb) | 80.1 |\n| TransPose-H | 67.1 |\n| HRFormer-B | 67.1 |\n| SPM | 53.2 |\n| CrowdPose | 29.9 |\n| SimplePose | 26.8 |\n| Mask R-CNN | 24.5 |",
  "src_docs": [
   "1703.06870",
   "2110.09408",
   "1812.00324",
   "2101.11223",
   "1804.06208",
   "1908.09220",
   "2012.14214"
  ],
  "updated_answer": [
   [
    "2101.11223",
    "MIPNet (gt-bb)",
    "80.1",
    80.1
   ],
   [
    "2012.14214",
    "TransPose-H",
    "67.1",
    67.1
   ],
   [
    "2110.09408",
    "HRFormer-B",
    "67.1",
    67.1
   ],
   [
    "1908.09220",
    "SPM",
    "53.2",
    53.2
   ],
   [
    "1812.00324",
    "CrowdPose",
    "29.9",
    29.9
   ],
   [
    "1804.06208",
    "SimplePose",
    "26.8",
    26.8
   ],
   [
    "1703.06870",
    "Mask R-CNN",
    "24.5",
    24.5
   ]
  ],
  "meta_info": {
   "datasets": "OCHuman",
   "datasets_short": "OCHuman",
   "task": "Multi-Person Pose Estimation",
   "metric": "AP75"
  },
  "updated_answer2": [
   [
    "2101.11223",
    "MIPNet (gt-bb)",
    "80.1",
    80.1
   ],
   [
    "2012.14214",
    "TransPose-H",
    "67.1",
    67.1
   ],
   [
    "2110.09408",
    "HRFormer-B",
    "67.1",
    67.1
   ],
   [
    "1908.09220",
    "SPM",
    "53.2",
    53.2
   ],
   [
    "1812.00324",
    "CrowdPose",
    "29.9",
    29.9
   ],
   [
    "1804.06208",
    "SimplePose",
    "26.8",
    26.8
   ],
   [
    "1703.06870",
    "Mask R-CNN",
    "24.5",
    24.5
   ]
  ]
 },
 "./longdocdata/docs/2763.json": {
  "question": "List the performance scores of various methods on the OCHuman (OCHuman) dataset on the Pose Estimation task using metric Test AP.",
  "answer": "| Method | Test AP |\n| --- | --- |\n| ViTPose (ViTAE-G, GT bounding boxes) | 93.3 |\n| MIPNet (HRNet-W48) | 42.5 |\n| HRNet-W48 | 37.2 |\n| HGG (AE+) | 36.0 |\n| ResNet-152 | 33.3 |\n| Associative Embedding+ | 32.8 |\n| RMPE | 30.7 |\n| ResNet-50 | 29.5 |\n| Associative Embedding | 29.5 |\n| Pose2Seg | 23.8 |",
  "src_docs": [
   "1612.00137",
   "2101.11223",
   "1803.10683",
   "2007.11864",
   "1611.05424",
   "1804.06208",
   "2204.12484"
  ],
  "updated_answer": [
   [
    "2204.12484",
    "ViTPose (ViTAE-G, GT bounding boxes)",
    "93.3",
    93.3
   ],
   [
    "2101.11223",
    "MIPNet (HRNet-W48)",
    "42.5",
    42.5
   ],
   [
    "2101.11223",
    "HRNet-W48",
    "37.2",
    37.2
   ],
   [
    "2007.11864",
    "HGG (AE+)",
    "36.0",
    36.0
   ],
   [
    "1804.06208",
    "ResNet-152",
    "33.3",
    33.3
   ],
   [
    "1611.05424",
    "Associative Embedding+",
    "32.8",
    32.8
   ],
   [
    "1612.00137",
    "RMPE",
    "30.7",
    30.7
   ],
   [
    "1804.06208",
    "ResNet-50",
    "29.5",
    29.5
   ],
   [
    "1611.05424",
    "Associative Embedding",
    "29.5",
    29.5
   ],
   [
    "1803.10683",
    "Pose2Seg",
    "23.8",
    23.8
   ]
  ],
  "meta_info": {
   "datasets": "OCHuman",
   "datasets_short": "OCHuman",
   "task": "Pose Estimation",
   "metric": "Test AP"
  },
  "updated_answer2": [
   [
    "2204.12484",
    "ViTPose (ViTAE-G, GT bounding boxes)",
    "93.3",
    93.3
   ],
   [
    "2101.11223",
    "MIPNet (HRNet-W48)",
    "42.5",
    42.5
   ],
   [
    "2007.11864",
    "HGG (AE+)",
    "36.0",
    36.0
   ],
   [
    "1804.06208",
    "ResNet-152",
    "33.3",
    33.3
   ],
   [
    "1611.05424",
    "Associative Embedding+",
    "32.8",
    32.8
   ],
   [
    "1612.00137",
    "RMPE",
    "30.7",
    30.7
   ],
   [
    "1803.10683",
    "Pose2Seg",
    "23.8",
    23.8
   ]
  ]
 },
 "./longdocdata/docs/2815.json": {
  "question": "List the performance scores of various methods on the CityPersons (CityPersons) dataset on the Pedestrian Detection task using metric Bare MR^-2.",
  "answer": "| Method | Bare MR^-2 |\n| --- | --- |\n| ACSP + EuroCity Persons | 4.9 |\n| ACSP | 5.6 |\n| Pedestron | 6.2 |\n| NOH-NMS | 6.6 |\n| OR-CNN | 6.7 |\n| CSP (with offset) + ResNet-50 | 7.3 |\n| RepLoss | 7.6 |\n| TLL+MRF | 9.2 |\n| TLL | 10.0 |",
  "src_docs": [
   "1807.08407",
   "2007.13376",
   "2003.08799",
   "1904.02948",
   "1711.07752",
   "1807.01438",
   "2002.09053"
  ],
  "updated_answer": [
   [
    "2003.08799",
    "Pedestron",
    "6.2",
    6.2
   ],
   [
    "2002.09053",
    "ACSP",
    "5.6",
    5.6
   ],
   [
    "2007.13376",
    "NOH-NMS",
    "6.6",
    6.6
   ],
   [
    "1904.02948",
    "CSP (with offset) + ResNet-50",
    "7.3",
    7.3
   ],
   [
    "1807.08407",
    "OR-CNN",
    "6.7",
    6.7
   ],
   [
    "1711.07752",
    "RepLoss",
    "7.6",
    7.6
   ],
   [
    "1807.01438",
    "TLL+MRF",
    "9.2",
    9.2
   ],
   [
    "1807.01438",
    "TLL",
    "10.0",
    10.0
   ],
   [
    "2002.09053",
    "ACSP + EuroCity Persons",
    "4.9",
    4.9
   ]
  ],
  "meta_info": {
   "datasets": "CityPersons",
   "datasets_short": "CityPersons",
   "task": "Pedestrian Detection",
   "metric": "Bare MR^-2"
  },
  "updated_answer2": [
   [
    "2002.09053",
    "ACSP + EuroCity Persons",
    "4.9",
    4.9
   ],
   [
    "2003.08799",
    "Pedestron",
    "6.2",
    6.2
   ],
   [
    "2007.13376",
    "NOH-NMS",
    "6.6",
    6.6
   ],
   [
    "1807.08407",
    "OR-CNN",
    "6.7",
    6.7
   ],
   [
    "1904.02948",
    "CSP (with offset) + ResNet-50",
    "7.3",
    7.3
   ],
   [
    "1711.07752",
    "RepLoss",
    "7.6",
    7.6
   ],
   [
    "1807.01438",
    "TLL+MRF",
    "9.2",
    9.2
   ]
  ]
 },
 "./longdocdata/docs/2816.json": {
  "question": "List the performance scores of various methods on the CityPersons (CityPersons) dataset on the Pedestrian Detection task using metric Partial MR^-2.",
  "answer": "| Method | Partial MR^-2 |\n| --- | --- |\n| Pedestron | 5.7 |\n| ACSP + EuroCity Persons | 6.9 |\n| ACSP | 8.7 |\n| CSP (with offset) + ResNet-50 | 10.4 |\n| NOH-NMS | 11.2 |\n| OR-CNN | 15.3 |\n| TLL+MRF | 15.9 |\n| RepLoss | 16.8 |\n| TLL | 17.2 |",
  "src_docs": [
   "1807.08407",
   "2007.13376",
   "2003.08799",
   "1904.02948",
   "1711.07752",
   "1807.01438",
   "2002.09053"
  ],
  "updated_answer": [
   [
    "2003.08799",
    "Pedestron",
    "5.7",
    5.7
   ],
   [
    "2002.09053",
    "ACSP",
    "8.7",
    8.7
   ],
   [
    "2007.13376",
    "NOH-NMS",
    "11.2",
    11.2
   ],
   [
    "1904.02948",
    "CSP (with offset) + ResNet-50",
    "10.4",
    10.4
   ],
   [
    "1807.08407",
    "OR-CNN",
    "15.3",
    15.3
   ],
   [
    "1711.07752",
    "RepLoss",
    "16.8",
    16.8
   ],
   [
    "1807.01438",
    "TLL+MRF",
    "15.9",
    15.9
   ],
   [
    "1807.01438",
    "TLL",
    "17.2",
    17.2
   ],
   [
    "2002.09053",
    "ACSP + EuroCity Persons",
    "6.9",
    6.9
   ]
  ],
  "meta_info": {
   "datasets": "CityPersons",
   "datasets_short": "CityPersons",
   "task": "Pedestrian Detection",
   "metric": "Partial MR^-2"
  },
  "updated_answer2": [
   [
    "2003.08799",
    "Pedestron",
    "5.7",
    5.7
   ],
   [
    "2002.09053",
    "ACSP + EuroCity Persons",
    "6.9",
    6.9
   ],
   [
    "1904.02948",
    "CSP (with offset) + ResNet-50",
    "10.4",
    10.4
   ],
   [
    "2007.13376",
    "NOH-NMS",
    "11.2",
    11.2
   ],
   [
    "1807.08407",
    "OR-CNN",
    "15.3",
    15.3
   ],
   [
    "1807.01438",
    "TLL+MRF",
    "15.9",
    15.9
   ],
   [
    "1711.07752",
    "RepLoss",
    "16.8",
    16.8
   ]
  ]
 },
 "./longdocdata/docs/2851.json": {
  "question": "List the performance scores of various methods on the VRD Phrase Detection (VRD) dataset on the Visual Relationship Detection task using metric R@50.",
  "answer": "| Method | R@50 |\n| --- | --- |\n| Yu et. al [[Yu et al.2017a]] | 26.32 |\n| BLOCK | 26.32 |\n| Liang et. al [[Liang, Lee, and Xing2017]] | 21.37 |\n| Dai et. al [[Dai, Zhang, and Lin2017]] | 19.93 |\n| Zhang et. al [[Hanwang Zhang2017]] | 19.42 |\n| Peyre et. al [[Peyre et al.2017]] | 17.9 |\n| Lu et. al [[Lu et al.2016]] | 16.17 |",
  "src_docs": [
   "1703.03054",
   "1702.08319",
   "1608.00187",
   "1902.00038",
   "1707.09423",
   "1704.03114",
   "1707.09472"
  ],
  "updated_answer": [
   [
    "1707.09423",
    "Yu et. al [[Yu et al.2017a]]",
    "26.32",
    26.32
   ],
   [
    "1902.00038",
    "BLOCK",
    "26.32",
    26.32
   ],
   [
    "1704.03114",
    "Dai et. al [[Dai, Zhang, and Lin2017]]",
    "19.93",
    19.93
   ],
   [
    "1703.03054",
    "Liang et. al [[Liang, Lee, and Xing2017]]",
    "21.37",
    21.37
   ],
   [
    "1702.08319",
    "Zhang et. al [[Hanwang Zhang2017]]",
    "19.42",
    19.42
   ],
   [
    "1707.09472",
    "Peyre et. al [[Peyre et al.2017]]",
    "17.9",
    17.9
   ],
   [
    "1608.00187",
    "Lu et. al [[Lu et al.2016]]",
    "16.17",
    16.17
   ]
  ],
  "meta_info": {
   "datasets": "VRD Phrase Detection",
   "datasets_short": "VRD",
   "task": "Visual Relationship Detection",
   "metric": "R@50"
  },
  "updated_answer2": [
   [
    "1707.09423",
    "Yu et. al [[Yu et al.2017a]]",
    "26.32",
    26.32
   ],
   [
    "1902.00038",
    "BLOCK",
    "26.32",
    26.32
   ],
   [
    "1703.03054",
    "Liang et. al [[Liang, Lee, and Xing2017]]",
    "21.37",
    21.37
   ],
   [
    "1704.03114",
    "Dai et. al [[Dai, Zhang, and Lin2017]]",
    "19.93",
    19.93
   ],
   [
    "1702.08319",
    "Zhang et. al [[Hanwang Zhang2017]]",
    "19.42",
    19.42
   ],
   [
    "1707.09472",
    "Peyre et. al [[Peyre et al.2017]]",
    "17.9",
    17.9
   ],
   [
    "1608.00187",
    "Lu et. al [[Lu et al.2016]]",
    "16.17",
    16.17
   ]
  ]
 },
 "./longdocdata/docs/2852.json": {
  "question": "List the performance scores of various methods on the VRD Phrase Detection (VRD) dataset on the Visual Relationship Detection task using metric R@100.",
  "answer": "| Method | R@100 |\n| --- | --- |\n| Yu et. al [[Yu et al.2017a]] | 29.43 |\n| BLOCK | 28.96 |\n| Dai et. al [[Dai, Zhang, and Lin2017]] | 23.45 |\n| Liang et. al [[Liang, Lee, and Xing2017]] | 22.6 |\n| Zhang et. al [[Hanwang Zhang2017]] | 22.42 |\n| Peyre et. al [[Peyre et al.2017]] | 19.5 |\n| Lu et. al [[Lu et al.2016]] | 17.03 |",
  "src_docs": [
   "1703.03054",
   "1702.08319",
   "1608.00187",
   "1902.00038",
   "1707.09423",
   "1704.03114",
   "1707.09472"
  ],
  "updated_answer": [
   [
    "1707.09423",
    "Yu et. al [[Yu et al.2017a]]",
    "29.43",
    29.43
   ],
   [
    "1902.00038",
    "BLOCK",
    "28.96",
    28.96
   ],
   [
    "1704.03114",
    "Dai et. al [[Dai, Zhang, and Lin2017]]",
    "23.45",
    23.45
   ],
   [
    "1703.03054",
    "Liang et. al [[Liang, Lee, and Xing2017]]",
    "22.60",
    22.6
   ],
   [
    "1702.08319",
    "Zhang et. al [[Hanwang Zhang2017]]",
    "22.42",
    22.42
   ],
   [
    "1707.09472",
    "Peyre et. al [[Peyre et al.2017]]",
    "19.5",
    19.5
   ],
   [
    "1608.00187",
    "Lu et. al [[Lu et al.2016]]",
    "17.03",
    17.03
   ]
  ],
  "meta_info": {
   "datasets": "VRD Phrase Detection",
   "datasets_short": "VRD",
   "task": "Visual Relationship Detection",
   "metric": "R@100"
  },
  "updated_answer2": [
   [
    "1707.09423",
    "Yu et. al [[Yu et al.2017a]]",
    "29.43",
    29.43
   ],
   [
    "1902.00038",
    "BLOCK",
    "28.96",
    28.96
   ],
   [
    "1704.03114",
    "Dai et. al [[Dai, Zhang, and Lin2017]]",
    "23.45",
    23.45
   ],
   [
    "1703.03054",
    "Liang et. al [[Liang, Lee, and Xing2017]]",
    "22.60",
    22.6
   ],
   [
    "1702.08319",
    "Zhang et. al [[Hanwang Zhang2017]]",
    "22.42",
    22.42
   ],
   [
    "1707.09472",
    "Peyre et. al [[Peyre et al.2017]]",
    "19.5",
    19.5
   ],
   [
    "1608.00187",
    "Lu et. al [[Lu et al.2016]]",
    "17.03",
    17.03
   ]
  ]
 },
 "./longdocdata/docs/2853.json": {
  "question": "List the performance scores of various methods on the VRD Relationship Detection (VRD) dataset on the Visual Relationship Detection task using metric R@50.",
  "answer": "| Method | R@50 |\n| --- | --- |\n| Yu et. al [[Yu et al.2017a]] | 22.68 |\n| BLOCK | 19.06 |\n| Liang et. al [[Liang, Lee, and Xing2017]] | 18.19 |\n| Dai et. al [[Dai, Zhang, and Lin2017]] | 17.73 |\n| Peyre et. al [[Peyre et al.2017]] | 15.8 |\n| Zhang et. al [[Hanwang Zhang2017]] | 14.07 |\n| Lu et. al [[Lu et al.2016]] | 13.86 |",
  "src_docs": [
   "1703.03054",
   "1702.08319",
   "1608.00187",
   "1902.00038",
   "1707.09423",
   "1704.03114",
   "1707.09472"
  ],
  "updated_answer": [
   [
    "1707.09423",
    "Yu et. al [[Yu et al.2017a]]",
    "22.68",
    22.68
   ],
   [
    "1902.00038",
    "BLOCK",
    "19.06",
    19.06
   ],
   [
    "1704.03114",
    "Dai et. al [[Dai, Zhang, and Lin2017]]",
    "17.73",
    17.73
   ],
   [
    "1703.03054",
    "Liang et. al [[Liang, Lee, and Xing2017]]",
    "18.19",
    18.19
   ],
   [
    "1707.09472",
    "Peyre et. al [[Peyre et al.2017]]",
    "15.8",
    15.8
   ],
   [
    "1702.08319",
    "Zhang et. al [[Hanwang Zhang2017]]",
    "14.07",
    14.07
   ],
   [
    "1608.00187",
    "Lu et. al [[Lu et al.2016]]",
    "13.86",
    13.86
   ]
  ],
  "meta_info": {
   "datasets": "VRD Relationship Detection",
   "datasets_short": "VRD",
   "task": "Visual Relationship Detection",
   "metric": "R@50"
  },
  "updated_answer2": [
   [
    "1707.09423",
    "Yu et. al [[Yu et al.2017a]]",
    "22.68",
    22.68
   ],
   [
    "1902.00038",
    "BLOCK",
    "19.06",
    19.06
   ],
   [
    "1703.03054",
    "Liang et. al [[Liang, Lee, and Xing2017]]",
    "18.19",
    18.19
   ],
   [
    "1704.03114",
    "Dai et. al [[Dai, Zhang, and Lin2017]]",
    "17.73",
    17.73
   ],
   [
    "1707.09472",
    "Peyre et. al [[Peyre et al.2017]]",
    "15.8",
    15.8
   ],
   [
    "1702.08319",
    "Zhang et. al [[Hanwang Zhang2017]]",
    "14.07",
    14.07
   ],
   [
    "1608.00187",
    "Lu et. al [[Lu et al.2016]]",
    "13.86",
    13.86
   ]
  ]
 },
 "./longdocdata/docs/525.json": {
  "question": "List the performance scores of various methods on the MS-COCO (COCO) dataset on the Generalized Zero-Shot Object Detection task using metric HM(Recall).",
  "answer": "| Method | HM(Recall) |\n| --- | --- |\n| ContrastZSD | 60.7 |\n| RRFS-ZSD | 60.2 |\n| SUZOD | 55.74 |\n| BLC | 53.92 |\n| PL | 36.76 |",
  "src_docs": [
   "2010.09425",
   "1811.08982",
   "2109.06062",
   "2010.04502",
   "2201.00103"
  ],
  "updated_answer": [
   [
    "2201.00103",
    "RRFS-ZSD",
    "60.20",
    60.2
   ],
   [
    "2010.09425",
    "SUZOD",
    "55.74",
    55.74
   ],
   [
    "2109.06062",
    "ContrastZSD",
    "60.70",
    60.7
   ],
   [
    "2010.04502",
    "BLC",
    "53.92",
    53.92
   ],
   [
    "1811.08982",
    "PL",
    "36.76",
    36.76
   ]
  ],
  "meta_info": {
   "datasets": "MS-COCO 65/15 split",
   "datasets_short": "COCO",
   "task": "Generalized Zero-Shot Object Detection",
   "metric": "HM(Recall)"
  },
  "updated_answer2": [
   [
    "2109.06062",
    "ContrastZSD",
    "60.70",
    60.7
   ],
   [
    "2201.00103",
    "RRFS-ZSD",
    "60.20",
    60.2
   ],
   [
    "2010.09425",
    "SUZOD",
    "55.74",
    55.74
   ],
   [
    "2010.04502",
    "BLC",
    "53.92",
    53.92
   ],
   [
    "1811.08982",
    "PL",
    "36.76",
    36.76
   ]
  ],
  "updated_answer3": [
   [
    "2109.06062",
    "ContrastZSD",
    "60.70",
    60.7
   ],
   [
    "2201.00103",
    "RRFS-ZSD",
    "60.20",
    60.2
   ],
   [
    "2010.09425",
    "SUZOD",
    "55.74",
    55.74
   ],
   [
    "2010.04502",
    "BLC",
    "53.92",
    53.92
   ],
   [
    "1811.08982",
    "PL",
    "36.76",
    36.76
   ]
  ],
  "distubed_docid": [
   "1903.00179",
   "2010.05006",
   "2008.01178",
   "2105.06421",
   "2111.05170",
   "2007.03347",
   "2005.09123",
   "2102.06583",
   "1411.4555",
   "2005.14165"
  ]
 },
 "./longdocdata/docs/542.json": {
  "question": "List the performance scores of various methods on the COCO (COCO) dataset on the Real-Time Object Detection task using metric box AP.",
  "answer": "| Method | box AP |\n| --- | --- |\n| PRB-FPN6(1280) | 56.9 |\n| PP-YOLOE+_X | 54.7 |\n| PP-YOLOE+_L(distillation) | 54.0 |\n| PP-YOLOE+_L | 52.9 |\n| PP-YOLOE+_M(distillation) | 51.0 |\n| YOLOv5-X | 50.4 |\n| PP-YOLOE+_M | 49.8 |\n| Mask R-CNN X-152-32x8d | 45.2 |\n| Faster RCNN-FPN+ | 42.0 |",
  "src_docs": [
   "1703.06870",
   "2005.12872",
   "2107.08430",
   "2203.16250",
   "2012.01724"
  ],
  "updated_answer": [
   [
    "2012.01724",
    "PRB-FPN6(1280)",
    "56.9",
    56.9
   ],
   [
    "2203.16250",
    "PP-YOLOE+_X",
    "54.7",
    54.7
   ],
   [
    "2203.16250",
    "PP-YOLOE+_L(distillation)",
    "54.0",
    54.0
   ],
   [
    "2203.16250",
    "PP-YOLOE+_L",
    "52.9",
    52.9
   ],
   [
    "2203.16250",
    "PP-YOLOE+_M(distillation)",
    "51.0",
    51.0
   ],
   [
    "2107.08430",
    "YOLOv5-X",
    "50.4",
    50.4
   ],
   [
    "2203.16250",
    "PP-YOLOE+_M",
    "49.8",
    49.8
   ],
   [
    "1703.06870",
    "Mask R-CNN X-152-32x8d",
    "45.2",
    45.2
   ],
   [
    "2005.12872",
    "Faster RCNN-FPN+",
    "42",
    42.0
   ]
  ],
  "meta_info": {
   "datasets": "COCO",
   "datasets_short": "COCO",
   "task": "Real-Time Object Detection",
   "metric": "box AP"
  },
  "updated_answer2": [
   [
    "2012.01724",
    "PRB-FPN6(1280)",
    "56.9",
    56.9
   ],
   [
    "2203.16250",
    "PP-YOLOE+_X",
    "54.7",
    54.7
   ],
   [
    "2107.08430",
    "YOLOv5-X",
    "50.4",
    50.4
   ],
   [
    "1703.06870",
    "Mask R-CNN X-152-32x8d",
    "45.2",
    45.2
   ],
   [
    "2005.12872",
    "Faster RCNN-FPN+",
    "42",
    42.0
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the box AP metric for the Real-Time Object Detection task on the COCO (COCO) dataset, and provide the top three metric results.",
    "answer": [
     "2012.01724",
     "PRB-FPN6(1280)",
     "56.9",
     56.9
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the box AP metric for the Real-Time Object Detection task on the COCO (COCO) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2012.01724",
      "PRB-FPN6(1280)",
      "56.9",
      56.9
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the box AP metric for the Real-Time Object Detection task on the COCO (COCO) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2012.01724",
      "PRB-FPN6(1280)",
      "56.9",
      56.9
     ],
     [
      "2203.16250",
      "PP-YOLOE+_X",
      "54.7",
      54.7
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2012.01724",
    "PRB-FPN6(1280)",
    "56.9",
    56.9
   ],
   [
    "2203.16250",
    "PP-YOLOE+_X",
    "54.7",
    54.7
   ],
   [
    "2107.08430",
    "YOLOv5-X",
    "50.4",
    50.4
   ],
   [
    "1703.06870",
    "Mask R-CNN X-152-32x8d",
    "45.2",
    45.2
   ],
   [
    "2005.12872",
    "Faster RCNN-FPN+",
    "42",
    42.0
   ]
  ],
  "distubed_docid": [
   "2012.13912",
   "2203.03089",
   "1810.02569",
   "2201.09206",
   "2205.07177",
   "2108.13576",
   "1411.5379",
   "2006.09242",
   "2111.15557",
   "2203.13250"
  ]
 },
 "./longdocdata/docs/1662.json": {
  "question": "List the performance scores of various methods on the CelebA-Test (CelebA-HQ) dataset on the Blind Face Restoration task using metric SSIM.",
  "answer": "| Method | SSIM |\n| --- | --- |\n| DeblurGANv2* | 0.6952 |\n| GFP-GAN | 0.6777 |\n| mGANprior | 0.6758 |\n| PSFRGAN | 0.6557 |\n| HiFaceGAN | 0.6195 |",
  "src_docs": [
   "1908.03826",
   "2009.08709",
   "1912.07116",
   "2005.05005",
   "2101.04061"
  ],
  "updated_answer": [
   [
    "2101.04061",
    "GFP-GAN",
    "0.6777",
    0.6777
   ],
   [
    "1908.03826",
    "DeblurGANv2*",
    "0.6952",
    0.6952
   ],
   [
    "2009.08709",
    "PSFRGAN",
    "0.6557",
    0.6557
   ],
   [
    "1912.07116",
    "mGANprior",
    "0.6758",
    0.6758
   ],
   [
    "2005.05005",
    "HiFaceGAN",
    "0.6195",
    0.6195
   ]
  ],
  "meta_info": {
   "datasets": "CelebA-Test",
   "datasets_short": "CelebA-HQ",
   "task": "Blind Face Restoration",
   "metric": "SSIM"
  },
  "updated_answer2": [
   [
    "1908.03826",
    "DeblurGANv2*",
    "0.6952",
    0.6952
   ],
   [
    "2101.04061",
    "GFP-GAN",
    "0.6777",
    0.6777
   ],
   [
    "1912.07116",
    "mGANprior",
    "0.6758",
    0.6758
   ],
   [
    "2009.08709",
    "PSFRGAN",
    "0.6557",
    0.6557
   ],
   [
    "2005.05005",
    "HiFaceGAN",
    "0.6195",
    0.6195
   ]
  ],
  "updated_answer3": [
   [
    "1908.03826",
    "DeblurGANv2*",
    "0.6952",
    0.6952
   ],
   [
    "2101.04061",
    "GFP-GAN",
    "0.6777",
    0.6777
   ],
   [
    "1912.07116",
    "mGANprior",
    "0.6758",
    0.6758
   ],
   [
    "2009.08709",
    "PSFRGAN",
    "0.6557",
    0.6557
   ],
   [
    "2005.05005",
    "HiFaceGAN",
    "0.6195",
    0.6195
   ]
  ]
 },
 "./longdocdata/docs/898.json": {
  "question": "List the performance scores of various methods on the HMDB51 (HMDB51) dataset on the Self-supervised Video Retrieval task using metric Top-1.",
  "answer": "| Method | Top-1 |\n| --- | --- |\n| ViCC (S3D; R+F) | 29.7 |\n| ViCC (R2+1D; R+F) | 28.3 |\n| CrissCross (R2+1D) | 26.4 |\n| ViCC (S3D; RGB) | 25.5 |\n| ViCC (R2+1D; RGB) | 25.3 |\n| TCLR (R3D-18) | 22.8 |\n| IIC (R3D) | 13.4 |\n| VCP (R3D) | 7.6 |",
  "src_docs": [
   "2101.07974",
   "2111.05329",
   "2106.10137",
   "2008.02531",
   "2001.00294"
  ],
  "updated_answer": [
   [
    "2106.10137",
    "ViCC (S3D; R+F)",
    "29.7",
    29.7
   ],
   [
    "2106.10137",
    "ViCC (R2+1D; R+F)",
    "28.3",
    28.3
   ],
   [
    "2111.05329",
    "CrissCross (R2+1D)",
    "26.4",
    26.4
   ],
   [
    "2106.10137",
    "ViCC (S3D; RGB)",
    "25.5",
    25.5
   ],
   [
    "2106.10137",
    "ViCC (R2+1D; RGB)",
    "25.3",
    25.3
   ],
   [
    "2101.07974",
    "TCLR (R3D-18)",
    "22.8",
    22.8
   ],
   [
    "2008.02531",
    "IIC (R3D)",
    "13.4",
    13.4
   ],
   [
    "2001.00294",
    "VCP (R3D)",
    "7.6",
    7.6
   ]
  ],
  "meta_info": {
   "datasets": "HMDB51",
   "datasets_short": "HMDB51",
   "task": "Self-supervised Video Retrieval",
   "metric": "Top-1"
  },
  "updated_answer2": [
   [
    "2106.10137",
    "ViCC (S3D; R+F)",
    "29.7",
    29.7
   ],
   [
    "2111.05329",
    "CrissCross (R2+1D)",
    "26.4",
    26.4
   ],
   [
    "2101.07974",
    "TCLR (R3D-18)",
    "22.8",
    22.8
   ],
   [
    "2008.02531",
    "IIC (R3D)",
    "13.4",
    13.4
   ],
   [
    "2001.00294",
    "VCP (R3D)",
    "7.6",
    7.6
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Top-1 metric for the Self-supervised Video Retrieval task on the HMDB51 (HMDB51) dataset, and provide the top three metric results.",
    "answer": [
     "2106.10137",
     "ViCC (S3D; R+F)",
     "29.7",
     29.7
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Top-1 metric for the Self-supervised Video Retrieval task on the HMDB51 (HMDB51) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2106.10137",
      "ViCC (S3D; R+F)",
      "29.7",
      29.7
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Top-1 metric for the Self-supervised Video Retrieval task on the HMDB51 (HMDB51) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2106.10137",
      "ViCC (S3D; R+F)",
      "29.7",
      29.7
     ],
     [
      "2111.05329",
      "CrissCross (R2+1D)",
      "26.4",
      26.4
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2106.10137",
    "ViCC (S3D; R+F)",
    "29.7",
    29.7
   ],
   [
    "2111.05329",
    "CrissCross (R2+1D)",
    "26.4",
    26.4
   ],
   [
    "2101.07974",
    "TCLR (R3D-18)",
    "22.8",
    22.8
   ],
   [
    "2008.02531",
    "IIC (R3D)",
    "13.4",
    13.4
   ],
   [
    "2001.00294",
    "VCP (R3D)",
    "7.6",
    7.6
   ]
  ],
  "distubed_docid": [
   "1707.06750",
   "2007.00808",
   "2202.00874",
   "1811.07490",
   "2111.14822",
   "2205.13543",
   "2010.00904",
   "2111.11133",
   "1411.5379",
   "1804.06300"
  ]
 },
 "./longdocdata/docs/1011.json": {
  "question": "List the performance scores of various methods on the MARS (MARS) dataset on the Person Re-Identification task using metric Rank-5.",
  "answer": "| Method | Rank-5 |\n| --- | --- |\n| FGReID | 97.0 |\n| VKD (ResVKD-50bam) | 96.8 |\n| mgh | 96.7 |\n| TKP | 93.7 |\n| TriNet | 91.36 |\n| TriNet (RK) | 90.76 |\n| LuNet | 89.7 |\n| LuNet (RK) | 88.74 |",
  "src_docs": [
   "1908.03885",
   "2104.14913",
   "1703.07737",
   "2011.13475",
   "2007.04174"
  ],
  "updated_answer": [
   [
    "2011.13475",
    "FGReID",
    "97.0",
    97.0
   ],
   [
    "2104.14913",
    "mgh",
    "96.7",
    96.7
   ],
   [
    "2007.04174",
    "VKD (ResVKD-50bam)",
    "96.8",
    96.8
   ],
   [
    "1703.07737",
    "TriNet (RK)",
    "90.76",
    90.76
   ],
   [
    "1703.07737",
    "LuNet (RK)",
    "88.74",
    88.74
   ],
   [
    "1908.03885",
    "TKP",
    "93.7",
    93.7
   ],
   [
    "1703.07737",
    "TriNet",
    "91.36",
    91.36
   ],
   [
    "1703.07737",
    "LuNet",
    "89.70",
    89.7
   ]
  ],
  "meta_info": {
   "datasets": "MARS",
   "datasets_short": "MARS",
   "task": "Person Re-Identification",
   "metric": "Rank-5"
  },
  "updated_answer2": [
   [
    "2011.13475",
    "FGReID",
    "97.0",
    97.0
   ],
   [
    "2007.04174",
    "VKD (ResVKD-50bam)",
    "96.8",
    96.8
   ],
   [
    "2104.14913",
    "mgh",
    "96.7",
    96.7
   ],
   [
    "1908.03885",
    "TKP",
    "93.7",
    93.7
   ],
   [
    "1703.07737",
    "TriNet",
    "91.36",
    91.36
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Rank-5 metric for the Person Re-Identification task on the MARS (MARS) dataset, and provide the top three metric results.",
    "answer": [
     "2011.13475",
     "FGReID",
     "97.0",
     97.0
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Rank-5 metric for the Person Re-Identification task on the MARS (MARS) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2011.13475",
      "FGReID",
      "97.0",
      97.0
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Rank-5 metric for the Person Re-Identification task on the MARS (MARS) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2011.13475",
      "FGReID",
      "97.0",
      97.0
     ],
     [
      "2007.04174",
      "VKD (ResVKD-50bam)",
      "96.8",
      96.8
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2011.13475",
    "FGReID",
    "97.0",
    97.0
   ],
   [
    "2007.04174",
    "VKD (ResVKD-50bam)",
    "96.8",
    96.8
   ],
   [
    "2104.14913",
    "mgh",
    "96.7",
    96.7
   ],
   [
    "1908.03885",
    "TKP",
    "93.7",
    93.7
   ],
   [
    "1703.07737",
    "TriNet",
    "91.36",
    91.36
   ]
  ],
  "distubed_docid": [
   "1911.08670",
   "2005.14165",
   "2002.05969",
   "1904.03116",
   "1612.00137",
   "1903.07414",
   "1806.07078",
   "2110.05069",
   "1912.06798",
   "2105.02465"
  ]
 },
 "./longdocdata/docs/1015.json": {
  "question": "List the performance scores of various methods on the iLIDS-VID (iLIDS-VID) dataset on the Person Re-Identification task using metric Rank-20.",
  "answer": "| Method | Rank-20 |\n| --- | --- |\n| FGReID | 100.0 |\n| AGRL | 99.5 |\n| TKP | 93.5 |\n| uPMnet | 92.5 |\n| UTAL | 83.8 |",
  "src_docs": [
   "1903.00535",
   "1908.03885",
   "1909.02240",
   "2011.13475",
   "2111.05170"
  ],
  "updated_answer": [
   [
    "2011.13475",
    "FGReID",
    "100",
    100.0
   ],
   [
    "1909.02240",
    "AGRL",
    "99.5",
    99.5
   ],
   [
    "2111.05170",
    "uPMnet",
    "92.5",
    92.5
   ],
   [
    "1908.03885",
    "TKP",
    "93.5",
    93.5
   ],
   [
    "1903.00535",
    "UTAL",
    "83.8",
    83.8
   ]
  ],
  "meta_info": {
   "datasets": "iLIDS-VID",
   "datasets_short": "iLIDS-VID",
   "task": "Person Re-Identification",
   "metric": "Rank-20"
  },
  "updated_answer2": [
   [
    "2011.13475",
    "FGReID",
    "100",
    100.0
   ],
   [
    "1909.02240",
    "AGRL",
    "99.5",
    99.5
   ],
   [
    "1908.03885",
    "TKP",
    "93.5",
    93.5
   ],
   [
    "2111.05170",
    "uPMnet",
    "92.5",
    92.5
   ],
   [
    "1903.00535",
    "UTAL",
    "83.8",
    83.8
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Rank-20 metric for the Person Re-Identification task on the iLIDS-VID (iLIDS-VID) dataset, and provide the top three metric results.",
    "answer": [
     "2011.13475",
     "FGReID",
     "100",
     100.0
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Rank-20 metric for the Person Re-Identification task on the iLIDS-VID (iLIDS-VID) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2011.13475",
      "FGReID",
      "100",
      100.0
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Rank-20 metric for the Person Re-Identification task on the iLIDS-VID (iLIDS-VID) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2011.13475",
      "FGReID",
      "100",
      100.0
     ],
     [
      "1909.02240",
      "AGRL",
      "99.5",
      99.5
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2011.13475",
    "FGReID",
    "100",
    100.0
   ],
   [
    "1909.02240",
    "AGRL",
    "99.5",
    99.5
   ],
   [
    "1908.03885",
    "TKP",
    "93.5",
    93.5
   ],
   [
    "2111.05170",
    "uPMnet",
    "92.5",
    92.5
   ],
   [
    "1903.00535",
    "UTAL",
    "83.8",
    83.8
   ]
  ],
  "distubed_docid": [
   "2204.02574",
   "2006.04535",
   "1806.03146",
   "2109.06062",
   "2104.02867",
   "1903.07227",
   "1606.09549",
   "2111.10050",
   "1801.08290",
   "2109.07592"
  ]
 },
 "./longdocdata/docs/2062.json": {
  "question": "List the performance scores of various methods on the ADE20K-Outdoor Labels-to-Photos (ADE20K) dataset on the Image-to-Image Translation task using metric mIoU.",
  "answer": "| Method | mIoU |\n| --- | --- |\n| OASIS | 40.4 |\n| SPADE | 30.8 |\n| pix2pixHD | 17.4 |\n| CRN | 16.5 |\n| SIMS | 13.1 |",
  "src_docs": [
   "2012.04781",
   "1707.09405",
   "1903.07291",
   "1711.11585",
   "1804.10992"
  ],
  "updated_answer": [
   [
    "2012.04781",
    "OASIS",
    "40.4",
    40.4
   ],
   [
    "1903.07291",
    "SPADE",
    "30.8",
    30.8
   ],
   [
    "1711.11585",
    "pix2pixHD",
    "17.4",
    17.4
   ],
   [
    "1707.09405",
    "CRN",
    "16.5",
    16.5
   ],
   [
    "1804.10992",
    "SIMS",
    "13.1",
    13.1
   ]
  ],
  "meta_info": {
   "datasets": "ADE20K-Outdoor Labels-to-Photos",
   "datasets_short": "ADE20K",
   "task": "Image-to-Image Translation",
   "metric": "mIoU"
  },
  "updated_answer2": [
   [
    "2012.04781",
    "OASIS",
    "40.4",
    40.4
   ],
   [
    "1903.07291",
    "SPADE",
    "30.8",
    30.8
   ],
   [
    "1711.11585",
    "pix2pixHD",
    "17.4",
    17.4
   ],
   [
    "1707.09405",
    "CRN",
    "16.5",
    16.5
   ],
   [
    "1804.10992",
    "SIMS",
    "13.1",
    13.1
   ]
  ],
  "updated_answer3": [
   [
    "2012.04781",
    "OASIS",
    "40.4",
    40.4
   ],
   [
    "1903.07291",
    "SPADE",
    "30.8",
    30.8
   ],
   [
    "1711.11585",
    "pix2pixHD",
    "17.4",
    17.4
   ],
   [
    "1707.09405",
    "CRN",
    "16.5",
    16.5
   ],
   [
    "1804.10992",
    "SIMS",
    "13.1",
    13.1
   ]
  ]
 },
 "./longdocdata/docs/1310.json": {
  "question": "List the performance scores of various methods on the SciTail (SciTail) dataset on the Natural Language Inference task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| CA-MTL | 96.8 |\n| MT-DNN | 94.1 |\n| Hierarchical BiLSTM Max Pooling | 86.0 |\n| RE2 | 86.0 |\n| CAFE | 83.3 |",
  "src_docs": [
   "1808.08762",
   "1901.11504",
   "1908.00300",
   "2009.09139",
   "1801.00102"
  ],
  "updated_answer": [
   [
    "2009.09139",
    "CA-MTL",
    "96.8",
    96.8
   ],
   [
    "1901.11504",
    "MT-DNN",
    "94.1",
    94.1
   ],
   [
    "1808.08762",
    "Hierarchical BiLSTM Max Pooling",
    "86.0",
    86.0
   ],
   [
    "1908.00300",
    "RE2",
    "86.0",
    86.0
   ],
   [
    "1801.00102",
    "CAFE",
    "83.3",
    83.3
   ]
  ],
  "meta_info": {
   "datasets": "SciTail",
   "datasets_short": "SciTail",
   "task": "Natural Language Inference",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2009.09139",
    "CA-MTL",
    "96.8",
    96.8
   ],
   [
    "1901.11504",
    "MT-DNN",
    "94.1",
    94.1
   ],
   [
    "1808.08762",
    "Hierarchical BiLSTM Max Pooling",
    "86.0",
    86.0
   ],
   [
    "1908.00300",
    "RE2",
    "86.0",
    86.0
   ],
   [
    "1801.00102",
    "CAFE",
    "83.3",
    83.3
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Accuracy metric for the Natural Language Inference task on the SciTail (SciTail) dataset, and provide the top three metric results.",
    "answer": [
     "2009.09139",
     "CA-MTL",
     "96.8",
     96.8
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Accuracy metric for the Natural Language Inference task on the SciTail (SciTail) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2009.09139",
      "CA-MTL",
      "96.8",
      96.8
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Accuracy metric for the Natural Language Inference task on the SciTail (SciTail) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2009.09139",
      "CA-MTL",
      "96.8",
      96.8
     ],
     [
      "1901.11504",
      "MT-DNN",
      "94.1",
      94.1
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2009.09139",
    "CA-MTL",
    "96.8",
    96.8
   ],
   [
    "1901.11504",
    "MT-DNN",
    "94.1",
    94.1
   ],
   [
    "1808.08762",
    "Hierarchical BiLSTM Max Pooling",
    "86.0",
    86.0
   ],
   [
    "1908.00300",
    "RE2",
    "86.0",
    86.0
   ],
   [
    "1801.00102",
    "CAFE",
    "83.3",
    83.3
   ]
  ],
  "distubed_docid": [
   "2203.09303",
   "2103.03483",
   "2011.08627",
   "1903.00138",
   "2009.00258",
   "2205.13543",
   "1805.08092",
   "1904.01324",
   "1911.08670",
   "2005.09007"
  ]
 },
 "./longdocdata/docs/1463.json": {
  "question": "List the performance scores of various methods on the OTB-2013 (OTB-2013) dataset on the Visual Object Tracking task using metric AUC.",
  "answer": "| Method | AUC |\n| --- | --- |\n| SE-SiamFC | 0.68 |\n| SA-Siam | 0.677 |\n| SiamVGG | 0.665 |\n| CFNet | 0.611 |\n| SiamFC-3s | 0.607 |",
  "src_docs": [
   "1902.02804",
   "1704.06036",
   "1802.08817",
   "2007.09115",
   "1606.09549"
  ],
  "updated_answer": [
   [
    "2007.09115",
    "SE-SiamFC",
    "0.68",
    0.68
   ],
   [
    "1802.08817",
    "SA-Siam",
    "0.677",
    0.677
   ],
   [
    "1902.02804",
    "SiamVGG",
    "0.665",
    0.665
   ],
   [
    "1704.06036",
    "CFNet",
    "0.611",
    0.611
   ],
   [
    "1606.09549",
    "SiamFC-3s",
    "0.607",
    0.607
   ]
  ],
  "meta_info": {
   "datasets": "OTB-2013",
   "datasets_short": "OTB-2013",
   "task": "Visual Object Tracking",
   "metric": "AUC"
  },
  "updated_answer2": [
   [
    "2007.09115",
    "SE-SiamFC",
    "0.68",
    0.68
   ],
   [
    "1802.08817",
    "SA-Siam",
    "0.677",
    0.677
   ],
   [
    "1902.02804",
    "SiamVGG",
    "0.665",
    0.665
   ],
   [
    "1704.06036",
    "CFNet",
    "0.611",
    0.611
   ],
   [
    "1606.09549",
    "SiamFC-3s",
    "0.607",
    0.607
   ]
  ],
  "updated_answer3": [
   [
    "2007.09115",
    "SE-SiamFC",
    "0.68",
    0.68
   ],
   [
    "1802.08817",
    "SA-Siam",
    "0.677",
    0.677
   ],
   [
    "1902.02804",
    "SiamVGG",
    "0.665",
    0.665
   ],
   [
    "1704.06036",
    "CFNet",
    "0.611",
    0.611
   ],
   [
    "1606.09549",
    "SiamFC-3s",
    "0.607",
    0.607
   ]
  ],
  "distubed_docid": [
   "2203.09581",
   "1909.11874",
   "1811.04210",
   "2010.04520",
   "2104.00556",
   "1805.08092",
   "2104.06064",
   "2111.11821",
   "2203.03089",
   "2002.11566"
  ]
 },
 "./longdocdata/docs/1633.json": {
  "question": "List the performance scores of various methods on the NUS-WIDE (NUS-WIDE) dataset on the Multi-label zero-shot learning task using metric mAP.",
  "answer": "| Method | mAP |\n| --- | --- |\n| ML-Decoder | 31.1 |\n| BiAM | 26.3 |\n| SDL | 25.9 |\n| ZSL_Generative_MLZSL | 25.7 |\n| fast0tag | 15.1 |",
  "src_docs": [
   "2101.11606",
   "2108.09301",
   "1605.09759",
   "2105.05926",
   "2111.12933"
  ],
  "updated_answer": [
   [
    "2111.12933",
    "ML-Decoder",
    "31.1",
    31.1
   ],
   [
    "2108.09301",
    "BiAM",
    "26.3",
    26.3
   ],
   [
    "2105.05926",
    "SDL",
    "25.9",
    25.9
   ],
   [
    "2101.11606",
    "ZSL_Generative_MLZSL",
    "25.7",
    25.7
   ],
   [
    "1605.09759",
    "fast0tag",
    "15.1",
    15.1
   ]
  ],
  "meta_info": {
   "datasets": "NUS-WIDE",
   "datasets_short": "NUS-WIDE",
   "task": "Multi-label zero-shot learning",
   "metric": "mAP"
  },
  "updated_answer2": [
   [
    "2111.12933",
    "ML-Decoder",
    "31.1",
    31.1
   ],
   [
    "2108.09301",
    "BiAM",
    "26.3",
    26.3
   ],
   [
    "2105.05926",
    "SDL",
    "25.9",
    25.9
   ],
   [
    "2101.11606",
    "ZSL_Generative_MLZSL",
    "25.7",
    25.7
   ],
   [
    "1605.09759",
    "fast0tag",
    "15.1",
    15.1
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the mAP metric for the Multi-label zero-shot learning task on the NUS-WIDE (NUS-WIDE) dataset, and provide the top three metric results.",
    "answer": [
     "2111.12933",
     "ML-Decoder",
     "31.1",
     31.1
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the mAP metric for the Multi-label zero-shot learning task on the NUS-WIDE (NUS-WIDE) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2111.12933",
      "ML-Decoder",
      "31.1",
      31.1
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the mAP metric for the Multi-label zero-shot learning task on the NUS-WIDE (NUS-WIDE) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2111.12933",
      "ML-Decoder",
      "31.1",
      31.1
     ],
     [
      "2108.09301",
      "BiAM",
      "26.3",
      26.3
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2111.12933",
    "ML-Decoder",
    "31.1",
    31.1
   ],
   [
    "2108.09301",
    "BiAM",
    "26.3",
    26.3
   ],
   [
    "2105.05926",
    "SDL",
    "25.9",
    25.9
   ],
   [
    "2101.11606",
    "ZSL_Generative_MLZSL",
    "25.7",
    25.7
   ],
   [
    "1605.09759",
    "fast0tag",
    "15.1",
    15.1
   ]
  ],
  "distubed_docid": [
   "1912.10211",
   "1709.00023",
   "2105.04443",
   "1703.07737",
   "1903.10663",
   "1911.07918",
   "2205.14871",
   "1904.01169",
   "2101.06184",
   "1808.08762"
  ]
 },
 "./longdocdata/docs/2071.json": {
  "question": "List the performance scores of various methods on the Human3.6M (Human3.6M) dataset on the Monocular 3D Human Pose Estimation task using metric PA-MPJPE.",
  "answer": "| Method | PA-MPJPE |\n| --- | --- |\n| HEMlets Pose (H36M+MPII) | 27.9 |\n| Spatio-Temporal Network (T=128) | 30.7 |\n| HR-Net+VPose+PoseAug | 39.1 |\n| Neural Body Fitting (NBF) | 59.9 |\n| SMPLify (dense) | 80.7 |",
  "src_docs": [
   "2105.02465",
   "1910.12032",
   "1701.02468",
   "2004.11822",
   "1808.05942"
  ],
  "updated_answer": [
   [
    "1910.12032",
    "HEMlets Pose (H36M+MPII)",
    "27.9",
    27.9
   ],
   [
    "2004.11822",
    "Spatio-Temporal Network (T=128)",
    "30.7",
    30.7
   ],
   [
    "2105.02465",
    "HR-Net+VPose+PoseAug",
    "39.1",
    39.1
   ],
   [
    "1808.05942",
    "Neural Body Fitting (NBF)",
    "59.9",
    59.9
   ],
   [
    "1701.02468",
    "SMPLify (dense)",
    "80.7",
    80.7
   ]
  ],
  "meta_info": {
   "datasets": "Human3.6M",
   "datasets_short": "Human3.6M",
   "task": "Monocular 3D Human Pose Estimation",
   "metric": "PA-MPJPE"
  },
  "updated_answer2": [
   [
    "1910.12032",
    "HEMlets Pose (H36M+MPII)",
    "27.9",
    27.9
   ],
   [
    "2004.11822",
    "Spatio-Temporal Network (T=128)",
    "30.7",
    30.7
   ],
   [
    "2105.02465",
    "HR-Net+VPose+PoseAug",
    "39.1",
    39.1
   ],
   [
    "1808.05942",
    "Neural Body Fitting (NBF)",
    "59.9",
    59.9
   ],
   [
    "1701.02468",
    "SMPLify (dense)",
    "80.7",
    80.7
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the PA-MPJPE metric for the Monocular 3D Human Pose Estimation task on the Human3.6M (Human3.6M) dataset, and provide the top three metric results.",
    "answer": [
     "1910.12032",
     "HEMlets Pose (H36M+MPII)",
     "27.9",
     27.9
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the PA-MPJPE metric for the Monocular 3D Human Pose Estimation task on the Human3.6M (Human3.6M) dataset, and provide the top two metric results.",
    "answer": [
     [
      "1910.12032",
      "HEMlets Pose (H36M+MPII)",
      "27.9",
      27.9
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the PA-MPJPE metric for the Monocular 3D Human Pose Estimation task on the Human3.6M (Human3.6M) dataset, and provide the best current metric value.",
    "answer": [
     [
      "1910.12032",
      "HEMlets Pose (H36M+MPII)",
      "27.9",
      27.9
     ],
     [
      "2004.11822",
      "Spatio-Temporal Network (T=128)",
      "30.7",
      30.7
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "1910.12032",
    "HEMlets Pose (H36M+MPII)",
    "27.9",
    27.9
   ],
   [
    "2004.11822",
    "Spatio-Temporal Network (T=128)",
    "30.7",
    30.7
   ],
   [
    "2105.02465",
    "HR-Net+VPose+PoseAug",
    "39.1",
    39.1
   ],
   [
    "1808.05942",
    "Neural Body Fitting (NBF)",
    "59.9",
    59.9
   ],
   [
    "1701.02468",
    "SMPLify (dense)",
    "80.7",
    80.7
   ]
  ],
  "distubed_docid": [
   "1912.00862",
   "2204.00987",
   "1904.01324",
   "2012.13912",
   "2205.15936",
   "1709.01829",
   "1904.03349",
   "1904.06627",
   "2104.06064",
   "2205.09443"
  ]
 },
 "./longdocdata/docs/3005.json": {
  "question": "List the performance scores of various methods on the DUT-OMRON (DUT-OMRON) dataset on the RGB Salient Object Detection task using metric F-measure.",
  "answer": "| Method | F-measure |\n| --- | --- |\n| TRACER-TE7 | 0.849 |\n| PoolNet (VGG-16) | 0.833 |\n| DSS (Res2Net-50) | 0.8 |\n| CPD-R (ResNet50) | 0.747 |",
  "src_docs": [
   "2112.07380",
   "1904.01169",
   "1904.08739",
   "1904.09569",
   "2005.09007"
  ],
  "updated_answer": [
   [
    "2112.07380",
    "TRACER-TE7",
    "0.853",
    0.853
   ],
   [
    "1904.09569",
    "PoolNet (VGG-16)",
    "0.833",
    0.833
   ],
   [
    "2005.09007",
    "UÂ²-Net",
    "0.823",
    0.823
   ],
   [
    "1904.08739",
    "CPD-R (ResNet50)",
    "0.797",
    0.797
   ],
   [
    "1904.01169",
    "DSS (Res2Net-50)",
    "0.800",
    0.8
   ]
  ],
  "meta_info": {
   "datasets": "DUT-OMRON",
   "datasets_short": "DUT-OMRON",
   "task": "RGB Salient Object Detection",
   "metric": "max F-measure"
  },
  "updated_answer2": [
   [
    "2112.07380",
    "TRACER-TE5",
    "0.853",
    0.853
   ],
   [
    "1904.09569",
    "PoolNet (VGG-16)",
    "0.833",
    0.833
   ],
   [
    "2005.09007",
    "UÂ²-Net",
    "0.823",
    0.823
   ],
   [
    "1904.01169",
    "DSS (Res2Net-50)",
    "0.800",
    0.8
   ],
   [
    "1904.08739",
    "CPD-R (ResNet50)",
    "0.797",
    0.797
   ]
  ],
  "updated_answer3": [
   [
    "2112.07380",
    "TRACER-TE5",
    "0.853",
    0.853
   ],
   [
    "1904.09569",
    "PoolNet (VGG-16)",
    "0.833",
    0.833
   ],
   [
    "2005.09007",
    "UÂ²-Net",
    "0.823",
    0.823
   ],
   [
    "1904.01169",
    "DSS (Res2Net-50)",
    "0.800",
    0.8
   ],
   [
    "1904.08739",
    "CPD-R (ResNet50)",
    "0.797",
    0.797
   ]
  ],
  "distubed_docid": [
   "2102.12443",
   "1506.04214",
   "1710.00017",
   "2103.03483",
   "1804.06208",
   "1902.00749",
   "2102.01063",
   "2109.01652",
   "1912.09654",
   "1907.10936"
  ]
 },
 "./longdocdata/docs/2438.json": {
  "question": "List the performance scores of various methods on the Tox21 (Tox21) dataset on the Molecular Property Prediction task using metric ROC-AUC.",
  "answer": "| Method | ROC-AUC |\n| --- | --- |\n| ChemRL-GEM | 78.1 |\n| PretrainGNN | 78.1 |\n| D-MPNN | 75.9 |\n| N-GramXGB | 75.8 |\n| N-GramRF | 74.3 |\n| GROVER (base) | 74.3 |\n| GROVER (large) | 73.5 |",
  "src_docs": [
   "2007.02835",
   "1905.12265",
   "1806.09206",
   "2106.06130",
   "1904.01561"
  ],
  "updated_answer": [
   [
    "2106.06130",
    "ChemRL-GEM",
    "78.1",
    78.1
   ],
   [
    "1905.12265",
    "PretrainGNN",
    "78.1",
    78.1
   ],
   [
    "1904.01561",
    "D-MPNN",
    "75.9",
    75.9
   ],
   [
    "1806.09206",
    "N-GramXGB",
    "75.8",
    75.8
   ],
   [
    "1806.09206",
    "N-GramRF",
    "74.3",
    74.3
   ],
   [
    "2007.02835",
    "GROVER (base)",
    "74.3",
    74.3
   ],
   [
    "2007.02835",
    "GROVER (large)",
    "73.5",
    73.5
   ]
  ],
  "meta_info": {
   "datasets": "Tox21",
   "datasets_short": "Tox21",
   "task": "Molecular Property Prediction",
   "metric": "ROC-AUC"
  },
  "updated_answer2": [
   [
    "2106.06130",
    "ChemRL-GEM",
    "78.1",
    78.1
   ],
   [
    "1905.12265",
    "PretrainGNN",
    "78.1",
    78.1
   ],
   [
    "1904.01561",
    "D-MPNN",
    "75.9",
    75.9
   ],
   [
    "1806.09206",
    "N-GramXGB",
    "75.8",
    75.8
   ],
   [
    "2007.02835",
    "GROVER (base)",
    "74.3",
    74.3
   ]
  ],
  "updated_answer3": [
   [
    "2106.06130",
    "ChemRL-GEM",
    "78.1",
    78.1
   ],
   [
    "1905.12265",
    "PretrainGNN",
    "78.1",
    78.1
   ],
   [
    "1904.01561",
    "D-MPNN",
    "75.9",
    75.9
   ],
   [
    "1806.09206",
    "N-GramXGB",
    "75.8",
    75.8
   ],
   [
    "2007.02835",
    "GROVER (base)",
    "74.3",
    74.3
   ]
  ]
 },
 "./longdocdata/docs/3316.json": {
  "question": "List the performance scores of various methods on the MPI-INF-3DHP (MPI-INF-3DHP) dataset on the 3D Human Pose Estimation task using metric Acceleration Error.",
  "answer": "| Method | Acceleration Error |\n| --- | --- |\n| TCMR (T=16 w/o 3DPW) | 8.0 |\n| TCMR (T=16 w/o H3.6M) | 8.5 |\n| MoCap-SPIN + PoseBERT | 8.7 |\n| MPS-Net (T=16) | 9.6 |\n| MEVA | 11.1 |\n| DST-VIBE | 11.9 |",
  "src_docs": [
   "2110.09243",
   "2008.03789",
   "2203.08534",
   "2011.08627",
   "2110.11680"
  ],
  "updated_answer": [
   [
    "2110.11680",
    "DST-VIBE",
    "11.9",
    11.9
   ],
   [
    "2008.03789",
    "MEVA",
    "11.1",
    11.1
   ],
   [
    "2203.08534",
    "MPS-Net (T=16)",
    "9.6",
    9.6
   ],
   [
    "2011.08627",
    "TCMR (T=16 w/o H3.6M)",
    "8.5",
    8.5
   ],
   [
    "2011.08627",
    "TCMR (T=16 w/o 3DPW)",
    "8",
    8.0
   ],
   [
    "2110.09243",
    "MoCap-SPIN + PoseBERT",
    "8.7",
    8.7
   ]
  ],
  "meta_info": {
   "datasets": "MPI-INF-3DHP",
   "datasets_short": "MPI-INF-3DHP",
   "task": "3D Human Pose Estimation",
   "metric": "Acceleration Error"
  },
  "updated_answer2": [
   [
    "2011.08627",
    "TCMR (T=16 w/o 3DPW)",
    "8",
    8.0
   ],
   [
    "2110.09243",
    "MoCap-SPIN + PoseBERT",
    "8.7",
    8.7
   ],
   [
    "2203.08534",
    "MPS-Net (T=16)",
    "9.6",
    9.6
   ],
   [
    "2008.03789",
    "MEVA",
    "11.1",
    11.1
   ],
   [
    "2110.11680",
    "DST-VIBE",
    "11.9",
    11.9
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Acceleration Error metric for the 3D Human Pose Estimation task on the MPI-INF-3DHP (MPI-INF-3DHP) dataset, and provide the top three metric results.",
    "answer": [
     "2011.08627",
     "TCMR (T=16 w/o 3DPW)",
     "8",
     8.0
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Acceleration Error metric for the 3D Human Pose Estimation task on the MPI-INF-3DHP (MPI-INF-3DHP) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2011.08627",
      "TCMR (T=16 w/o 3DPW)",
      "8",
      8.0
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Acceleration Error metric for the 3D Human Pose Estimation task on the MPI-INF-3DHP (MPI-INF-3DHP) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2011.08627",
      "TCMR (T=16 w/o 3DPW)",
      "8",
      8.0
     ],
     [
      "2110.09243",
      "MoCap-SPIN + PoseBERT",
      "8.7",
      8.7
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2011.08627",
    "TCMR (T=16 w/o 3DPW)",
    "8",
    8.0
   ],
   [
    "2110.09243",
    "MoCap-SPIN + PoseBERT",
    "8.7",
    8.7
   ],
   [
    "2203.08534",
    "MPS-Net (T=16)",
    "9.6",
    9.6
   ],
   [
    "2008.03789",
    "MEVA",
    "11.1",
    11.1
   ],
   [
    "2110.11680",
    "DST-VIBE",
    "11.9",
    11.9
   ]
  ],
  "distubed_docid": [
   "1707.06777",
   "2105.13626",
   "2202.12555",
   "1709.07871",
   "2101.07974",
   "2108.10531",
   "2003.12943",
   "2111.07224",
   "1903.05625",
   "1903.10663"
  ]
 },
 "./longdocdata/docs/3579.json": {
  "question": "List the performance scores of various methods on the Breakfast (Breakfast) dataset on the Weakly Supervised Action Segmentation (Transcript) task using metric Acc.",
  "answer": "| Method | Acc |\n| --- | --- |\n| FIFA + MuCon | 51.3 |\n| CDFL | 50.2 |\n| MuCon | 48.5 |\n| D3TW | 45.7 |\n| NNV | 43.0 |",
  "src_docs": [
   "1901.02598",
   "1909.13155",
   "1904.03116",
   "2108.03894",
   "1805.06875"
  ],
  "updated_answer": [
   [
    "2108.03894",
    "FIFA + MuCon",
    "51.3",
    51.3
   ],
   [
    "1909.13155",
    "CDFL",
    "50.2",
    50.2
   ],
   [
    "1904.03116",
    "MuCon",
    "48.5",
    48.5
   ],
   [
    "1901.02598",
    "D3TW",
    "45.7",
    45.7
   ],
   [
    "1805.06875",
    "NNV",
    "43",
    43.0
   ]
  ],
  "meta_info": {
   "datasets": "Breakfast",
   "datasets_short": "Breakfast",
   "task": "Weakly Supervised Action Segmentation (Transcript)",
   "metric": "Acc"
  },
  "updated_answer2": [
   [
    "2108.03894",
    "FIFA + MuCon",
    "51.3",
    51.3
   ],
   [
    "1909.13155",
    "CDFL",
    "50.2",
    50.2
   ],
   [
    "1904.03116",
    "MuCon",
    "48.5",
    48.5
   ],
   [
    "1901.02598",
    "D3TW",
    "45.7",
    45.7
   ],
   [
    "1805.06875",
    "NNV",
    "43",
    43.0
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Acc metric for the Weakly Supervised Action Segmentation (Transcript) task on the Breakfast (Breakfast) dataset, and provide the top three metric results.",
    "answer": [
     "2108.03894",
     "FIFA + MuCon",
     "51.3",
     51.3
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Acc metric for the Weakly Supervised Action Segmentation (Transcript) task on the Breakfast (Breakfast) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2108.03894",
      "FIFA + MuCon",
      "51.3",
      51.3
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the Acc metric for the Weakly Supervised Action Segmentation (Transcript) task on the Breakfast (Breakfast) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2108.03894",
      "FIFA + MuCon",
      "51.3",
      51.3
     ],
     [
      "1909.13155",
      "CDFL",
      "50.2",
      50.2
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2108.03894",
    "FIFA + MuCon",
    "51.3",
    51.3
   ],
   [
    "1909.13155",
    "CDFL",
    "50.2",
    50.2
   ],
   [
    "1904.03116",
    "MuCon",
    "48.5",
    48.5
   ],
   [
    "1901.02598",
    "D3TW",
    "45.7",
    45.7
   ],
   [
    "1805.06875",
    "NNV",
    "43",
    43.0
   ]
  ],
  "distubed_docid": [
   "1707.06750",
   "2102.01243",
   "2106.02638",
   "2109.01652",
   "1812.05770",
   "2205.00823",
   "1812.00739",
   "2103.14030",
   "2102.02887",
   "2111.05329"
  ]
 },
 "./longdocdata/docs/4140.json": {
  "question": "List the performance scores of various methods on the AGENDA (AGENDA) dataset on the KG-to-Text Generation task using metric BLEU.",
  "answer": "| Method | BLEU |\n| --- | --- |\n| BART-large+ STA | 25.66 |\n| BART-large | 23.65 |\n| Writer-Reviewer | 19.6 |\n| CGE-LW | 18.01 |\n| Graformer | 17.8 |\n| GraphWriter | 14.3 |",
  "src_docs": [
   "1904.02342",
   "2101.00916",
   "2006.09242",
   "2007.08426",
   "2001.11003"
  ],
  "updated_answer": [
   [
    "2007.08426",
    "BART-large+ STA",
    "25.66",
    25.66
   ],
   [
    "2007.08426",
    "BART-large",
    "23.65",
    23.65
   ],
   [
    "2101.00916",
    "Writer-Reviewer",
    "19.60",
    19.6
   ],
   [
    "2001.11003",
    "CGE-LW",
    "18.01",
    18.01
   ],
   [
    "2006.09242",
    "Graformer",
    "17.80",
    17.8
   ],
   [
    "1904.02342",
    "GraphWriter",
    "14.3",
    14.3
   ]
  ],
  "meta_info": {
   "datasets": "AGENDA",
   "datasets_short": "AGENDA",
   "task": "KG-to-Text Generation",
   "metric": "BLEU"
  },
  "updated_answer2": [
   [
    "2007.08426",
    "BART-large+ STA",
    "25.66",
    25.66
   ],
   [
    "2101.00916",
    "Writer-Reviewer",
    "19.60",
    19.6
   ],
   [
    "2001.11003",
    "CGE-LW",
    "18.01",
    18.01
   ],
   [
    "2006.09242",
    "Graformer",
    "17.80",
    17.8
   ],
   [
    "1904.02342",
    "GraphWriter",
    "14.3",
    14.3
   ]
  ],
  "list_question": [
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the BLEU metric for the KG-to-Text Generation task on the AGENDA (AGENDA) dataset, and provide the top three metric results.",
    "answer": [
     "2007.08426",
     "BART-large+ STA",
     "25.66",
     25.66
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the BLEU metric for the KG-to-Text Generation task on the AGENDA (AGENDA) dataset, and provide the top two metric results.",
    "answer": [
     [
      "2007.08426",
      "BART-large+ STA",
      "25.66",
      25.66
     ]
    ]
   },
   {
    "question": "Please help me summarize the performance of methods proposed in different existing papers on the BLEU metric for the KG-to-Text Generation task on the AGENDA (AGENDA) dataset, and provide the best current metric value.",
    "answer": [
     [
      "2007.08426",
      "BART-large+ STA",
      "25.66",
      25.66
     ],
     [
      "2101.00916",
      "Writer-Reviewer",
      "19.60",
      19.6
     ]
    ]
   }
  ],
  "updated_answer3": [
   [
    "2007.08426",
    "BART-large+ STA",
    "25.66",
    25.66
   ],
   [
    "2101.00916",
    "Writer-Reviewer",
    "19.60",
    19.6
   ],
   [
    "2001.11003",
    "CGE-LW",
    "18.01",
    18.01
   ],
   [
    "2006.09242",
    "Graformer",
    "17.80",
    17.8
   ],
   [
    "1904.02342",
    "GraphWriter",
    "14.3",
    14.3
   ]
  ],
  "distubed_docid": [
   "2005.12872",
   "2002.02798",
   "1908.05968",
   "2202.02299",
   "2103.08808",
   "1910.10750",
   "2201.03546",
   "1206.6392",
   "2203.07845",
   "2109.14084"
  ]
 },
 "./longdocdata/docs/5774.json": {
  "question": "List the performance scores of various methods on the Speech Commands (Speech Commands) dataset on the Time Series Analysis task using metric % Test Accuracy.",
  "answer": "| Method | % Test Accuracy |\n| --- | --- |\n| SepTr | 98.51 |\n| ViT | 98.11 |\n| FlexTCN-4 | 97.73 |\n| MatchboxNet | 97.4 |\n| CKCNN (100k) | 95.27 |",
  "src_docs": [
   "2104.01778",
   "2102.02611",
   "2203.09581",
   "2004.08531",
   "2110.08059",
   "2204.11479"
  ],
  "updated_answer": [
   [
    "2203.09581",
    "SepTr",
    "98.51",
    98.51
   ],
   [
    "2204.11479",
    "EAT-S",
    "98.15",
    98.15
   ],
   [
    "2104.01778",
    "ViT",
    "98.11",
    98.11
   ],
   [
    "2110.08059",
    "FlexTCN-4",
    "97.73",
    97.73
   ],
   [
    "2004.08531",
    "MatchboxNet",
    "97.40",
    97.4
   ],
   [
    "2102.02611",
    "CKCNN (100k)",
    "95.27",
    95.27
   ]
  ],
  "meta_info": {
   "datasets": "Speech Commands v2",
   "datasets_short": "Speech Commands",
   "task": "Time Series Analysis",
   "metric": "% Test Accuracy"
  },
  "updated_answer2": [
   [
    "2203.09581",
    "SepTr",
    "98.51",
    98.51
   ],
   [
    "2204.11479",
    "EAT-S",
    "98.15",
    98.15
   ],
   [
    "2104.01778",
    "ViT",
    "98.11",
    98.11
   ],
   [
    "2110.08059",
    "FlexTCN-4",
    "97.73",
    97.73
   ],
   [
    "2004.08531",
    "MatchboxNet",
    "97.40",
    97.4
   ],
   [
    "2102.02611",
    "CKCNN (100k)",
    "95.27",
    95.27
   ]
  ],
  "updated_answer3": [
   [
    "2203.09581",
    "SepTr",
    "98.51",
    98.51
   ],
   [
    "2204.11479",
    "EAT-S",
    "98.15",
    98.15
   ],
   [
    "2104.01778",
    "ViT",
    "98.11",
    98.11
   ],
   [
    "2110.08059",
    "FlexTCN-4",
    "97.73",
    97.73
   ],
   [
    "2102.02611",
    "CKCNN (100k)",
    "95.27",
    95.27
   ]
  ],
  "distubed_docid": [
   "2104.00924",
   "1809.02874",
   "2203.08942",
   "1411.4555",
   "2205.01917",
   "2006.11149",
   "1907.10936",
   "2205.15936",
   "2111.07991",
   "2001.00294"
  ]
 },
 "./longdocdata/docs/49.json": {
  "question": "List the performance scores of various methods on the MNIST-test (MNIST) dataset on the Image Clustering task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| DynAE | 0.987 |\n| DTI-Clustering | 0.978 |\n| DDC-DA | 0.97 |\n| PSSC | 0.967 |\n| DDC | 0.965 |\n| AE+SNNL | 0.962 |\n| N2D (UMAP) | 0.948 |\n| SR-K-means | 0.863 |",
  "src_docs": [
   "2006.11132",
   "1901.07752",
   "1908.05968",
   "2006.04535",
   "2011.11586",
   "1810.04246",
   "1812.04287"
  ],
  "updated_answer": [
   [
    "1901.07752",
    "DynAE",
    "0.987",
    0.987
   ],
   [
    "2006.11132",
    "DTI-Clustering",
    "0.978",
    0.978
   ],
   [
    "1812.04287",
    "DDC-DA",
    "0.97",
    0.97
   ],
   [
    "2011.11586",
    "PSSC",
    "0.967",
    0.967
   ],
   [
    "1812.04287",
    "DDC",
    "0.965",
    0.965
   ],
   [
    "2006.04535",
    "AE+SNNL",
    "0.962",
    0.962
   ],
   [
    "1908.05968",
    "N2D (UMAP)",
    "0.948",
    0.948
   ],
   [
    "1810.04246",
    "SR-K-means",
    "0.863",
    0.863
   ]
  ],
  "meta_info": {
   "datasets": "MNIST-test",
   "datasets_short": "MNIST",
   "task": "Image Clustering",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "1901.07752",
    "DynAE",
    "0.987",
    0.987
   ],
   [
    "2006.11132",
    "DTI-Clustering",
    "0.978",
    0.978
   ],
   [
    "1812.04287",
    "DDC-DA",
    "0.97",
    0.97
   ],
   [
    "2011.11586",
    "PSSC",
    "0.967",
    0.967
   ],
   [
    "2006.04535",
    "AE+SNNL",
    "0.962",
    0.962
   ],
   [
    "1908.05968",
    "N2D (UMAP)",
    "0.948",
    0.948
   ],
   [
    "1810.04246",
    "SR-K-means",
    "0.863",
    0.863
   ]
  ],
  "updated_answer3": [
   [
    "1901.07752",
    "DynAE",
    "0.987",
    0.987
   ],
   [
    "1812.04287",
    "DDC-DA",
    "0.97",
    0.97
   ],
   [
    "2011.11586",
    "PSSC",
    "0.967",
    0.967
   ],
   [
    "1908.05968",
    "N2D (UMAP)",
    "0.948",
    0.948
   ],
   [
    "1810.04246",
    "SR-K-means",
    "0.863",
    0.863
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "1906.03158",
   "1903.00535",
   "2105.03588",
   "1605.09759",
   "1903.07414",
   "1904.08265",
   "1802.05695",
   "1610.05047",
   "1711.00199",
   "2006.12070"
  ]
 },
 "./longdocdata/docs/833.json": {
  "question": "List the performance scores of various methods on the KITTI 2012 (KITTI) dataset on the Optical Flow Estimation task using metric Average End-Point Error.",
  "answer": "| Method | Average End-Point Error |\n| --- | --- |\n| MaskFlownet-S | 1.1 |\n| MaskFlownet | 1.1 |\n| LiteFlowNet2-ft | 1.4 |\n| SelFlow | 1.5 |\n| PWC-Net + ft - axXiv | 1.5 |\n| FDFlowNet-ft | 1.5 |\n| IRR-PWC | 1.6 |\n| LiteFlowNet-ft | 1.6 |",
  "src_docs": [
   "1904.09117",
   "1805.07036",
   "2003.10955",
   "2006.12263",
   "1904.05290",
   "1903.07414",
   "1809.05571"
  ],
  "updated_answer": [
   [
    "2003.10955",
    "MaskFlownet-S",
    "1.1",
    1.1
   ],
   [
    "2003.10955",
    "MaskFlownet",
    "1.1",
    1.1
   ],
   [
    "1903.07414",
    "LiteFlowNet2-ft",
    "1.4",
    1.4
   ],
   [
    "1904.09117",
    "SelFlow",
    "1.5",
    1.5
   ],
   [
    "1809.05571",
    "PWC-Net + ft - axXiv",
    "1.5",
    1.5
   ],
   [
    "2006.12263",
    "FDFlowNet-ft",
    "1.5",
    1.5
   ],
   [
    "1904.05290",
    "IRR-PWC",
    "1.6",
    1.6
   ],
   [
    "1805.07036",
    "LiteFlowNet-ft",
    "1.6",
    1.6
   ]
  ],
  "meta_info": {
   "datasets": "KITTI 2012",
   "datasets_short": "KITTI",
   "task": "Optical Flow Estimation",
   "metric": "Average End-Point Error"
  },
  "updated_answer2": [
   [
    "2003.10955",
    "MaskFlownet-S",
    "1.1",
    1.1
   ],
   [
    "2003.10955",
    "MaskFlownet",
    "1.1",
    1.1
   ],
   [
    "1903.07414",
    "LiteFlowNet2-ft",
    "1.4",
    1.4
   ],
   [
    "1904.09117",
    "SelFlow",
    "1.5",
    1.5
   ],
   [
    "1809.05571",
    "PWC-Net + ft - axXiv",
    "1.5",
    1.5
   ],
   [
    "2006.12263",
    "FDFlowNet-ft",
    "1.5",
    1.5
   ],
   [
    "1904.05290",
    "IRR-PWC",
    "1.6",
    1.6
   ],
   [
    "1805.07036",
    "LiteFlowNet-ft",
    "1.6",
    1.6
   ]
  ],
  "updated_answer3": [
   [
    "2003.10955",
    "MaskFlownet-S",
    "1.1",
    1.1
   ],
   [
    "2003.10955",
    "MaskFlownet",
    "1.1",
    1.1
   ],
   [
    "1903.07414",
    "LiteFlowNet2-ft",
    "1.4",
    1.4
   ],
   [
    "1904.09117",
    "SelFlow",
    "1.5",
    1.5
   ],
   [
    "1809.05571",
    "PWC-Net + ft - axXiv",
    "1.5",
    1.5
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2012.04584",
   "1910.10750",
   "2108.07181",
   "2010.05006",
   "2205.14871",
   "2008.03156",
   "2110.03007",
   "1911.07451",
   "2003.06156",
   "2108.04536"
  ]
 },
 "./longdocdata/docs/1064.json": {
  "question": "List the performance scores of various methods on the PRID2011 (PRID2011) dataset on the Person Re-Identification task using metric Rank-5.",
  "answer": "| Method | Rank-5 |\n| --- | --- |\n| FGReID | 99.1 |\n| uPMnet | 97.7 |\n| DAL | 97.0 |\n| DGM+MLAPG+ | 92.5 |\n| UTAL | 83.1 |\n| DGM+IDE+ | 81.3 |\n| TAUDL | 78.7 |",
  "src_docs": [
   "1903.00535",
   "1809.02874",
   "1808.07301",
   "2011.13475",
   "2111.05170",
   "1709.09297"
  ],
  "updated_answer": [
   [
    "2011.13475",
    "FGReID",
    "99.1",
    99.1
   ],
   [
    "2111.05170",
    "uPMnet",
    "97.7",
    97.7
   ],
   [
    "1808.07301",
    "DAL",
    "97.0",
    97.0
   ],
   [
    "1709.09297",
    "DGM+MLAPG+",
    "92.5",
    92.5
   ],
   [
    "1709.09297",
    "DGM+IDE+",
    "81.3",
    81.3
   ],
   [
    "1903.00535",
    "UTAL",
    "83.1",
    83.1
   ],
   [
    "1809.02874",
    "TAUDL",
    "78.7",
    78.7
   ]
  ],
  "meta_info": {
   "datasets": "PRID2011",
   "datasets_short": "PRID2011",
   "task": "Person Re-Identification",
   "metric": "Rank-5"
  },
  "updated_answer2": [
   [
    "2011.13475",
    "FGReID",
    "99.1",
    99.1
   ],
   [
    "2111.05170",
    "uPMnet",
    "97.7",
    97.7
   ],
   [
    "1808.07301",
    "DAL",
    "97.0",
    97.0
   ],
   [
    "1709.09297",
    "DGM+MLAPG+",
    "92.5",
    92.5
   ],
   [
    "1903.00535",
    "UTAL",
    "83.1",
    83.1
   ],
   [
    "1809.02874",
    "TAUDL",
    "78.7",
    78.7
   ]
  ],
  "updated_answer3": [
   [
    "2111.05170",
    "uPMnet",
    "97.7",
    97.7
   ],
   [
    "1808.07301",
    "DAL",
    "97.0",
    97.0
   ],
   [
    "1709.09297",
    "DGM+MLAPG+",
    "92.5",
    92.5
   ],
   [
    "1903.00535",
    "UTAL",
    "83.1",
    83.1
   ],
   [
    "1809.02874",
    "TAUDL",
    "78.7",
    78.7
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2109.04290",
   "2111.15162",
   "2010.04502",
   "1904.09117",
   "2101.00259",
   "2111.07224",
   "2101.00916",
   "2108.00516",
   "1907.13487",
   "2111.05329"
  ]
 },
 "./longdocdata/docs/1357.json": {
  "question": "List the performance scores of various methods on the JFLEG (JFLEG) dataset on the Grammatical Error Correction task using metric GLEU.",
  "answer": "| Method | GLEU |\n| --- | --- |\n| VERNet | 62.1 |\n| Transformer + Pre-train with Pseudo Data + BERT | 62.0 |\n| SMT + BiGRU | 61.5 |\n| Copy-augmented Model (4 Ensemble +Denoising Autoencoder) | 61.0 |\n| Transformer | 59.9 |\n| CNN Seq2Seq | 57.47 |",
  "src_docs": [
   "1903.00138",
   "1801.08831",
   "2105.04443",
   "2005.00987",
   "1804.05945",
   "1804.05940"
  ],
  "updated_answer": [
   [
    "2105.04443",
    "VERNet",
    "62.1",
    62.1
   ],
   [
    "2005.00987",
    "Transformer + Pre-train with Pseudo Data + BERT",
    "62.0",
    62.0
   ],
   [
    "1804.05945",
    "SMT + BiGRU",
    "61.5",
    61.5
   ],
   [
    "1903.00138",
    "Copy-augmented Model (4 Ensemble +Denoising Autoencoder)",
    "61.0",
    61.0
   ],
   [
    "1804.05940",
    "Transformer",
    "59.9",
    59.9
   ],
   [
    "1801.08831",
    "CNN Seq2Seq",
    "57.47",
    57.47
   ]
  ],
  "meta_info": {
   "datasets": "JFLEG",
   "datasets_short": "JFLEG",
   "task": "Grammatical Error Correction",
   "metric": "GLEU"
  },
  "updated_answer2": [
   [
    "2105.04443",
    "VERNet",
    "62.1",
    62.1
   ],
   [
    "2005.00987",
    "Transformer + Pre-train with Pseudo Data + BERT",
    "62.0",
    62.0
   ],
   [
    "1804.05945",
    "SMT + BiGRU",
    "61.5",
    61.5
   ],
   [
    "1903.00138",
    "Copy-augmented Model (4 Ensemble +Denoising Autoencoder)",
    "61.0",
    61.0
   ],
   [
    "1804.05940",
    "Transformer",
    "59.9",
    59.9
   ],
   [
    "1801.08831",
    "CNN Seq2Seq",
    "57.47",
    57.47
   ]
  ],
  "updated_answer3": [
   [
    "2005.00987",
    "Transformer + Pre-train with Pseudo Data + BERT",
    "62.0",
    62.0
   ],
   [
    "1804.05945",
    "SMT + BiGRU",
    "61.5",
    61.5
   ],
   [
    "1903.00138",
    "Copy-augmented Model (4 Ensemble +Denoising Autoencoder)",
    "61.0",
    61.0
   ],
   [
    "1804.05940",
    "Transformer",
    "59.9",
    59.9
   ],
   [
    "1801.08831",
    "CNN Seq2Seq",
    "57.47",
    57.47
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2011.07457",
   "2104.11530",
   "1511.04196",
   "1912.06798",
   "2001.03905",
   "2104.01546",
   "1903.05625",
   "1902.00749",
   "2106.13884",
   "2007.11864"
  ]
 },
 "./longdocdata/docs/1476.json": {
  "question": "List the performance scores of various methods on the VOT2017 (VOT2017) dataset on the Visual Object Tracking task using metric Expected Average Overlap (EAO).",
  "answer": "| Method | Expected Average Overlap (EAO) |\n| --- | --- |\n| GFS-DCF | 0.397 |\n| SiamRPN+ | 0.3 |\n| SiamVGG | 0.286 |\n| SE-SiamFC | 0.27 |\n| SiamFC-lu (Ours) | 0.263 |\n| GradNet | 0.247 |",
  "src_docs": [
   "1907.13242",
   "1806.07078",
   "1909.06800",
   "1901.01660",
   "1902.02804",
   "2007.09115",
   "1802.08817"
  ],
  "updated_answer": [
   [
    "1907.13242",
    "GFS-DCF",
    "0.397",
    0.397
   ],
   [
    "1901.01660",
    "SiamRPN+",
    "0.30",
    0.3
   ],
   [
    "1902.02804",
    "SiamVGG",
    "0.286",
    0.286
   ],
   [
    "2007.09115",
    "SE-SiamFC",
    "0.27",
    0.27
   ],
   [
    "1806.07078",
    "SiamFC-lu (Ours)",
    "0.263",
    0.263
   ],
   [
    "1909.06800",
    "GradNet",
    "0.247",
    0.247
   ]
  ],
  "meta_info": {
   "datasets": "VOT2017",
   "datasets_short": "VOT2017",
   "task": "Visual Object Tracking",
   "metric": "Expected Average Overlap (EAO)"
  },
  "updated_answer2": [
   [
    "1907.13242",
    "GFS-DCF",
    "0.397",
    0.397
   ],
   [
    "1901.01660",
    "SiamRPN+",
    "0.30",
    0.3
   ],
   [
    "1902.02804",
    "SiamVGG",
    "0.286",
    0.286
   ],
   [
    "2007.09115",
    "SE-SiamFC",
    "0.27",
    0.27
   ],
   [
    "1806.07078",
    "SiamFC-lu (Ours)",
    "0.263",
    0.263
   ],
   [
    "1909.06800",
    "GradNet",
    "0.247",
    0.247
   ]
  ],
  "updated_answer3": [
   [
    "1907.13242",
    "GFS-DCF",
    "0.397",
    0.397
   ],
   [
    "1902.02804",
    "SiamVGG",
    "0.286",
    0.286
   ],
   [
    "2007.09115",
    "SE-SiamFC",
    "0.27",
    0.27
   ],
   [
    "1909.06800",
    "GradNet",
    "0.247",
    0.247
   ],
   [
    "1802.08817",
    "SA-Siam",
    "0.236",
    0.236
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2109.06067",
   "1903.00179",
   "2003.10955",
   "2010.00904",
   "2204.07580",
   "1805.01033",
   "2010.12812",
   "2009.09687",
   "2009.08553",
   "2104.00924"
  ]
 },
 "./longdocdata/docs/2155.json": {
  "question": "List the performance scores of various methods on the SBD (SBD) dataset on the Interactive Segmentation task using metric NoC@90.",
  "answer": "| Method | NoC@90 |\n| --- | --- |\n| UCP-Net | 5.0 |\n| IA-FP-Net(HRNet,SBD) | 5.25 |\n| RITM (HRNet18, SBD) | 5.43 |\n| FocalClick | 5.59 |\n| f-BRS-B (ResNet-101) | 7.73 |\n| DOS with GC | 12.8 |\n| DOS w/o GC | 16.79 |",
  "src_docs": [
   "1603.04042",
   "2203.05145",
   "2001.10331",
   "2102.06583",
   "2204.02574",
   "2109.07592"
  ],
  "updated_answer": [
   [
    "2109.07592",
    "UCP-Net",
    "5.00",
    5.0
   ],
   [
    "2203.05145",
    "IA-FP-Net(HRNet,SBD)",
    "5.25",
    5.25
   ],
   [
    "2102.06583",
    "RITM (HRNet18, SBD)",
    "5.43",
    5.43
   ],
   [
    "2204.02574",
    "FocalClick",
    "5.59",
    5.59
   ],
   [
    "2001.10331",
    "f-BRS-B (ResNet-101)",
    "7.73",
    7.73
   ],
   [
    "1603.04042",
    "DOS with GC",
    "12.80",
    12.8
   ],
   [
    "1603.04042",
    "DOS w/o GC",
    "16.79",
    16.79
   ]
  ],
  "meta_info": {
   "datasets": "SBD",
   "datasets_short": "SBD",
   "task": "Interactive Segmentation",
   "metric": "NoC@90"
  },
  "updated_answer2": [
   [
    "2109.07592",
    "UCP-Net",
    "5.00",
    5.0
   ],
   [
    "2203.05145",
    "IA-FP-Net(HRNet,SBD)",
    "5.25",
    5.25
   ],
   [
    "2102.06583",
    "RITM (HRNet18, SBD)",
    "5.43",
    5.43
   ],
   [
    "2204.02574",
    "FocalClick",
    "5.59",
    5.59
   ],
   [
    "2001.10331",
    "f-BRS-B (ResNet-101)",
    "7.73",
    7.73
   ],
   [
    "1603.04042",
    "DOS with GC",
    "12.80",
    12.8
   ]
  ],
  "updated_answer3": [
   [
    "2109.07592",
    "UCP-Net",
    "5.00",
    5.0
   ],
   [
    "2203.05145",
    "IA-FP-Net(HRNet,SBD)",
    "5.25",
    5.25
   ],
   [
    "2102.06583",
    "RITM (HRNet18, SBD)",
    "5.43",
    5.43
   ],
   [
    "2204.02574",
    "FocalClick",
    "5.59",
    5.59
   ],
   [
    "2001.10331",
    "f-BRS-B (ResNet-101)",
    "7.73",
    7.73
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "1903.10663",
   "2010.11465",
   "2202.13514",
   "2105.13290",
   "2109.03502",
   "1902.05356",
   "1806.03146",
   "1511.05926",
   "1910.07179",
   "1710.00925"
  ]
 },
 "./longdocdata/docs/2697.json": {
  "question": "List the performance scores of various methods on the Natural Questions (Natural Questions) dataset on the Passage Retrieval task using metric Precision@20.",
  "answer": "| Method | Precision@20 |\n| --- | --- |\n| DPR+ELECTRA-large-extreader-reranker | 85.26 |\n| DPR-PAQ | 84.68 |\n| DPR+RoBERTa-base-crossencoder-reranker | 84.46 |\n| RocketQA | 82.7 |\n| ANCE | 81.9 |\n| DPR | 79.4 |\n| BM25+RM3 | 64.2 |",
  "src_docs": [
   "2007.00808",
   "2109.03502",
   "2009.08553",
   "2010.08191",
   "2004.04906",
   "2107.13602"
  ],
  "updated_answer": [
   [
    "2107.13602",
    "DPR-PAQ",
    "84.68",
    84.68
   ],
   [
    "2010.08191",
    "RocketQA",
    "82.7",
    82.7
   ],
   [
    "2109.03502",
    "DPR+ELECTRA-large-extreader-reranker",
    "85.26",
    85.26
   ],
   [
    "2109.03502",
    "DPR+RoBERTa-base-crossencoder-reranker",
    "84.46",
    84.46
   ],
   [
    "2007.00808",
    "ANCE",
    "81.9",
    81.9
   ],
   [
    "2004.04906",
    "DPR",
    "79.4",
    79.4
   ],
   [
    "2009.08553",
    "BM25+RM3",
    "64.2",
    64.2
   ]
  ],
  "meta_info": {
   "datasets": "Natural Questions",
   "datasets_short": "Natural Questions",
   "task": "Passage Retrieval",
   "metric": "Precision@20"
  },
  "updated_answer2": [
   [
    "2109.03502",
    "DPR+ELECTRA-large-extreader-reranker",
    "85.26",
    85.26
   ],
   [
    "2107.13602",
    "DPR-PAQ",
    "84.68",
    84.68
   ],
   [
    "2010.08191",
    "RocketQA",
    "82.7",
    82.7
   ],
   [
    "2007.00808",
    "ANCE",
    "81.9",
    81.9
   ],
   [
    "2004.04906",
    "DPR",
    "79.4",
    79.4
   ],
   [
    "2009.08553",
    "BM25+RM3",
    "64.2",
    64.2
   ]
  ],
  "updated_answer3": [
   [
    "2107.13602",
    "DPR-PAQ",
    "84.68",
    84.68
   ],
   [
    "2010.08191",
    "RocketQA",
    "82.7",
    82.7
   ],
   [
    "2007.00808",
    "ANCE",
    "81.9",
    81.9
   ],
   [
    "2004.04906",
    "DPR",
    "79.4",
    79.4
   ],
   [
    "2009.08553",
    "BM25+RM3",
    "64.2",
    64.2
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "1901.03861",
   "2204.12484",
   "2010.04502",
   "2108.12202",
   "2004.05571",
   "2109.04223",
   "2105.07107",
   "2002.05969",
   "1703.04816",
   "2004.11822"
  ]
 },
 "./longdocdata/docs/555.json": {
  "question": "List the performance scores of various methods on the COCO (COCO) dataset on the Text-to-Image Generation task using metric SOA-C.",
  "answer": "| Method | SOA-C |\n| --- | --- |\n| Lafite | 61.09 |\n| OP-GAN | 35.85 |\n| DM-GAN | 33.44 |\n| AttnGAN | 25.88 |\n| AttnGAN + OP | 25.46 |",
  "src_docs": [
   "1711.10485",
   "2111.13792",
   "1910.13321",
   "1904.01310",
   "1901.00686"
  ],
  "updated_answer": [
   [
    "2111.13792",
    "Lafite",
    "61.09",
    61.09
   ],
   [
    "1910.13321",
    "OP-GAN",
    "35.85",
    35.85
   ],
   [
    "1904.01310",
    "DM-GAN",
    "33.44",
    33.44
   ],
   [
    "1901.00686",
    "AttnGAN + OP",
    "25.46",
    25.46
   ],
   [
    "1711.10485",
    "AttnGAN",
    "25.88",
    25.88
   ]
  ],
  "meta_info": {
   "datasets": "COCO",
   "datasets_short": "COCO",
   "task": "Text-to-Image Generation",
   "metric": "SOA-C"
  },
  "updated_answer2": [
   [
    "2111.13792",
    "Lafite",
    "61.09",
    61.09
   ],
   [
    "1910.13321",
    "OP-GAN",
    "35.85",
    35.85
   ],
   [
    "1904.01310",
    "DM-GAN",
    "33.44",
    33.44
   ],
   [
    "1711.10485",
    "AttnGAN",
    "25.88",
    25.88
   ],
   [
    "1901.00686",
    "AttnGAN + OP",
    "25.46",
    25.46
   ]
  ],
  "updated_answer3": [
   [
    "2111.13792",
    "Lafite",
    "61.09",
    61.09
   ],
   [
    "1910.13321",
    "OP-GAN",
    "35.85",
    35.85
   ],
   [
    "1904.01310",
    "DM-GAN",
    "33.44",
    33.44
   ],
   [
    "1711.10485",
    "AttnGAN",
    "25.88",
    25.88
   ],
   [
    "1901.00686",
    "AttnGAN + OP",
    "25.46",
    25.46
   ]
  ]
 },
 "./longdocdata/docs/996.json": {
  "question": "List the performance scores of various methods on the COCO-Stuff 64x64 (COCO-Stuff) dataset on the Layout-to-Image Generation task using metric FID.",
  "answer": "| Method | FID |\n| --- | --- |\n| OC-GAN | 29.57 |\n| LostGAN | 34.31 |\n| Layout2Im | 38.14 |\n| SOARISG | 48.7 |\n| SG2Im | 67.96 |",
  "src_docs": [
   "1811.11389",
   "2003.07449",
   "1909.05379",
   "1804.01622",
   "1908.07500"
  ],
  "updated_answer": [
   [
    "2003.07449",
    "OC-GAN",
    "29.57",
    29.57
   ],
   [
    "1908.07500",
    "LostGAN",
    "34.31",
    34.31
   ],
   [
    "1811.11389",
    "Layout2Im",
    "38.14",
    38.14
   ],
   [
    "1909.05379",
    "SOARISG",
    "48.7",
    48.7
   ],
   [
    "1804.01622",
    "SG2Im",
    "67.96",
    67.96
   ]
  ],
  "meta_info": {
   "datasets": "COCO-Stuff 64x64",
   "datasets_short": "COCO-Stuff",
   "task": "Layout-to-Image Generation",
   "metric": "FID"
  },
  "updated_answer2": [
   [
    "2003.07449",
    "OC-GAN",
    "29.57",
    29.57
   ],
   [
    "1908.07500",
    "LostGAN",
    "34.31",
    34.31
   ],
   [
    "1811.11389",
    "Layout2Im",
    "38.14",
    38.14
   ],
   [
    "1909.05379",
    "SOARISG",
    "48.7",
    48.7
   ],
   [
    "1804.01622",
    "SG2Im",
    "67.96",
    67.96
   ]
  ],
  "updated_answer3": [
   [
    "2003.07449",
    "OC-GAN",
    "29.57",
    29.57
   ],
   [
    "1908.07500",
    "LostGAN",
    "34.31",
    34.31
   ],
   [
    "1811.11389",
    "Layout2Im",
    "38.14",
    38.14
   ],
   [
    "1909.05379",
    "SOARISG",
    "48.7",
    48.7
   ],
   [
    "1804.01622",
    "SG2Im",
    "67.96",
    67.96
   ]
  ]
 },
 "./longdocdata/docs/1122.json": {
  "question": "List the performance scores of various methods on the  CUB-200-2011 (CUB-200-2011) dataset on the Weakly-Supervised Object Localization task using metric Top-1 Localization Accuracy.",
  "answer": "| Method | Top-1 Localization Accuracy |\n| --- | --- |\n| PSOL-DenseNet161-Sep | 74.97 |\n| TokenCut | 72.9 |\n| LOST | 71.3 |\n| ART | 65.22 |\n| InfoCAM | 55.83 |",
  "src_docs": [
   "2202.11539",
   "2109.14279",
   "1911.10688",
   "1911.13073",
   "2002.11359"
  ],
  "updated_answer": [
   [
    "2002.11359",
    "PSOL-DenseNet161-Sep",
    "74.97",
    74.97
   ],
   [
    "2202.11539",
    "TokenCut",
    "72.9",
    72.9
   ],
   [
    "2109.14279",
    "LOST",
    "71.3",
    71.3
   ],
   [
    "1911.13073",
    "ART",
    "65.22",
    65.22
   ],
   [
    "1911.10688",
    "InfoCAM",
    "55.83",
    55.83
   ]
  ],
  "meta_info": {
   "datasets": " CUB-200-2011",
   "datasets_short": "CUB-200-2011",
   "task": "Weakly-Supervised Object Localization",
   "metric": "Top-1 Localization Accuracy"
  },
  "updated_answer2": [
   [
    "2002.11359",
    "PSOL-DenseNet161-Sep",
    "74.97",
    74.97
   ],
   [
    "2202.11539",
    "TokenCut",
    "72.9",
    72.9
   ],
   [
    "2109.14279",
    "LOST",
    "71.3",
    71.3
   ],
   [
    "1911.13073",
    "ART",
    "65.22",
    65.22
   ],
   [
    "1911.10688",
    "InfoCAM",
    "55.83",
    55.83
   ]
  ],
  "updated_answer3": [
   [
    "2002.11359",
    "PSOL-DenseNet161-Sep",
    "74.97",
    74.97
   ],
   [
    "2202.11539",
    "TokenCut",
    "72.9",
    72.9
   ],
   [
    "2109.14279",
    "LOST",
    "71.3",
    71.3
   ],
   [
    "1911.13073",
    "ART",
    "65.22",
    65.22
   ],
   [
    "1911.10688",
    "InfoCAM",
    "55.83",
    55.83
   ]
  ]
 },
 "./longdocdata/docs/733.json": {
  "question": "List the performance scores of various methods on the KITTI Pedestrians Moderate (KITTI) dataset on the Birds Eye View Object Detection task using metric AP.",
  "answer": "| Method | AP |\n| --- | --- |\n| STD | 51.39 |\n| AVOD-FPN | 51.05 |\n| PointPillars | 50.23 |\n| F-PointNet | 50.22 |\n| VoxelNet | 40.74 |",
  "src_docs": [
   "1812.05784",
   "1711.06396",
   "1907.10471",
   "1712.02294",
   "1711.08488"
  ],
  "updated_answer": [
   [
    "1907.10471",
    "STD",
    "51.39%",
    51.39
   ],
   [
    "1812.05784",
    "PointPillars",
    "50.23%",
    50.23
   ],
   [
    "1711.08488",
    "F-PointNet",
    "50.22%",
    50.22
   ],
   [
    "1712.02294",
    "AVOD-FPN",
    "51.05%",
    51.05
   ],
   [
    "1711.06396",
    "VoxelNet",
    "40.74%",
    40.74
   ]
  ],
  "meta_info": {
   "datasets": "KITTI Pedestrians Moderate test set",
   "datasets_short": "KITTI test set",
   "task": "Birds Eye View Object Detection",
   "metric": "AP"
  },
  "updated_answer2": [
   [
    "1907.10471",
    "STD",
    "51.39%",
    51.39
   ],
   [
    "1812.05784",
    "PointPillars",
    "50.23%",
    50.23
   ],
   [
    "1711.08488",
    "F-PointNet",
    "50.22%",
    50.22
   ],
   [
    "1712.02294",
    "AVOD-FPN",
    "51.05%",
    51.05
   ],
   [
    "1711.06396",
    "VoxelNet",
    "40.74%",
    40.74
   ]
  ],
  "updated_answer3": [
   [
    "1907.10471",
    "STD",
    "51.39%",
    51.39
   ],
   [
    "1812.05784",
    "PointPillars",
    "50.23%",
    50.23
   ],
   [
    "1711.08488",
    "F-PointNet",
    "50.22%",
    50.22
   ],
   [
    "1712.02294",
    "AVOD-FPN",
    "51.05%",
    51.05
   ],
   [
    "1711.06396",
    "VoxelNet",
    "40.74%",
    40.74
   ]
  ]
 },
 "./longdocdata/docs/369.json": {
  "question": "List the performance scores of various methods on the Fashion-MNIST (Fashion-MNIST) dataset on the Anomaly Detection task using metric ROC AUC.",
  "answer": "| Method | ROC AUC |\n| --- | --- |\n| PANDA | 95.6 |\n| Reverse Distillation | 95.0 |\n| IGD (pre-trained SSL) | 94.41 |\n| IGD (pre-trained ImageNet) | 93.57 |\n| Self-Supervised One-class SVM, RBF kernel | 92.8 |\n| DASVDD | 92.6 |\n| IGD (scratch) | 92.01 |\n| PANDA-OE | 91.8 |\n| Self-Supervised DeepSVDD | 84.8 |\n| P-KDGAN | 0.9293 |",
  "src_docs": [
   "2010.05903",
   "2201.10703",
   "2106.05410",
   "2101.10043",
   "2007.06963"
  ],
  "updated_answer": [
   [
    "2010.05903",
    "PANDA",
    "95.6",
    95.6
   ],
   [
    "2201.10703",
    "Reverse Distillation",
    "95.0",
    95.0
   ],
   [
    "2101.10043",
    "IGD (pre-trained SSL)",
    "94.41",
    94.41
   ],
   [
    "2101.10043",
    "IGD (pre-trained ImageNet)",
    "93.57",
    93.57
   ],
   [
    "2010.05903",
    "Self-Supervised One-class SVM, RBF kernel",
    "92.8",
    92.8
   ],
   [
    "2106.05410",
    "DASVDD",
    "92.6",
    92.6
   ],
   [
    "2101.10043",
    "IGD (scratch)",
    "92.01",
    92.01
   ],
   [
    "2010.05903",
    "PANDA-OE",
    "91.8",
    91.8
   ],
   [
    "2010.05903",
    "Self-Supervised DeepSVDD",
    "84.8",
    84.8
   ],
   [
    "2007.06963",
    "P-KDGAN",
    "0.9293",
    0.9293
   ]
  ],
  "meta_info": {
   "datasets": "Fashion-MNIST",
   "datasets_short": "Fashion-MNIST",
   "task": "Anomaly Detection",
   "metric": "ROC AUC"
  },
  "updated_answer2": [
   [
    "2010.05903",
    "PANDA",
    "0.956",
    0.956
   ],
   [
    "2201.10703",
    "Reverse Distillation",
    "0.95",
    0.95
   ],
   [
    "2101.10043",
    "IGD (pre-trained SSL)",
    "0.9441",
    0.9441
   ],
   [
    "2106.05410",
    "DASVDD",
    "0.926",
    0.926
   ],
   [
    "2007.06963",
    "P-KDGAN",
    "0.9293",
    0.9293
   ]
  ],
  "updated_answer3": [
   [
    "2010.05903",
    "PANDA",
    "0.956",
    0.956
   ],
   [
    "2201.10703",
    "Reverse Distillation",
    "0.950",
    0.95
   ],
   [
    "2101.10043",
    "IGD (pre-trained SSL)",
    "0.9441",
    0.9441
   ],
   [
    "2007.06963",
    "P-KDGAN",
    "0.9293",
    0.9293
   ],
   [
    "2106.05410",
    "DASVDD",
    "0.926",
    0.926
   ]
  ]
 },
 "./longdocdata/docs/229.json": {
  "question": "List the performance scores of various methods on the ImageNet V2 (ImageNet) dataset on the Zero-Shot Transfer Image Classification task using metric Accuracy (Private).",
  "answer": "| Method | Accuracy (Private) |\n| --- | --- |\n| CoCa | 80.7 |\n| BASIC | 80.6 |\n| LiT-tuning | 78.7 |\n| ALIGN | 70.1 |\n| CLIP | 70.1 |",
  "src_docs": [
   "2205.01917",
   "2111.10050",
   "2102.05918",
   "2111.07991",
   "2103.00020"
  ],
  "updated_answer": [
   [
    "2205.01917",
    "CoCa",
    "80.7",
    80.7
   ],
   [
    "2111.10050",
    "BASIC",
    "80.6",
    80.6
   ],
   [
    "2111.07991",
    "LiT-tuning",
    "78.7",
    78.7
   ],
   [
    "2102.05918",
    "ALIGN",
    " 70.1",
    70.1
   ],
   [
    "2103.00020",
    "CLIP",
    "70.1",
    70.1
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet V2",
   "datasets_short": "ImageNet",
   "task": "Zero-Shot Transfer Image Classification",
   "metric": "Accuracy (Private)"
  },
  "updated_answer2": [
   [
    "2205.01917",
    "CoCa",
    "80.7",
    80.7
   ],
   [
    "2111.10050",
    "BASIC",
    "80.6",
    80.6
   ],
   [
    "2111.07991",
    "LiT-tuning",
    "78.7",
    78.7
   ],
   [
    "2102.05918",
    "ALIGN",
    " 70.1",
    70.1
   ],
   [
    "2103.00020",
    "CLIP",
    "70.1",
    70.1
   ]
  ],
  "updated_answer3": [
   [
    "2205.01917",
    "CoCa",
    "80.7",
    80.7
   ],
   [
    "2111.10050",
    "BASIC",
    "80.6",
    80.6
   ],
   [
    "2111.07991",
    "LiT-tuning",
    "78.7",
    78.7
   ],
   [
    "2102.05918",
    "ALIGN",
    " 70.1",
    70.1
   ],
   [
    "2103.00020",
    "CLIP",
    "70.1",
    70.1
   ]
  ]
 },
 "./longdocdata/docs/291.json": {
  "question": "List the performance scores of various methods on the ImageNet-10 (ImageNet) dataset on the Image Clustering task using metric ARI.",
  "answer": "| Method | ARI |\n| --- | --- |\n| SPICE (Full ImageNet pre-train) | 0.933 |\n| ProPos* | 0.918 |\n| ConCURL | 0.909 |\n| ProPos | 0.906 |\n| IDFD | 0.901 |\n| CC | 0.822 |",
  "src_docs": [
   "2105.01289",
   "2111.11821",
   "2009.09687",
   "2103.09382",
   "2106.00131"
  ],
  "updated_answer": [
   [
    "2103.09382",
    "SPICE",
    "0.912",
    0.912
   ],
   [
    "2111.11821",
    "ProPos*",
    "0.918",
    0.918
   ],
   [
    "2105.01289",
    "ConCURL",
    "0.909",
    0.909
   ],
   [
    "2106.00131",
    "IDFD",
    "0.901",
    0.901
   ],
   [
    "2111.11821",
    "ProPos",
    "0.906",
    0.906
   ],
   [
    "2009.09687",
    "CC",
    "0.822",
    0.822
   ]
  ],
  "meta_info": {
   "datasets": "ImageNet-10",
   "datasets_short": "ImageNet",
   "task": "Image Clustering",
   "metric": "ARI"
  },
  "updated_answer2": [
   [
    "2111.11821",
    "ProPos*",
    "0.918",
    0.918
   ],
   [
    "2103.09382",
    "SPICE",
    "0.912",
    0.912
   ],
   [
    "2105.01289",
    "ConCURL",
    "0.909",
    0.909
   ],
   [
    "2106.00131",
    "IDFD",
    "0.901",
    0.901
   ],
   [
    "2009.09687",
    "CC",
    "0.822",
    0.822
   ]
  ],
  "updated_answer3": [
   [
    "2103.09382",
    "SPICE",
    "0.912",
    0.912
   ],
   [
    "2111.11821",
    "ProPos*",
    "0.918",
    0.918
   ],
   [
    "2105.01289",
    "ConCURL",
    "0.909",
    0.909
   ],
   [
    "2106.00131",
    "IDFD",
    "0.901",
    0.901
   ],
   [
    "2009.09687",
    "CC",
    "0.822",
    0.822
   ]
  ]
 },
 "./longdocdata/docs/3174.json": {
  "question": "List the performance scores of various methods on the MSVD (MSVD) dataset on the Video Retrieval task using metric video-to-text R@1.",
  "answer": "| Method | video-to-text R@1 |\n| --- | --- |\n| HunYuan_tvr (huge) | 73.0 |\n| CAMoE | 69.3 |\n| HunYuan_tvr | 69.1 |\n| CenterCLIP (ViT-B/16) | 68.4 |\n| X-Pool | 66.4 |\n| CLIP4Clip | 62.0 |\n| CLIP | 59.9 |",
  "src_docs": [
   "2102.12443",
   "2204.03382",
   "2205.00823",
   "2203.15086",
   "2109.04290",
   "2104.08860"
  ],
  "updated_answer": [
   [
    "2204.03382",
    "HunYuan_tvr (huge)",
    "73.0",
    73.0
   ],
   [
    "2204.03382",
    "HunYuan_tvr",
    "69.1",
    69.1
   ],
   [
    "2109.04290",
    "CAMoE",
    "69.3",
    69.3
   ],
   [
    "2205.00823",
    "CenterCLIP (ViT-B/16)",
    "68.4",
    68.4
   ],
   [
    "2203.15086",
    "X-Pool",
    "66.4",
    66.4
   ],
   [
    "2104.08860",
    "CLIP4Clip",
    "62.0",
    62.0
   ],
   [
    "2102.12443",
    "CLIP",
    "59.9",
    59.9
   ]
  ],
  "meta_info": {
   "datasets": "MSVD",
   "datasets_short": "MSVD",
   "task": "Video Retrieval",
   "metric": "video-to-text R@1"
  },
  "updated_answer2": [
   [
    "2204.03382",
    "HunYuan_tvr (huge)",
    "73.0",
    73.0
   ],
   [
    "2109.04290",
    "CAMoE",
    "69.3",
    69.3
   ],
   [
    "2205.00823",
    "CenterCLIP (ViT-B/16)",
    "68.4",
    68.4
   ],
   [
    "2203.15086",
    "X-Pool",
    "66.4",
    66.4
   ],
   [
    "2104.08860",
    "CLIP4Clip",
    "62.0",
    62.0
   ],
   [
    "2102.12443",
    "CLIP",
    "59.9",
    59.9
   ]
  ],
  "updated_answer3": [
   [
    "2204.03382",
    "HunYuan_tvr (huge)",
    "73.0",
    73.0
   ],
   [
    "2205.00823",
    "CenterCLIP (ViT-B/16)",
    "68.4",
    68.4
   ],
   [
    "2203.15086",
    "X-Pool",
    "66.4",
    66.4
   ],
   [
    "2104.08860",
    "CLIP4Clip",
    "62.0",
    62.0
   ],
   [
    "2102.12443",
    "CLIP",
    "59.9",
    59.9
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "1911.07771",
   "2010.13668",
   "1506.04214",
   "2203.14211",
   "1709.03741",
   "1912.06290",
   "1511.04196",
   "2004.08795",
   "2203.16250",
   "2105.05926"
  ]
 },
 "./longdocdata/docs/3413.json": {
  "question": "List the performance scores of various methods on the COPA (SuperGLUE) dataset on the Question Answering task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| PaLM 540B (finetuned)  | 100.0 |\n| DeBERTa-Ensemble | 98.4 |\n| DeBERTa-1.5B | 96.8 |\n| T5-11B | 94.8 |\n| GPT-3 175B (Few-Shot) | 92.0 |\n| FLAN 137B zero-shot | 91.0 |\n| KELM (finetuning BERT-large based single model) | 78.0 |",
  "src_docs": [
   "2204.02311",
   "2006.03654",
   "2005.14165",
   "1910.10683",
   "2109.01652",
   "2109.04223"
  ],
  "updated_answer": [
   [
    "2204.02311",
    "PaLM 540B (finetuned) ",
    "100",
    100.0
   ],
   [
    "2006.03654",
    "DeBERTa-Ensemble",
    "98.4",
    98.4
   ],
   [
    "2006.03654",
    "DeBERTa-1.5B",
    "96.8",
    96.8
   ],
   [
    "1910.10683",
    "T5-11B",
    "94.8",
    94.8
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "92",
    92.0
   ],
   [
    "2109.01652",
    "FLAN 137B zero-shot",
    "91.0",
    91.0
   ],
   [
    "2109.04223",
    "KELM (finetuning BERT-large based single model)",
    "78.0",
    78.0
   ]
  ],
  "meta_info": {
   "datasets": "COPA",
   "datasets_short": "SuperGLUE",
   "task": "Question Answering",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "2204.02311",
    "PaLM 540B (finetuned) ",
    "100",
    100.0
   ],
   [
    "2006.03654",
    "DeBERTa-Ensemble",
    "98.4",
    98.4
   ],
   [
    "1910.10683",
    "T5-11B",
    "94.8",
    94.8
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "92",
    92.0
   ],
   [
    "2109.01652",
    "FLAN 137B zero-shot",
    "91.0",
    91.0
   ],
   [
    "2109.04223",
    "KELM (finetuning BERT-large based single model)",
    "78.0",
    78.0
   ]
  ],
  "updated_answer3": [
   [
    "2006.03654",
    "DeBERTa-Ensemble",
    "98.4",
    98.4
   ],
   [
    "1910.10683",
    "T5-11B",
    "94.8",
    94.8
   ],
   [
    "2005.14165",
    "GPT-3 175B (Few-Shot)",
    "92",
    92.0
   ],
   [
    "2109.01652",
    "FLAN 137B zero-shot",
    "91.0",
    91.0
   ],
   [
    "2109.04223",
    "KELM (finetuning BERT-large based single model)",
    "78.0",
    78.0
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2108.00516",
   "1508.01991",
   "2203.15086",
   "1908.00300",
   "1904.10424",
   "1711.11248",
   "1911.11775",
   "2004.05571",
   "1902.08850",
   "1909.11874"
  ]
 },
 "./longdocdata/docs/3421.json": {
  "question": "List the performance scores of various methods on the LIDC-IDRI (LIDC-IDRI) dataset on the Lung Nodule Classification task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| ProCAN | 94.11 |\n| Gated-Dilated | 92.57 |\n| NASLung (ours) | 90.77 |\n| DeepLung | 90.44 |\n| Local-Global | 88.46 |",
  "src_docs": [
   "2101.07429",
   "2010.15417",
   "1901.00120",
   "1904.10126",
   "1801.09555"
  ],
  "updated_answer": [
   [
    "2010.15417",
    "ProCAN",
    "94.11",
    94.11
   ],
   [
    "1901.00120",
    "Gated-Dilated",
    "92.57",
    92.57
   ],
   [
    "2101.07429",
    "NASLung (ours)",
    "90.77",
    90.77
   ],
   [
    "1801.09555",
    "DeepLung",
    "90.44",
    90.44
   ],
   [
    "1904.10126",
    "Local-Global",
    "88.46",
    88.46
   ]
  ],
  "meta_info": {
   "datasets": "LIDC-IDRI",
   "datasets_short": "LIDC-IDRI",
   "task": "Lung Nodule Classification",
   "metric": "Accuracy (not Ensemble method)"
  },
  "updated_answer2": [
   [
    "2010.15417",
    "ProCAN",
    "94.11",
    94.11
   ],
   [
    "1901.00120",
    "Gated-Dilated",
    "92.57",
    92.57
   ],
   [
    "2101.07429",
    "NASLung (ours)",
    "90.77",
    90.77
   ],
   [
    "1801.09555",
    "DeepLung",
    "90.44",
    90.44
   ],
   [
    "1904.10126",
    "Local-Global",
    "88.46",
    88.46
   ]
  ],
  "updated_answer3": [
   [
    "2010.15417",
    "ProCAN",
    "94.11",
    94.11
   ],
   [
    "1901.00120",
    "Gated-Dilated",
    "92.57",
    92.57
   ],
   [
    "2101.07429",
    "NASLung (ours)",
    "90.77",
    90.77
   ],
   [
    "1801.09555",
    "DeepLung",
    "90.44",
    90.44
   ],
   [
    "1904.10126",
    "Local-Global",
    "88.46",
    88.46
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2202.13514",
   "1803.08999",
   "1909.07755",
   "2007.04174",
   "1912.06798",
   "1904.08265",
   "1911.06393",
   "2009.06946",
   "1711.08184",
   "1812.06145"
  ]
 },
 "./longdocdata/docs/3518.json": {
  "question": "List the performance scores of various methods on the FER2013 (FER2013) dataset on the Facial Expression Recognition (FER) task using metric Accuracy.",
  "answer": "| Method | Accuracy |\n| --- | --- |\n| Ensemble ResMaskingNet with 6 other CNNs | 76.82 |\n| Local Learning Deep+BOW | 75.42 |\n| LHC-Net | 74.42 |\n| Residual Masking Network | 74.14 |\n| VGGNet | 73.28 |\n| VGG | 72.7 |\n| Res-Net | 72.4 |\n| Inception | 71.6 |\n| DeepEmotion | 70.02 |\n| Local Learning BOW | 67.48 |",
  "src_docs": [
   "2111.07224",
   "2105.03588",
   "1804.10892",
   "1612.02903",
   "1902.01019"
  ],
  "updated_answer": [

   [
    "1804.10892",
    "Local Learning Deep+BOW",
    "75.42",
    75.42
   ],
   [
    "1612.02903",
    "VGG ensemble of 8 deep CNNs",
    "75.2",
    75.2
   ],
   [
    "2111.07224",
    "LHC-Net",
    "74.42",
    74.42
   ],
   [
    "2105.03588",
    "VGGNet",
    "73.28",
    73.28
   ],
   [
    "1902.01019",
    "DeepEmotion",
    "70.02",
    70.02
   ]
  ],
  "meta_info": {
   "datasets": "FER2013",
   "datasets_short": "FER2013",
   "task": "Facial Expression Recognition (FER)",
   "metric": "Accuracy"
  },
  "updated_answer2": [
   [
    "1804.10892",
    "Local Learning Deep+BOW",
    "75.42",
    75.42
   ],
   [
    "1612.02903",
    "VGG ensemble of 8 deep CNNs",
    "75.2",
    75.2
   ],
   [
    "2111.07224",
    "LHC-Net",
    "74.42",
    74.42
   ],
   [
    "2105.03588",
    "VGGNet",
    "73.28",
    73.28
   ],
   [
    "1902.01019",
    "DeepEmotion",
    "70.02",
    70.02
   ]
  ],
  "updated_answer3": [
   [
    "1804.10892",
    "Local Learning Deep+BOW",
    "75.42",
    75.42
   ],
   [
    "1612.02903",
    "VGG ensemble of 8 deep CNNs",
    "75.2",
    75.2
   ],
   [
    "2111.07224",
    "LHC-Net",
    "74.42",
    74.42
   ],
   [
    "2105.03588",
    "VGGNet",
    "73.28",
    73.28
   ],
   [
    "1902.01019",
    "DeepEmotion",
    "70.02",
    70.02
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2101.11228",
   "2203.04038",
   "2106.00131",
   "2203.01929",
   "2007.04174",
   "2109.04290",
   "2003.13880",
   "2106.02638",
   "1701.02468",
   "2107.02423"
  ]
 },
 "./longdocdata/docs/3523.json": {
  "question": "List the performance scores of various methods on the LOL (LOL) dataset on the Low-Light Image Enhancement task using metric SSIM.",
  "answer": "| Method | SSIM |\n| --- | --- |\n| LLFlow | 0.93 |\n| MAXIM | 0.863 |\n| HWMNet | 0.852 |\n| Bread | 0.838 |\n| TreEnhance | 0.81 |\n| IAT (90k parameter) | 0.809 |",
  "src_docs": [
   "2203.01296",
   "2205.12639",
   "2109.05923",
   "2201.02973",
   "2205.14871",
   "2111.15557"
  ],
  "updated_answer": [
   [
    "2109.05923",
    "LLFlow",
    "0.93",
    0.93
   ],
   [
    "2203.01296",
    "HWMNet",
    "0.852",
    0.852
   ],
   [
    "2201.02973",
    "MAXIM",
    "0.863",
    0.863
   ],
   [
    "2205.14871",
    "IAT (90k parameter)",
    "0.809",
    0.809
   ],
   [
    "2111.15557",
    "Bread",
    "0.838",
    0.838
   ],
   [
    "2205.12639",
    "TreEnhance",
    "0.81",
    0.81
   ]
  ],
  "meta_info": {
   "datasets": "LOL v1",
   "datasets_short": "LOL v1",
   "task": "Low-Light Image Enhancement",
   "metric": "SSIM"
  },
  "updated_answer2": [
   [
    "2109.05923",
    "LLFlow",
    "0.93",
    0.93
   ],
   [
    "2201.02973",
    "MAXIM",
    "0.863",
    0.863
   ],
   [
    "2203.01296",
    "HWMNet",
    "0.852",
    0.852
   ],
   [
    "2111.15557",
    "Bread",
    "0.838",
    0.838
   ],
   [
    "2205.12639",
    "TreEnhance",
    "0.81",
    0.81
   ],
   [
    "2205.14871",
    "IAT (90k parameter)",
    "0.809",
    0.809
   ]
  ],
  "updated_answer3": [
   [
    "2109.05923",
    "LLFlow",
    "0.93",
    0.93
   ],
   [
    "2201.02973",
    "MAXIM",
    "0.863",
    0.863
   ],
   [
    "2111.15557",
    "Bread",
    "0.838",
    0.838
   ],
   [
    "2205.12639",
    "TreEnhance",
    "0.81",
    0.81
   ],
   [
    "2205.14871",
    "IAT (90k parameter)",
    "0.809",
    0.809
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "1605.06409",
   "2001.11314",
   "2005.14165",
   "2203.11442",
   "2010.01057",
   "1904.10126",
   "2009.07526",
   "2102.01243",
   "1804.00382",
   "1605.00937"
  ]
 },
 "./longdocdata/docs/3610.json": {
  "question": "List the performance scores of various methods on the In-Shop (In-Shop) dataset on the Image Retrieval task using metric R@1.",
  "answer": "| Method | R@1 |\n| --- | --- |\n| CGD (SG/GS) | 91.9 |\n| Cross-Batch Memory | 91.3 |\n| ProxyNCA++ | 90.9 |\n| MS512 | 89.7 |\n| NormSoftmax2048 (ResNet-50) | 89.4 |\n| EPSHN512 | 87.8 |\n| ABE-8 | 87.3 |",
  "src_docs": [
   "1804.00382",
   "1903.10663",
   "1811.12649",
   "1904.04370",
   "1912.06798",
   "1904.06627",
   "2004.01113"
  ],
  "updated_answer": [
   [
    "1903.10663",
    "CGD (SG/GS)",
    "91.9",
    91.9
   ],
   [
    "1912.06798",
    "Cross-Batch Memory",
    "91.3",
    91.3
   ],
   [
    "2004.01113",
    "ProxyNCA++",
    "90.9",
    90.9
   ],
   [
    "1904.06627",
    "MS512",
    "89.7",
    89.7
   ],
   [
    "1811.12649",
    "NormSoftmax2048 (ResNet-50)",
    "89.4",
    89.4
   ],
   [
    "1904.04370",
    "EPSHN512",
    "87.8",
    87.8
   ],
   [
    "1804.00382",
    "ABE-8",
    "87.3",
    87.3
   ]
  ],
  "meta_info": {
   "datasets": "In-Shop",
   "datasets_short": "In-Shop",
   "task": "Image Retrieval",
   "metric": "R@1"
  },
  "updated_answer2": [
   [
    "1903.10663",
    "CGD (SG/GS)",
    "91.9",
    91.9
   ],
   [
    "1912.06798",
    "Cross-Batch Memory",
    "91.3",
    91.3
   ],
   [
    "2004.01113",
    "ProxyNCA++",
    "90.9",
    90.9
   ],
   [
    "1904.06627",
    "MS512",
    "89.7",
    89.7
   ],
   [
    "1811.12649",
    "NormSoftmax2048 (ResNet-50)",
    "89.4",
    89.4
   ],
   [
    "1904.04370",
    "EPSHN512",
    "87.8",
    87.8
   ],
   [
    "1804.00382",
    "ABE-8",
    "87.3",
    87.3
   ]
  ],
  "updated_answer3": [
    [
        "1903.10663",
        "CGD (SG/GS)",
        "91.9",
        91.9
    ],
   [
    "1912.06798",
    "Cross-Batch Memory",
    "91.3",
    91.3
   ],
   [
    "2004.01113",
    "ProxyNCA++",
    "90.9",
    90.9
   ],
   [
    "1904.06627",
    "MS512",
    "89.7",
    89.7
   ],
   [
    "1811.12649",
    "NormSoftmax2048 (ResNet-50)",
    "89.4",
    89.4
   ],
   [
    "1904.04370",
    "EPSHN512",
    "87.8",
    87.8
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2009.08553",
   "2108.02350",
   "2010.05006",
   "2003.03123",
   "2111.07991",
   "2202.04901",
   "2203.07836",
   "2006.15864",
   "1904.09117",
   "2202.10571"
  ]
 },
 "./longdocdata/docs/3636.json": {
  "question": "List the performance scores of various methods on the MIT-States (MIT-States) dataset on the Image Retrieval with Multi-Modal Query task using metric Recall@1.",
  "answer": "| Method | Recall@1 |\n| --- | --- |\n| ComposeAE | 13.9 |\n| TIRG | 12.2 |\n| Show and Tell | 11.9 |\n| FiLM | 10.1 |\n| Attribute as Operator | 8.8 |",
  "src_docs": [
   "1812.07119",
   "2006.11149",
   "1411.4555",
   "1803.09851",
   "1709.07871"
  ],
  "updated_answer": [
   [
    "2006.11149",
    "ComposeAE",
    "13.9",
    13.9
   ],
   [
    "1812.07119",
    "TIRG",
    "12.2",
    12.2
   ],
   [
    "1411.4555",
    "Show and Tell",
    "11.9",
    11.9
   ],
   [
    "1709.07871",
    "FiLM",
    "10.1",
    10.1
   ],
   [
    "1803.09851",
    "Attribute as Operator",
    "8.8",
    8.8
   ]
  ],
  "meta_info": {
   "datasets": "MIT-States",
   "datasets_short": "MIT-States",
   "task": "Image Retrieval with Multi-Modal Query",
   "metric": "Recall@1"
  },
  "updated_answer2": [
   [
    "2006.11149",
    "ComposeAE",
    "13.9",
    13.9
   ],
   [
    "1812.07119",
    "TIRG",
    "12.2",
    12.2
   ],
   [
    "1411.4555",
    "Show and Tell",
    "11.9",
    11.9
   ],
   [
    "1709.07871",
    "FiLM",
    "10.1",
    10.1
   ],
   [
    "1803.09851",
    "Attribute as Operator",
    "8.8",
    8.8
   ]
  ],
  "updated_answer3": [
   [
    "2006.11149",
    "ComposeAE",
    "13.9",
    13.9
   ],
   [
    "1812.07119",
    "TIRG",
    "12.2",
    12.2
   ],
   [
    "1411.4555",
    "Show and Tell",
    "11.9",
    11.9
   ],
   [
    "1709.07871",
    "FiLM",
    "10.1",
    10.1
   ],
   [
    "1803.09851",
    "Attribute as Operator",
    "8.8",
    8.8
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "1804.03786",
   "1612.01105",
   "1709.01829",
   "2105.04443",
   "2003.12943",
   "2202.11981",
   "1905.03197",
   "1808.07042",
   "1902.05356",
   "1511.03776"
  ]
 },
 "./longdocdata/docs/3673.json": {
  "question": "List the performance scores of various methods on the REAL275 (REAL275) dataset on the 6D Pose Estimation using RGBD task using metric mAP 3DIou@25.",
  "answer": "| Method | mAP 3DIou@25 |\n| --- | --- |\n| BundleTrack | 99.9 |\n| FS-Net | 95.1 |\n| 6-PACK | 94.2 |\n| NOCS (128 bins) | 84.9 |\n| GPV-Pose | 84.2 |\n| CenterSnap | 83.5 |\n| CPPF | 78.2 |",
  "src_docs": [
   "2103.07054",
   "1901.02970",
   "2203.01929",
   "2203.03089",
   "1910.10750",
   "2203.07918",
   "2108.00516"
  ],
  "updated_answer": [
   [
    "2203.07918",
    "GPV-Pose",
    "84.2",
    84.2
   ],
   [
    "2203.01929",
    "CenterSnap",
    "83.5",
    83.5
   ],
   [
    "2103.07054",
    "FS-Net",
    "95.1",
    95.1
   ],
   [
    "2203.03089",
    "CPPF",
    "78.2",
    78.2
   ],
   [
    "1901.02970",
    "NOCS (128 bins)",
    "84.9",
    84.9
   ],
   [
    "2108.00516",
    "BundleTrack",
    "99.9",
    99.9
   ],
   [
    "1910.10750",
    "6-PACK",
    "94.2",
    94.2
   ]
  ],
  "meta_info": {
   "datasets": "REAL275",
   "datasets_short": "REAL275",
   "task": "6D Pose Estimation using RGBD",
   "metric": "mAP 3DIou@25"
  },
  "updated_answer2": [
   [
    "2108.00516",
    "BundleTrack",
    "99.9",
    99.9
   ],
   [
    "2103.07054",
    "FS-Net",
    "95.1",
    95.1
   ],
   [
    "1910.10750",
    "6-PACK",
    "94.2",
    94.2
   ],
   [
    "1901.02970",
    "NOCS (128 bins)",
    "84.9",
    84.9
   ],
   [
    "2203.07918",
    "GPV-Pose",
    "84.2",
    84.2
   ],
   [
    "2203.01929",
    "CenterSnap",
    "83.5",
    83.5
   ],
   [
    "2203.03089",
    "CPPF",
    "78.2",
    78.2
   ]
  ],
  "updated_answer3": [
   [
    "1910.10750",
    "6-PACK",
    "95.1",
    95.1
   ],
   [
    "1901.02970",
    "NOCS (128 bins)",
    "84.9",
    84.9
   ],
   [
    "2203.07918",
    "GPV-Pose",
    "84.2",
    84.2
   ],
   [
    "2203.01929",
    "CenterSnap",
    "83.5",
    83.5
   ],
   [
    "2203.03089",
    "CPPF",
    "78.5",
    78.5
   ]
  ],
  "new_added": true,
  "distubed_docid": [
   "2202.04901",
   "1904.09569",
   "2010.04520",
   "2003.03164",
   "1606.07659",
   "2203.13250",
   "2007.03347",
   "1907.12347",
   "2009.07526",
   "2101.00916"
  ]
 },
 "./longdocdata/docs/404.json": {
  "question": "List the performance scores of various methods on the Flickr30k Captions test (Flickr30k) dataset on the Image Captioning task using metric CIDEr.",
  "answer": "| Method | CIDEr |\n| --- | --- |\n| Unified VLP | 67.4 |\n| Cornia et al | 46.4 |\n| FewVLM | 31.0 |\n| BRNN | 24.7 |\n| VL-T5 | 2.6 |",
  "src_docs": [
   "2102.02779",
   "1412.2306",
   "1909.11059",
   "2110.08484",
   "1706.08474"
  ],
  "updated_answer": [
   [
    "1909.11059",
    "seq2seq pre-training only",
    "68.5",
    68.5
   ],
   [
    "1706.08474",
    "Saliency+Context Attention",
    "46.4",
    46.4
   ],
   [
    "1412.2306",
    "BRNN",
    "24.7",
    24.7
   ],
   [
    "2110.08484",
    "FEWVLMlarge",
    " 37.0",
    37.0
   ],
   [
    "2102.02779",
    "VL-T5",
    "12.8",
    12.8
   ]
  ],
  "meta_info": {
   "datasets": "Flickr30k Captions test",
   "datasets_short": "Flickr30k",
   "task": "Image Captioning",
   "metric": "CIDEr"
  },
  "updated_answer2": [
   [
    "1909.11059",
    "seq2seq pre-training only",
    "68.5",
    68.5
   ],
   [
    "1706.08474",
    "Saliency+Context Attention",
    "46.4",
    46.4
   ],
   [
    "2110.08484",
    "FEWVLMlarge",
    "37.0",
    37.0
   ],
   [
    "1412.2306",
    "BRNN",
    "24.7",
    24.7
   ],
   [
    "2102.02779",
    "VL-T5",
    "12.8",
    12.8
   ]
  ],
  "updated_answer3": [
   [
    "1909.11059",
    "seq2seq pre-training only",
    "68.5",
    68.5
   ],
   [
    "1706.08474",
    "Saliency+Context Attention",
    "46.4",
    46.4
   ],
   [
    "2110.08484",
    "FEWVLMlarge",
    "37.0",
    37.0
   ],
   [
    "1412.2306",
    "BRNN",
    "24.7",
    24.7
   ],
   [
    "2102.02779",
    "VL-T5",
    "12.8",
    12.8
   ]
  ]
 }
}
Following the typical multi-modal retrieval frameworks [27, 39], our text-video retrieval model is composed of a text encoder $g$ and a video encoder $h$ . Given a text $t_{i}$ and a video $\boldsymbol{v}_{i}$ , $g(t_{i})$ and $h(v_{i})$ produce the normalized high-dimensional feature of the input, where $\ell_{2}$ normalization is often considered in final feature encoding. Then the similarity score of this text-video pair $(v_{i},t_{i})$ is $$
f(v_{i},t_{i})=h(v_{i})^{T}g(t_{i}).
$$

In training, a video-text pair $(v_{i},\ t_{i})$ is treated as the positive if $\boldsymbol{v}_{i}$ and $t_{i}$ are corresponded. All other instances of video or text in the mini-batch are treated as the negative. The text and video encoders are optimized in an end-to-end manner via normalized softmax loss [66]. The overall loss $\mathcal{L}$ is the average of video-to-text classification loss $(\mathcal{L}_{v2t})$ and text-to-video classification loss $(\mathcal{L}_{t2v})$ : $$
\mathcal{L}_{v2t}=-\frac{1}{N}\sum_{i}^{N}\log\frac{\exp(h(v_{i})^{T}g(t_{i})/\tau)}{\sum_{j=1}^{N}\exp(h(v_{i})^{T}g(t_{j})/\tau)},
$$
$$
\begin{array}{r l r}{\lefteqn{\mathcal{L}_{t2v}=-\frac{1}{N}\sum_{i}^{N}\log\frac{\exp(g(t_{i})^{T}h(v_{i})/\tau)}{\sum_{j=1}^{N}\exp(g(t_{i})^{T}h(v_{j})/\tau)},}}\\ &{}&{\mathcal{L}=\frac{1}{2}(\mathcal{L}_{v2t}+\mathcal{L}_{t2v}),}\end{array}
$$


where $N$ is the mini-batch size and $\tau$ is the temperature to scale the logits. It is worth noting that $\tau$ is crucial because both $h(v_{i})$ and $g(t_{i})$ are normalized. We set it as a trainable parameter following the CLIP model. During training, our model is initialized from the pre-trained weight of CLIP. We describe the details of the text encoder and video encoder below. 
3.1.1 Text encoder. We instantiate the text encoder using the text model of CLIP. It is a transformer [51] with the architecture modifications described in BERT [40], i.e., only encoder and no decoder. A transformer model typically consists of repeated blocks (layers) of multi-head self-attention (MHSA) and feed-forward networks (FFN). We use a transformer with 12 layers and 512 width with 8 attention heads, where the width is the dimension of the query, key, and value feature. The text tokenizer is a lower-cased byte pair encoding (BPE) [46] with a 49 152 vocab size. The text sequence is padded with [SOS] and [EOS] tokens. [SOS] and [EOS] is padded at the beginning and end of the text sequence, respectively. The final text feature representation is the activation from the last layer of the transformer that corresponds to the [EOS] token. This text representation is later normalized by layer normalization and linearly projected into the joint video-text embedding space. 
3.1.2 Video encoder. Our video encoder is a vision transformer (ViT), which first successfully applied transformers in vision tasks. The architecture of ViT is the same as the transformer in natural language processing, except ViT introduces an additional visual tokenization process to convert images into discrete sequences. When feeding images or videos into a ViT, we first convert the nonoverlapped image patches into visual sequences, where a [CLASS] token is prepended to the beginning of sequences as BERT [40]. Then the output of [CLASS] token at the final layer is extracted as the visual representation. In this work, we adopt a 2D linear projection to project image patches of different frames into an embedding space independently following the practice of CLIP4clip [32]. For convenience, we name this linear transformation process as visual tokenization. Generally, we use a ViT-B/32 model [11] with 12 layers and 512 width with 8 attention heads. ViT-B/32 means the nonoverlapped input image patch size is $32\times32$ . 
When applying the visual tokenization process to videos, it inevitably produces many redundant tokens as shown in Figure 1. Generally, an input video $\boldsymbol{v}_{i}$ consists of many temporal related ifrn .eAs:f $v_{i}=\{v_{i}^{1},v_{i}^{2},\ldots,v_{i}^{|v_{i}|}\}$ , iwf ehaecreh $\left|v_{i}\right|$ mise tphreodnucems rt okfefnras,mthees $\boldsymbol{v}_{i}$ $L$   
number of visual tokens is $\boldsymbol{L}|\boldsymbol{v}_{i}|$ (do not consider [CLASS] token). It shows that the number of visual tokens is linear to the number of tokens per frame $(L)$ and the video length. Given an input frame with a size of $224\times224$ , $L=49$ for the ViT-B/32 model and $L=196$ for the ViT-B/16 model. With a larger $L$ , the number of visual tokens becomes much larger. When performing text-video retrieval on long videos, the total number of tokens for a video is large. For example, videos in the ActivityNet [12] dataset usually have a few minutes duration. In this case, $L|v_{i}|$ will be easily larger than 1 000. 
The redundant tokens considerably increase computation costs. To make the training and inference of text-video retrieval models more efficient, we propose to use clustering algorithms to find the most representative token embeddings. This process significantly reduces the number of tokens while maintaining the most valuable information of original tokens. After clustering, we only reserve the center tokens and remove other non-center tokens. The reserved tokens contain most of the information about the video and it is sufficient for text-video retrieval. We describe our multi-segment token clustering in the next section. 

 # 3.2 Multi-segment Token Clustering 

The overall framework of our video encoder can be found in Figure 2. We perform a multi-segment clustering strategy on visual tokens from a certain temporal segment. This is based on the assumption that neighbor frames are more likely to be the same; then tokens of these similar neighbor frames are more possible to be redundant. Our multi-segment token clustering method empowers the model to achieve segment-level semantic alignment of text and video representations. Previously, CLIP and CLIP4clip adopt the average of frame features or the late fusion of frame features as the video representation. However, the former loses the temporal information, and the latter is a post-processing step and loses the detail temporal variations at the early stage of the transformer. By clustering across multiple frames within a segment at an early or middle stage, image patches from different temporal positions can interact with each other via the self-attention mechanism. 
Specifically, a video sequence {ùë£ùëñ1, ùë£ùëñ2, . . . , ùë£ùëñ|ùë£ùëñ | } is divided into ùëÜ segments $\{s_{i}^{1},s_{i}^{2},\ldots,s_{i}^{S}\}$ . Each segment contains $\frac{|v_{i}|}{S}$ frames and $\textstyle{\frac{L|v_{i}|}{S}}$ tokens. Then we perform token clustering on these $\textstyle{\frac{L|v_{i}|}{S}}$ tokens segment-wise, namely, clustering for each segment independently. Then the centers of all clusters from one segment, i.e., center tokens, are selected and other non-center tokens are simply dropped. These center tokens are concatenated and arranged according to their original relative spatial-temporal position. Center tokens from the upper-left position and early frames are at the beginning of the new token sequence. Center tokens from the bottom-right position and late frames are at the rear-end of the new token sequence. 
Multi-segment token clustering algorithm makes our vision model achieve segment-level temporal modeling and be able to capture the detailed temporal variation of video frames. This allows our methods to achieve segment-level alignment of the text $t_{i}$ and the video ${\boldsymbol{v}}_{i}$ consisted of segments $\{s_{i}^{1},s_{i}^{2},\ldots,s_{i}^{S}\}$ : $$
f(v_{i},t_{i})=\frac{1}{S}\sum_{j=1}^{S}h(s_{i}^{j})^{T}g(t_{i}).
$$

The table caption:

Table 6: Performing clustering twice on LSMDC. 

The html body of the table: 


<html><body><table><tr><td>Method</td><td>Mem.</td><td>TV</td><td>R@1‚Üë</td><td>R@5‚Üë</td><td>R@10‚Üë</td><td>MdR‚Üì</td><td>MnR‚Üì</td></tr><tr><td>CenterCLIP</td><td>13.52</td><td>T‚ÜíV</td><td>21.0</td><td>42.7</td><td>51.9</td><td>9.0</td><td>57.0</td></tr><tr><td rowspan="3">(B4-6,49), (B8- 3,49) CenterCLIP (B6- 4, 49)</td><td rowspan="3">16.39</td><td>V‚ÜíT</td><td>20.9</td><td>41.8</td><td>51.5</td><td>9.0</td><td>50.7</td></tr><tr><td>T‚ÜíV</td><td>21.7</td><td>39.8</td><td>49.8</td><td>11.0</td><td>54.8</td></tr><tr><td>V‚ÜíT</td><td>21.4</td><td>40.3</td><td>50.8</td><td>10.0</td><td>48.4</td></tr><tr><td rowspan="2">CenterCLIP (B6- 6, 49)</td><td rowspan="2">14.95</td><td>T‚ÜíV</td><td>21.9</td><td>41.1</td><td>50.7</td><td>10.0</td><td>55.6</td></tr><tr><td>V‚ÜíT</td><td>21.1</td><td>41.2</td><td>50.2</td><td>10.0</td><td>48.7</td></tr></table></body></html>




We can also take multiple times of clustering. For instance, firstly perform clustering with $\mathit{\Omega}^{\prime}S=6,B=4,$ and then with $\mathit{S}=3,\mathit{B}=$ 
8). The results are shown in Table 6. Such progressive clustering strategy achieves pretty good $\mathrm{R}@5$ , $\mathrm{R}@10$ , MdR, and memory cost reduction. However, performing multiple times will increase the time complexity and this is not suitable for large amounts of tokens. Thus we generally perform clustering once in this work. 
4.3.3 The number of cluster ùêæ and segment ùëÜ. We perform experiments on LSMDC and ActivityNet. The results including $\mathbf{R}@1$ , memory cost, and inference time are shown in Figure 4. Along with the increase of $K$ , the performance increases, and computation costs also increase. At the same time, a small segment number ùëÜ does not always achieve better performance, e.g., $S=1$ on LSMDC and $S=6$ on ActivityNet. A small segment number means more tokens are dropped. This will cause the loss of more information. When $S=1$ on LSMDC and $S=6$ on ActivityNet, the number of tokens in a segment is large, i.e., $12\times49$ and $10\times49$ , this leads to more computational costs of clustering as shown in Figure 4c and Figure 4d. Thus a moderate segment number $s$ is usually adopted. 
4.3.4 The number of input frames $N_{i n}$ . We change the number of input video frames and take experiments with CenterCLIP $(B_{6}-$ 15, 49) on ActivityNet. The results are shown in Table 7. The large the number of input frames $N_{i n}$ , the more computation costs, and a small number of frames will lead to worse performance. Similar ablations about the input of frames on short video datasets like 
MSR-VTT can be found in CLIP4clip [32]. When the number of segments $s$ is fixed, a large number of input frames $N_{i n}$ will also increase computation costs of the clustering process as the number of tokens in one temporal segment increases. 

 # 5 CONCLUSION 

In this work, we propose a multi-segment clustering algorithm to reduce the number of redundant tokens of continuous video frames, and achieve segment-level alignment of video and text representations for text-video retrieval task. Our method, named CenterCLIP as we only reserve center tokens of token clusters and drop non-center tokens, is based on the knowledge of largescale image-text pairs pre-trained model ‚Äì CLIP. We take extensive experiments on four common text-video multi-modal datasets: MSRVTT, MSVD, LSMDC, and ActivityNet. CenterCLIP achieves stateof-the-art performance on all these four datasets and surpass the old SOTA by a large margin. At the same time, CenterCLIP realizes a decent reduction in memory costs and speedup of inference time. 
 # ACKNOWLEDGMENTS 

This work is supported by National Key R&D Program of China under Grant No. 2020AAA0108800. Thanks Naiyuan Liu for his helpful discussions. 
 # REFERENCES 


[1] David Arthur and Sergei Vassilvitskii. 2007. k-means++: the advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007, Nikhil Bansal, Kirk Pruhs, and Clifford Stein (Eds.). SIAM, 1027‚Äì1035.   
[2] Max Bain, Arsha Nagrani, G√ºl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650 (2021).   
[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The longdocument transformer. arXiv preprint arXiv:2004.05150 (2020).   
[4] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).   
[5] R. Bro, E. Acar, and Tamara G. Kolda. 2008. Resolving the sign ambiguity in the singular value decomposition. Journal of Chemometrics 22, 2 (2008), 135‚Äì140. https://doi.org/10.1002/cem.1122   
[6] Shuhao Cao. 2021. Choose a Transformer: Fourier or Galerkin. In Thirty-Fifth Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:arXiv: 2105.14995 [cs.CL]   
[7] David Chen and William B Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies. 190‚Äì200.   
[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019). [9] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794 (2020).   
[10] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 11583‚Äì11593.   
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.   
[12] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. 2015. ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 961‚Äì970.   
[13] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multimodal transformer for video retrieval. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part IV 16. Springer, 214‚Äì229.   
[14] Priya Goyal, Piotr Doll√°r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. CoRR abs/1706.02677 (2017). arXiv:1706.02677   
[15] Feng He, Qi Wang, Zhifan Feng, Wenbin Jiang, Yajuan L√º, Yong Zhu, and Xiao Tan. 2021. Improving Video Retrieval by Adaptive Margin. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1359‚Äì1368.   
[16] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. Bag of Tricks for Image Classification with Convolutional Neural Networks. CoRR abs/1812.01187 (2018).   
[17] Fabian Caba Heilbron and Juan Carlos Niebles. 2014. Collecting and Annotating Human Activities in Web Videos. In Proceedings of International Conference on Multimedia Retrieval. ACM, 377.   
[18] Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, et al. 2021. WenLan: Bridging vision and language by large-scale multi-modal pre-training. arXiv preprint arXiv:2103.06561 (2021).   
[19] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. 2021. Perceiver io: A general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795 (2021).   
[20] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2758‚Äì2766.   
[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and visionlanguage representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918 (2021).   
[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning. PMLR, 5156‚Äì5165.   
[23] I. Katsavounidis, C.-C. Jay Kuo, and Zhen Zhang. 1994. A new initialization technique for generalized Lloyd iteration. IEEE Signal Processing Letters 1, 10 (1994), 144‚Äì146.   
[24] Dotan Kaufman, Gil Levi, Tal Hassner, and Lior Wolf. 2017. Temporal tessellation: A unified approach for video analysis. In Proceedings of the IEEE International Conference on Computer Vision. 94‚Äì104.   
[25] Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451 (2020).   
[26] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 9972‚Äì9981.   
[27] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7331‚Äì7341.   
[28] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020. Hero: Hierarchical encoder for video $^{1+}$ language omni-representation pre-training. arXiv preprint arXiv:2005.00200 (2020).   
[29] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487 (2019).   
[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021).   
[31] Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization. In International Conference on Learning Representations.   
[32] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2021. CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval. CoRR abs/2104.08860 (2021). arXiv:2104.08860   
[33] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David Garc√≠a, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed Precision Training. In ICLR.   
[34] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9879‚Äì9889.   
[35] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2630‚Äì2640.   
[36] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2001. On Spectral Clustering: Analysis and an algorithm. In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS. 849‚Äì856.   
[37] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2085‚Äì2094.   
[38] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Joao Henriques, and Andrea Vedaldi. 2020. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824 (2020).   
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. CoRR abs/2103.00020 (2021). arXiv:2103.00020   
[40] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.   
[41] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. 2021. DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. arXiv preprint arXiv:2106.02034 (2021).   
[42] Anna Rohrbach, Marcus Rohrbach, and Bernt Schiele. 2015. The long-short story of movie description. In German conference on pattern recognition. Springer, 209‚Äì221.   
[43] Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, et al. 2020. Avlnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199 (2020).   
[44] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics 9 (2021), 53‚Äì68.   
[45] Michael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. 2021. TokenLearner: What Can 8 Learned Tokens Do for Images and Videos? arXiv preprint arXiv:2106.11297 (2021).   
[46] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.   
[47] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, KaiWei Chang, Zhewei Yao, and Kurt Keutzer. 2021. How Much Can CLIP Benefit Vision-and-Language Tasks? arXiv preprint arXiv:2107.06383 (2021).   
[48] Ting Su and Jennifer G. Dy. 2007. In search of deterministic methods for initializing K-means and Gaussian mixture clustering. Intell. Data Anal. 11, 4 (2007), 319‚Äì338.   
[49] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 7464‚Äì 7473.   
[50] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research 9, 86 (2008), 2579‚Äì2605.   
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998‚Äì6008.   
[52] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and computing 17, 4 (2007), 395‚Äì416.   
[53] Junke Wang, Xitong Yang, Hengduo Li, Zuxuan Wu, and Yu-Gang Jiang. 2021. Efficient Video Transformers with Spatial-Temporal Token Selection. arXiv preprint arXiv:2111.11591 (2021).   
[54] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. 2016. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision. Springer, 20‚Äì36.   
[55] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 (2020).   
[56] Xiaohan Wang, Linchao Zhu, and Yi Yang. 2021. T2vlad: global-local sequence alignment for text-video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5079‚Äì5088.   
[57] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. 2020. Visual transformers: Token-based image representation and processing for computer vision. arXiv preprint arXiv:2006.03677 (2020).   
[58] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia. 1645‚Äì1653.   
[59] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084 (2021).   
[60] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition. 5288‚Äì5296.   
[61] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. 2021. AdaViT: Adaptive Tokens for Efficient Vision Transformer. arXiv preprint arXiv:2112.07658 (2021).   
[62] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei Xu. 2016. Video paragraph captioning using hierarchical recurrent neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4584‚Äì4593.   
[63] Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV). 471‚Äì487.   
[64] Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. 2017. End-to-end concept word detection for video captioning, retrieval, and question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3165‚Äì3173.   
[65] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: A New Foundation Model for Computer Vision. arXiv preprint arXiv:2111.11432 (2021).   
[66] Andrew Zhai and Hao-Yu Wu. 2019. Classification is a Strong Baseline for Deep Metric Learning. In 30th British Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019. 91.   
[67] Bowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-modal and hierarchical modeling of video and text. In Proceedings of the European Conference on Computer Vision (ECCV). 374‚Äì390.   
[68] Hao Zhang, Yanbin Hao, and Chong-Wah Ngo. 2021. Token shift transformer for video classification. In Proceedings of the 29th ACM International Conference on Multimedia. 917‚Äì925. 

 # Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy 

Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao, Ziwei Liu 
Abstract‚ÄîLarge-scale datasets play a vital role in computer vision. But current datasets are annotated blindly without differentiation to samples, making the data collection inefficient and unscalable. The open question is how to build a mega-scale dataset actively. Although advanced active learning algorithms might be the answer, we experimentally found that they are lame in the realistic annotation scenario where out-of-distribution data is extensive. This work thus proposes a novel active learning framework for realistic dataset annotation. Equipped with this framework, we build a high-quality vision dataset‚ÄîBamboo, which consists of 69M image classification annotations with 119K categories and 28M object bounding box annotations with 809 categories. We organize these categories by a hierarchical taxonomy integrated from several knowledge bases. The classification annotations are four times larger than ImageNet22K, and that of detection is three times larger than Object365. Compared to ImageNet22K and Objects365, models pre-trained on Bamboo achieve superior performance among various downstream tasks $6.2\%$ gains on classification and $2.1\%$ gains on detection). We believe our active learning framework and Bamboo are essential for future work. Code and dataset are available at https://github.com/ZhangYuanhan-AI/Bamboo. 
Index Terms‚ÄîVision Dataset, Human-Machine Synergy. 
 # 1 INTRODUCTION 


L ARGE-scale pre-trained models, either trained in a supervised [27], [37], [71] or unsupervised [10], [14], [29] manner, have become a foundation for modern computer vision. Pre-trained models [53] bring various applications by transferring to downstream tasks. Most importantly, the fuel of this foundation models [6] relies on the availability of increasingly large and diverse datasets [18], [40], [44], [60]. 
Building a high-quality dataset requires careful consideration in selecting data. However, public datasets are annotated blindly with no differentiation to samples, which brings a colossal waste of annotation budget: Citovsky et al [17] indicates that only $70\%$ data in OpenImages [41] can achieve on-par performance to its complete set. Though active learning (AL) researchers extensively study how to select the most valuable samples‚Äîinformative and indistribution‚Äìfrom unlabeled data pool [26], [28], [33], [34], [58], [61], [64], [66], [75], we experimentally observe that the current advanced active learning methods, e.g. ClusterMargin [17], Margin [54] and CoreSet [57], are lame in the realistic annotation scenario. Specifically, AL methods select high-informative data at the expense of choosing out-ofdistribution data discarded by annotators and not used for model supervised learning. Random sampling selects less high-informative data than AL but includes much more indistribution data than AL. As the performance gain bought by data quantity is superior to data quality when annotated data of AL is $70\%$ less than that of random sampling (as shown in Fig. 6), AL is inferior to random sampling for improving model performance. Given this shortage, we propose a novel active learning framework, which cleans off the out-of-distribution data in the unlabeled data pool before active sampling, ensuring the sampled data under active learning is informative and meanwhile in-distribution. This novel framework achieves better than random sampling for boosting supervised learning model performance. 
We aim to annotate a mega-scale classification and object detection dataset with our proposed active learning framework. First, we build a comprehensive label system for querying diverse data covering numerous semantics. Specifically, we form a unified label system with a hierarchical structure consisting of 304,048 categories. It integrates label systems from 19 latest public image classification datasets and five object detection datasets and also absorbs 170,586 new categories from knowledge bases, e.g. Wikidata [67]. Then, we contribute Bamboo Dataset, a megascale and information-dense dataset for the pre-training of both classification and detection, which is active annotating by human-machine synergy. Bamboo has three appealing properties: 
‚Ä¢ Comprehensive. It consists of 69M image classification annotations and 28M object bounding box annotations, spanning over 119K visual categories. The scale of the label system and the annotated data are the largest among all the publicly available datasets. We illustrate the comparison of Bamboo and publicly available datasets in the Fig. 1(c). 
Information-Dense. We guarantee Bamboo is highly informative through the label system and the annotated data. The label system is constructed by thoroughly integrating public datasets and knowledge bases. Our active annotation pipeline specifically selects the annotated data to reduce model uncertainty. 

Wikidata. Wikidata [67] contains a large number of concepts, such as different kinds of foods, animals, and locations. As the number of concepts in Wikidata continues to grow, so far, we have included 170,586 concepts from it. These concepts are the leaf nodes in their taxonomy. 
 # 3.2 Concepts Integration 

WordNet is a lexical graph whose concepts imply semantic relation. For example, the father node of ‚ÄúBritish Shorthair‚Äù is ‚ÄúDomestic Cat‚Äù. How to integrate concepts from public datasets and Wikidata into this WordNet is an open question. We propose three parallel solutions to integrate these categories into WordNet in this work. 
Solution 1: Leveraging on the subclassOf. The taxonomy of Wikidata is contributed by adding the ‚ÄúsubclassOf‚Äù that is related to the hypernyms relationship in the taxonomy of WordNet. Referred to [62], we link Wikidata leaf node concepts to the WordNet by leveraging the ‚ÄúsubclassOf‚Äù. 
Solution 2: Parsing the Concept. Referred to the previous work [23], we can also link the concept to the WordNet through word parsing. For example, for the concept Sumatran Orangutan, we parse this concept [31] and get its head compound ‚ÄúOrangutan‚Äù. In this way, we add Sumatran Orangutan as the new hyponym of the ‚ÄúOrangutan‚Äù if ‚ÄúOrangutan‚Äù exists in WordNet. 